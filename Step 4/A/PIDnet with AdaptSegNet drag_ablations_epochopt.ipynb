{"cells":[{"cell_type":"markdown","metadata":{"id":"cbIwgK7-uZ0l"},"source":["**WARNING**: Remember to run the `ExtractBoundaries` notebook present in this same folder before running all (or specifically the `Import Boundaries` cell), if you don't have them in your drive inside /LoveDA  \n","\n","**WARNING2** Be sure to have `best_model_Resize.pth` or similar on the colab disk if you're running the pretrained model"]},{"cell_type":"code","source":["SAVE_ON_DRIVE = True # Set to True if you want to save the datasets and trained models to your Google Drive\n","TYPE = 'Train'"],"metadata":{"id":"qeY1VfJ5T8kV","executionInfo":{"status":"ok","timestamp":1736182221729,"user_tz":-60,"elapsed":411,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}}},"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YwDm553BzPlB"},"source":["# Dataset initialization\n"]},{"cell_type":"markdown","source":["### Download Data"],"metadata":{"id":"7zeYu_g-bpip"}},{"cell_type":"code","execution_count":51,"metadata":{"id":"cjuW8Yf0zO7K","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ad74bde1-8c5d-411f-8b78-5475082a809f","executionInfo":{"status":"ok","timestamp":1736182226215,"user_tz":-60,"elapsed":2856,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Train dataset already in local\n"]}],"source":["from google.colab import drive\n","import os\n","import shutil\n","\n","drive.mount('/content/drive')\n","\n","# Set paths for Validation and Test datasets\n","val_dataset_path = '/content/drive/MyDrive/LoveDA/Val'\n","test_dataset_path = '/content/drive/MyDrive/LoveDA/Test'\n","\n","\n","# Function to handle dataset download and extraction\n","def handle_dataset(dataset_name, zip_url, local_path, drive_path):\n","    if not os.path.exists(local_path):\n","        if os.path.exists(f\"{drive_path}.zip\"):\n","            print(f\"{dataset_name} dataset available on own drive, unzipping...\")\n","            !unzip -q {drive_path}.zip -d ./\n","        else:\n","            print(f\"Downloading {dataset_name} dataset...\")\n","            !wget -O {dataset_name}.zip \"{zip_url}\"\n","            if SAVE_ON_DRIVE:\n","                print(f\"Saving {dataset_name} dataset on drive...\")\n","                !cp {dataset_name}.zip {drive_path}.zip\n","                print(f\"{dataset_name} dataset saved on drive\")\n","            print(f\"Unzipping {dataset_name} dataset...\")\n","            !unzip -q {dataset_name}.zip -d ./\n","    else:\n","        print(f\"{dataset_name} dataset already in local\")\n","\n","# Handle Validation dataset\n","#handle_dataset(\"Validation\", \"https://zenodo.org/records/5706578/files/Val.zip?download=1\", \"./Val\", \"/content/drive/MyDrive/LoveDA/Val\")\n","\n","# Handle Test dataset\n","#handle_dataset(\"Test\", \"https://zenodo.org/records/5706578/files/Test.zip?download=1\", \"./Test\", \"/content/drive/MyDrive/LoveDA/Test\")\n","\n","# Handle Train dataset\n","handle_dataset(\"Train\", \"https://zenodo.org/records/5706578/files/Train.zip?download=1\", \"./Train\", \"/content/drive/MyDrive/LoveDA/Train\")"]},{"cell_type":"markdown","source":["### Import Boundaries"],"metadata":{"id":"OftJbvlhH725"}},{"cell_type":"code","source":["# Paths\n","rural_boundaries_path = \"./Train/Rural/boundaries_png\"\n","rural_masks_path = './Train/Rural/masks_png'\n","\n","urban_boundaries_path = \"./Train/Urban/boundaries_png\"\n","urban_masks_path = './Train/Urban/masks_png'\n","drive_rural_boundaries_path = '/content/drive/MyDrive/LoveDA/boundaries/Rural/boundaries_png'\n","drive_urban_boundaries_path = '/content/drive/MyDrive/LoveDA/boundaries/Urban/boundaries_png'\n","\n","boundaries_paths = [rural_boundaries_path, urban_boundaries_path]\n","\n","# Make dir inside ./Train/...\n","for boundaries_path in boundaries_paths:\n","    if (os.path.exists(boundaries_path) == False):\n","        print(f\"Creating {boundaries_path}...\")\n","        os.makedirs(boundaries_path)\n","    else:\n","        print(f\"{boundaries_path} exists...\")\n","\n","\n","# Check if files are already present\n","rural_file_count = len([name for name in os.listdir(rural_boundaries_path) if os.path.isfile(os.path.join(rural_boundaries_path, name))])\n","rural_mask_file_count = len([name for name in os.listdir(rural_masks_path) if os.path.isfile(os.path.join(rural_masks_path, name))])\n","urban_file_count = len([name for name in os.listdir(urban_boundaries_path) if os.path.isfile(os.path.join(urban_boundaries_path, name))])\n","urban_mask_file_count = len([name for name in os.listdir(urban_masks_path) if os.path.isfile(os.path.join(urban_masks_path, name))])\n","\n","# if (rural_file_count != rural_mask_file_count):\n","#     print(f\"Importing boundaries, as we have {rural_file_count} rural boundaries as of now...\")\n","#     shutil.copytree(drive_rural_boundaries_path, rural_boundaries_path, dirs_exist_ok=True)\n","# else:\n","#     print(f\"Rural boundaries already present, {rural_file_count} files...\")\n","\n","if (urban_file_count != urban_mask_file_count):\n","    print(f\"Importing boundaries, as we have {urban_file_count} urban boundaries as of now...\")\n","    shutil.copytree(drive_urban_boundaries_path, urban_boundaries_path, dirs_exist_ok=True)\n","else:\n","    print(f\"Urban boundaries already present, {urban_file_count} files...\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XObla4TbH9Y7","executionInfo":{"status":"ok","timestamp":1736182226215,"user_tz":-60,"elapsed":9,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}},"outputId":"11066659-9dde-460f-96ad-49d48f36d9a4"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["./Train/Rural/boundaries_png exists...\n","./Train/Urban/boundaries_png exists...\n","Urban boundaries already present, 1156 files...\n"]}]},{"cell_type":"markdown","metadata":{"id":"68BKhuGh2vJH"},"source":["### Dataset Definition"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"iUsQczlZ3Wx0","executionInfo":{"status":"ok","timestamp":1736182226215,"user_tz":-60,"elapsed":8,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}}},"outputs":[],"source":["import os\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import torchvision.transforms as T\n","import random\n","import cv2\n","\n","\n","def pil_loader(path, color_type):\n","    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n","    with open(path, 'rb') as f:\n","        img = Image.open(f)\n","        return img.convert(color_type)\n","\n","class LoveDADataset(Dataset):\n","    def __init__(self, baseTransform, augTransforms, split = 'Urban', type = 'Train', useBoundaries=True, validation_ratio=0.2, seed=265637):\n","        # Validate type input\n","        if type not in ['Train', 'Validation', 'Total', 'ActualTest']:\n","            raise ValueError(\"Invalid type. Expected 'Train' or 'Validation' or 'Total' or 'ActualTest'.\")\n","        self.directory = []\n","        if type == 'ActualTest':\n","            directory_path = os.path.join('./Test', split, 'images_png')\n","        else:\n","            directory_path = os.path.join('./Train', split, 'images_png')\n","        # Check if the directory exists\n","        if not os.path.exists(directory_path):\n","            raise FileNotFoundError(f\"Directory not found: {directory_path}\")\n","        # Get all image paths\n","        all_images = [os.path.join(directory_path, entry) for entry in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, entry))]\n","        # Shuffle images for random splitting\n","        random.seed(seed)\n","        random.shuffle(all_images)\n","        # Split into training and validation sets\n","        split_idx = int(len(all_images) * (1 - validation_ratio))\n","        if type == 'Train':\n","            self.directory = all_images[:split_idx]\n","        elif type == 'Validation':\n","            self.directory = all_images[split_idx:]\n","        elif type == 'Total':\n","            self.directory = all_images\n","        elif type == 'ActualTest':\n","            self.directory = all_images\n","        else:\n","            raise ValueError(\"Invalid type. Expected 'Train' or 'Validation' or 'Total' or 'ActualTest.\")\n","        self.baseTransforms = baseTransform\n","        self.augTransforms = augTransforms\n","        self.useBoundaries = useBoundaries\n","        self.typeDataset = type\n","        # Print dataset size\n","        print(f\"Dataset size: {len(self.directory)}\")\n","\n","    def __len__(self):\n","        return len(self.directory)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.directory[idx]\n","        image = pil_loader(image_path, 'RGB')\n","        mask_path = image_path.replace('images_png', 'masks_png')\n","        boundaries_path = image_path.replace('images_png', 'boundaries_png')\n","\n","        mask = pil_loader(mask_path, 'L')\n","\n","        if self.useBoundaries:\n","          boundaries = pil_loader(boundaries_path, 'L')\n","        else:\n","          boundaries = mask\n","\n","        base_transformed = self.baseTransforms(image=np.array(image), mask=np.array(mask), boundaries=np.array(boundaries))\n","        base_image = base_transformed['image']\n","        base_mask = base_transformed['mask']\n","        base_boundaries = base_transformed['boundaries']\n","\n","        base_image = T.Compose([T.ToTensor()])(base_image)\n","        base_mask = torch.from_numpy(base_mask).long()\n","        base_mask -= 1\n","        base_boundaries = torch.from_numpy(base_boundaries)\n","\n","        if(self.typeDataset != 'Train'):\n","          return base_image, base_mask, image_path, base_boundaries\n","\n","\n","        if self.augTransforms == None:\n","          return [base_image], [base_mask], image_path, [base_boundaries]\n","        # Apply transformations\n","        augmented = self.augTransforms(image=np.array(image), mask=np.array(mask), boundaries=np.array(boundaries))\n","        augmented_image = T.Compose([T.ToTensor()])(augmented['image'])\n","\n","        mask = augmented['mask']\n","        mask = torch.from_numpy(mask).long()\n","        mask = mask-1\n","        boundaries = augmented['boundaries']\n","        boundaries = torch.from_numpy(boundaries)\n","\n","        image_list = [base_image, augmented_image]\n","        mask_list = [base_mask, mask]\n","        boundaries_list = [base_boundaries, boundaries]\n","\n","        return image_list, mask_list, image_path, boundaries_list"]},{"cell_type":"markdown","source":["### Dataset Utils"],"metadata":{"id":"xSaOfED0-zoc"}},{"cell_type":"code","source":["import matplotlib.patches as mpatches\n","\n","from collections import OrderedDict\n","COLOR_MAP = OrderedDict(\n","    Background=(255, 255, 255),\n","    Building=(255, 0, 0),\n","    Road=(255, 255, 0),\n","    Water=(0, 0, 255),\n","    Barren=(159, 129, 183),\n","    Forest=(34, 139, 34),\n","    Agricultural=(255, 195, 128),\n",")\n","\n","LABEL_MAP = OrderedDict(\n","    Background=0,\n","    Building=1,\n","    Road=2,\n","    Water=3,\n","    Barren=4,\n","    Forest=5,\n","    Agricultural=6,\n",")\n","inverted_label_map = OrderedDict((v, k) for k, v in LABEL_MAP.items())\n","\n","\n","def getLabelColor(label):\n","    # Default color for unclassified labels\n","    default_color = np.array([128, 128, 128])  # Gray\n","\n","    # Check if label exists in inverted_label_map\n","    label_name = inverted_label_map.get(label, None)\n","    if label_name is None or label_name not in COLOR_MAP:\n","        return default_color  # Return default color for unclassified\n","\n","    # Return the mapped color\n","    label_color = np.array(COLOR_MAP[label_name])\n","    return label_color\n","\n","\n","def getLegendHandles():\n","  handles = [mpatches.Patch(color=getLabelColor(i)/255, label=inverted_label_map[i]) for i in range(0, len(LABEL_MAP))]\n","  handles.append(mpatches.Patch(color=getLabelColor(-1)/255, label='Unclassified'))\n","  return handles\n","\n","def new_colors_mask(mask):\n","  new_image = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n","  for i, row in enumerate(mask):\n","    for j, cell in enumerate(row):\n","      new_image[i][j] = getLabelColor(cell.item())\n","  return new_image\n","\n"],"metadata":{"id":"tfi1pG3P-2H1","executionInfo":{"status":"ok","timestamp":1736182226215,"user_tz":-60,"elapsed":7,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Debug"],"metadata":{"id":"IrsSERdV_xZc"}},{"cell_type":"code","execution_count":55,"metadata":{"id":"l5iBVMMn6-3-","executionInfo":{"status":"ok","timestamp":1736182226215,"user_tz":-60,"elapsed":7,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}}},"outputs":[],"source":["# # Comment this cell to save GPU time\n","\n","# import matplotlib.pyplot as plt\n","# import torch\n","# from torch.utils.data import DataLoader\n","# import matplotlib.patches as mpatches\n","\n","# train_dataset = LoveDADataset(type='Train', seed=222)\n","# print(train_dataset.__len__())\n","\n","# # Get item\n","# image, mask, path, bd = train_dataset.__getitem__(88)\n","\n","# # Show path\n","# print(f\"Image is at {path}\")\n","\n","# # Show image\n","# image = image.permute(1, 2, 0)\n","# image = image.numpy()\n","# plt.imshow(image)\n","\n","# # Show mask\n","# new_image = new_colors_mask(mask)\n","# plt.imshow(image)\n","# plt.show()\n","# plt.legend(handles=getLegendHandles(), loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n","# plt.imshow(new_image)\n","# plt.show()\n","\n","# # Show boundaries\n","# # for row in bd:\n","# #     for col in row:\n","# #         if col != 0 and col != 1:\n","# #             print(col)\n","# bd = bd.numpy()\n","# plt.imshow(bd)\n"]},{"cell_type":"markdown","metadata":{"id":"uZwN55AlCRZc"},"source":["# Initialize model"]},{"cell_type":"markdown","source":["### PIDNet Util Modules"],"metadata":{"id":"V1PldejM1ZCB"}},{"cell_type":"code","source":["# ------------------------------------------------------------------------------\n","# Written by Jiacong Xu (jiacong.xu@tamu.edu)\n","# ------------------------------------------------------------------------------\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","BatchNorm2d = nn.BatchNorm2d\n","bn_mom = 0.1\n","algc = False\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=False):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.no_relu = no_relu\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","\n","        if self.no_relu:\n","            return out\n","        else:\n","            return self.relu(out)\n","\n","class Bottleneck(nn.Module):\n","    expansion = 2\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=True):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n","                               bias=False)\n","        self.bn3 = BatchNorm2d(planes * self.expansion, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.no_relu = no_relu\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        if self.no_relu:\n","            return out\n","        else:\n","            return self.relu(out)\n","\n","class segmenthead(nn.Module):\n","\n","    def __init__(self, inplanes, interplanes, outplanes, scale_factor=None):\n","        super(segmenthead, self).__init__()\n","        self.bn1 = BatchNorm2d(inplanes, momentum=bn_mom)\n","        self.conv1 = nn.Conv2d(inplanes, interplanes, kernel_size=3, padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(interplanes, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(interplanes, outplanes, kernel_size=1, padding=0, bias=True)\n","        self.scale_factor = scale_factor\n","\n","    def forward(self, x):\n","\n","        x = self.conv1(self.relu(self.bn1(x)))\n","        out = self.conv2(self.relu(self.bn2(x)))\n","\n","        if self.scale_factor is not None:\n","            height = x.shape[-2] * self.scale_factor\n","            width = x.shape[-1] * self.scale_factor\n","            out = F.interpolate(out,\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)\n","\n","        return out\n","\n","class DAPPM(nn.Module):\n","    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n","        super(DAPPM, self).__init__()\n","        bn_mom = 0.1\n","        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale0 = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.process1 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.process2 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.process3 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.process4 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.compression = nn.Sequential(\n","                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n","                                    )\n","        self.shortcut = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n","                                    )\n","\n","    def forward(self, x):\n","        width = x.shape[-1]\n","        height = x.shape[-2]\n","        x_list = []\n","\n","        x_list.append(self.scale0(x))\n","        x_list.append(self.process1((F.interpolate(self.scale1(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[0])))\n","        x_list.append((self.process2((F.interpolate(self.scale2(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[1]))))\n","        x_list.append(self.process3((F.interpolate(self.scale3(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[2])))\n","        x_list.append(self.process4((F.interpolate(self.scale4(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[3])))\n","\n","        out = self.compression(torch.cat(x_list, 1)) + self.shortcut(x)\n","        return out\n","\n","class PAPPM(nn.Module):\n","    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n","        super(PAPPM, self).__init__()\n","        bn_mom = 0.1\n","        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","\n","        self.scale0 = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","\n","        self.scale_process = nn.Sequential(\n","                                    BatchNorm(branch_planes*4, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes*4, branch_planes*4, kernel_size=3, padding=1, groups=4, bias=False),\n","                                    )\n","\n","\n","        self.compression = nn.Sequential(\n","                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n","                                    )\n","\n","        self.shortcut = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n","                                    )\n","\n","\n","    def forward(self, x):\n","        width = x.shape[-1]\n","        height = x.shape[-2]\n","        scale_list = []\n","\n","        x_ = self.scale0(x)\n","        scale_list.append(F.interpolate(self.scale1(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","        scale_list.append(F.interpolate(self.scale2(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","        scale_list.append(F.interpolate(self.scale3(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","        scale_list.append(F.interpolate(self.scale4(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","\n","        scale_out = self.scale_process(torch.cat(scale_list, 1))\n","\n","        out = self.compression(torch.cat([x_,scale_out], 1)) + self.shortcut(x)\n","        return out\n","\n","\n","class PagFM(nn.Module):\n","    def __init__(self, in_channels, mid_channels, after_relu=False, with_channel=False, BatchNorm=nn.BatchNorm2d):\n","        super(PagFM, self).__init__()\n","        self.with_channel = with_channel\n","        self.after_relu = after_relu\n","        self.f_x = nn.Sequential(\n","                                nn.Conv2d(in_channels, mid_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(mid_channels)\n","                                )\n","        self.f_y = nn.Sequential(\n","                                nn.Conv2d(in_channels, mid_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(mid_channels)\n","                                )\n","        if with_channel:\n","            self.up = nn.Sequential(\n","                                    nn.Conv2d(mid_channels, in_channels,\n","                                              kernel_size=1, bias=False),\n","                                    BatchNorm(in_channels)\n","                                   )\n","        if after_relu:\n","            self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x, y):\n","        input_size = x.size()\n","        if self.after_relu:\n","            y = self.relu(y)\n","            x = self.relu(x)\n","\n","        y_q = self.f_y(y)\n","        y_q = F.interpolate(y_q, size=[input_size[2], input_size[3]],\n","                            mode='bilinear', align_corners=False)\n","        x_k = self.f_x(x)\n","\n","        if self.with_channel:\n","            sim_map = torch.sigmoid(self.up(x_k * y_q))\n","        else:\n","            sim_map = torch.sigmoid(torch.sum(x_k * y_q, dim=1).unsqueeze(1))\n","\n","        y = F.interpolate(y, size=[input_size[2], input_size[3]],\n","                            mode='bilinear', align_corners=False)\n","        x = (1-sim_map)*x + sim_map*y\n","\n","        return x\n","\n","class Light_Bag(nn.Module):\n","    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n","        super(Light_Bag, self).__init__()\n","        self.conv_p = nn.Sequential(\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","        self.conv_i = nn.Sequential(\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","\n","    def forward(self, p, i, d):\n","        edge_att = torch.sigmoid(d)\n","\n","        p_add = self.conv_p((1-edge_att)*i + p)\n","        i_add = self.conv_i(i + edge_att*p)\n","\n","        return p_add + i_add\n","\n","\n","class DDFMv2(nn.Module):\n","    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n","        super(DDFMv2, self).__init__()\n","        self.conv_p = nn.Sequential(\n","                                BatchNorm(in_channels),\n","                                nn.ReLU(inplace=True),\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","        self.conv_i = nn.Sequential(\n","                                BatchNorm(in_channels),\n","                                nn.ReLU(inplace=True),\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","\n","    def forward(self, p, i, d):\n","        edge_att = torch.sigmoid(d)\n","\n","        p_add = self.conv_p((1-edge_att)*i + p)\n","        i_add = self.conv_i(i + edge_att*p)\n","\n","        return p_add + i_add\n","\n","class Bag(nn.Module):\n","    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n","        super(Bag, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","                                BatchNorm(in_channels),\n","                                nn.ReLU(inplace=True),\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=3, padding=1, bias=False)\n","                                )\n","\n","\n","    def forward(self, p, i, d):\n","        edge_att = torch.sigmoid(d)\n","        return self.conv(edge_att*p + (1-edge_att)*i)\n","\n"],"metadata":{"id":"8lOLZdcA1W04","executionInfo":{"status":"ok","timestamp":1736182226215,"user_tz":-60,"elapsed":6,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}}},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":["### PIDNet Definition"],"metadata":{"id":"uiBwR_Yg1dVO"}},{"cell_type":"code","execution_count":57,"metadata":{"id":"dcHkfgpmCUys","executionInfo":{"status":"ok","timestamp":1736182226215,"user_tz":-60,"elapsed":6,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}}},"outputs":[],"source":["# ------------------------------------------------------------------------------\n","# Written by Jiacong Xu (jiacong.xu@tamu.edu)\n","# ------------------------------------------------------------------------------\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","import logging\n","\n","BatchNorm2d = nn.BatchNorm2d\n","bn_mom = 0.1\n","algc = False\n","\n","INPUT_SIZE = (512, 512)\n","\n","class PIDNet(nn.Module):\n","\n","    def __init__(self, m=2, n=3, num_classes=19, planes=64, ppm_planes=96, head_planes=128, augment=True):\n","        super(PIDNet, self).__init__()\n","        self.augment = augment\n","\n","        # I Branch\n","        self.conv1 =  nn.Sequential(\n","                          nn.Conv2d(3,planes,kernel_size=3, stride=2, padding=1),\n","                          BatchNorm2d(planes, momentum=bn_mom),\n","                          nn.ReLU(inplace=True),\n","                          nn.Conv2d(planes,planes,kernel_size=3, stride=2, padding=1),\n","                          BatchNorm2d(planes, momentum=bn_mom),\n","                          nn.ReLU(inplace=True),\n","                      )\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.layer1 = self._make_layer(BasicBlock, planes, planes, m)\n","        self.layer2 = self._make_layer(BasicBlock, planes, planes * 2, m, stride=2)\n","        self.layer3 = self._make_layer(BasicBlock, planes * 2, planes * 4, n, stride=2)\n","        self.layer4 = self._make_layer(BasicBlock, planes * 4, planes * 8, n, stride=2)\n","        self.layer5 =  self._make_layer(Bottleneck, planes * 8, planes * 8, 2, stride=2)\n","\n","        # P Branch\n","        self.compression3 = nn.Sequential(\n","                                          nn.Conv2d(planes * 4, planes * 2, kernel_size=1, bias=False),\n","                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                          )\n","\n","        self.compression4 = nn.Sequential(\n","                                          nn.Conv2d(planes * 8, planes * 2, kernel_size=1, bias=False),\n","                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                          )\n","        self.pag3 = PagFM(planes * 2, planes)\n","        self.pag4 = PagFM(planes * 2, planes)\n","\n","        self.layer3_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n","        self.layer4_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n","        self.layer5_ = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n","\n","        # D Branch\n","        if m == 2:\n","            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes)\n","            self.layer4_d = self._make_layer(Bottleneck, planes, planes, 1)\n","            self.diff3 = nn.Sequential(\n","                                        nn.Conv2d(planes * 4, planes, kernel_size=3, padding=1, bias=False),\n","                                        BatchNorm2d(planes, momentum=bn_mom),\n","                                        )\n","            self.diff4 = nn.Sequential(\n","                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n","                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                     )\n","            self.spp = PAPPM(planes * 16, ppm_planes, planes * 4)\n","            self.dfm = Light_Bag(planes * 4, planes * 4)\n","        else:\n","            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n","            self.layer4_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n","            self.diff3 = nn.Sequential(\n","                                        nn.Conv2d(planes * 4, planes * 2, kernel_size=3, padding=1, bias=False),\n","                                        BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                        )\n","            self.diff4 = nn.Sequential(\n","                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n","                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                     )\n","            self.spp = DAPPM(planes * 16, ppm_planes, planes * 4)\n","            self.dfm = Bag(planes * 4, planes * 4)\n","\n","        self.layer5_d = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n","\n","        # Prediction Head\n","        if self.augment:\n","            self.seghead_p = segmenthead(planes * 2, head_planes, num_classes)\n","            self.seghead_d = segmenthead(planes * 2, planes, 1)\n","\n","        self.final_layer = segmenthead(planes * 4, head_planes, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","\n","    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n","            )\n","\n","        layers = []\n","        layers.append(block(inplanes, planes, stride, downsample))\n","        inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            if i == (blocks-1):\n","                layers.append(block(inplanes, planes, stride=1, no_relu=True))\n","            else:\n","                layers.append(block(inplanes, planes, stride=1, no_relu=False))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _make_single_layer(self, block, inplanes, planes, stride=1):\n","        downsample = None\n","        if stride != 1 or inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n","            )\n","\n","        layer = block(inplanes, planes, stride, downsample, no_relu=True)\n","\n","        return layer\n","\n","    def forward(self, x):\n","\n","        width_output = x.shape[-1] // 8\n","        height_output = x.shape[-2] // 8\n","\n","        h, w = x.size(2), x.size(3)\n","\n","        x = self.conv1(x)\n","        x = self.layer1(x)\n","        x = self.relu(self.layer2(self.relu(x)))\n","        x_ = self.layer3_(x)\n","        x_d = self.layer3_d(x)\n","\n","        x = self.relu(self.layer3(x))\n","        x_ = self.pag3(x_, self.compression3(x))\n","        x_d = x_d + F.interpolate(\n","                        self.diff3(x),\n","                        size=[height_output, width_output],\n","                        mode='bilinear', align_corners=algc)\n","        if self.augment:\n","            temp_p = x_\n","\n","        x = self.relu(self.layer4(x))\n","        x_ = self.layer4_(self.relu(x_))\n","        x_d = self.layer4_d(self.relu(x_d))\n","\n","        x_ = self.pag4(x_, self.compression4(x))\n","        x_d = x_d + F.interpolate(\n","                        self.diff4(x),\n","                        size=[height_output, width_output],\n","                        mode='bilinear', align_corners=algc)\n","        if self.augment:\n","            temp_d = x_d\n","\n","        x_ = self.layer5_(self.relu(x_))\n","        x_d = self.layer5_d(self.relu(x_d))\n","        x = F.interpolate(\n","                        self.spp(self.layer5(x)),\n","                        size=[height_output, width_output],\n","                        mode='bilinear', align_corners=algc)\n","\n","        x_ = self.final_layer(self.dfm(x_, x, x_d))\n","\n","        if self.augment:\n","            x_extra_p = self.seghead_p(temp_p)\n","            x_extra_d = self.seghead_d(temp_d)\n","            return [x_extra_p, x_, x_extra_d]\n","        else:\n","            return x_\n","\n","def get_seg_model(cfg, imgnet_pretrained):\n","\n","    if 's' in cfg.MODEL.NAME:\n","        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=32, ppm_planes=96, head_planes=128, augment=True)\n","    elif 'm' in cfg.MODEL.NAME:\n","        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=96, head_planes=128, augment=True)\n","    else:\n","        model = PIDNet(m=3, n=4, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=112, head_planes=256, augment=True)\n","\n","    if imgnet_pretrained:\n","        pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n","        model_dict = model.state_dict()\n","        pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n","        model_dict.update(pretrained_state)\n","        msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n","        logging.info('Attention!!!')\n","        logging.info(msg)\n","        logging.info('Over!!!')\n","        model.load_state_dict(model_dict, strict = False)\n","    else:\n","        pretrained_dict = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')\n","        if 'state_dict' in pretrained_dict:\n","            pretrained_dict = pretrained_dict['state_dict']\n","        model_dict = model.state_dict()\n","        pretrained_dict = {k[6:]: v for k, v in pretrained_dict.items() if (k[6:] in model_dict and v.shape == model_dict[k[6:]].shape)}\n","        msg = 'Loaded {} parameters!'.format(len(pretrained_dict))\n","        logging.info('Attention!!!')\n","        logging.info(msg)\n","        logging.info('Over!!!')\n","        model_dict.update(pretrained_dict)\n","        model.load_state_dict(model_dict, strict = False)\n","\n","    return model\n","\n","def get_pred_model(name, num_classes):\n","\n","    if 's' in name:\n","        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=False)\n","    elif 'm' in name:\n","        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=False)\n","    else:\n","        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=False)\n","\n","    return model"]},{"cell_type":"markdown","source":["### Discriminator Model"],"metadata":{"id":"BPXkqWOtzUOj"}},{"cell_type":"code","source":["# Remember to upsample the input x before running it through this, as the paper says\n","import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, num_classes):\n","        super(Discriminator, self).__init__()\n","        self.domain_classifier = nn.Sequential(\n","            nn.Conv2d(num_classes, 64, kernel_size=4, stride=2, padding=1),  # Conv1\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # Conv2\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # Conv3\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),  # Conv4\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(512, 1, kernel_size=4, stride=2, padding=1),  # Conv5\n","        )\n","\n","    def forward(self, x):\n","        return self.domain_classifier(x)\n","\n","# Initialize the model with Kaiming initialization\n","def initialize_weights_kaiming(m):\n","    if isinstance(m, nn.Conv2d):\n","        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n","        if m.bias is not None:\n","            init.zeros_(m.bias)"],"metadata":{"id":"XGBkU3KzzTdK","executionInfo":{"status":"ok","timestamp":1736182226215,"user_tz":-60,"elapsed":6,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}}},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":["### Load PIDNet Model"],"metadata":{"id":"isRcFkZ3CxpI"}},{"cell_type":"code","source":["import gdown\n","import tarfile\n","\n","if (os.path.exists(\"./PIDNet_S_ImageNet.pth.tar\") == False):\n","  url = \"https://drive.google.com/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-\"\n","  output = \"./\"\n","  gdown.download(url, output, quiet=False)\n","# Then keep as tar, as it's already the correct format to feed the model\n","\n","# Create a config object with required parameters\n","class Config:\n","    class MODEL:\n","        NAME = 'pidnet_s'  # or 'pidnet_m' or 'pidnet_l'\n","        PRETRAINED = 'PIDNet_S_ImageNet.pth.tar'\n","    class DATASET:\n","        NUM_CLASSES = len(LABEL_MAP)\n","\n","cfg = Config()\n","\n","model = get_seg_model(cfg, imgnet_pretrained=True)\n","# model = get_pred_model('s', len(LABEL_MAP))\n","\n","disc_model = Discriminator(len(LABEL_MAP))\n","disc_model.apply(initialize_weights_kaiming)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ujP_2PffskRk","executionInfo":{"status":"ok","timestamp":1736182226216,"user_tz":-60,"elapsed":7,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}},"outputId":"63d8b595-1360-4d96-9050-9615c685c32d"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-57-c45f6e9ccf7a>:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n"]},{"output_type":"execute_result","data":{"text/plain":["Discriminator(\n","  (domain_classifier): Sequential(\n","    (0): Conv2d(7, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (6): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (8): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","  )\n",")"]},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","source":["### Model Debugging"],"metadata":{"id":"RtcmrSGDcA4w"}},{"cell_type":"code","source":["# import torch.nn.functional as F\n","# from torch.utils.data import DataLoader\n","# import matplotlib.pyplot as plt\n","\n","# train_dataset = LoveDADataset(type='Train')\n","# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n","\n","# model = model.train()\n","# model = model.to('cuda')\n","\n","# for img, mask, _ in train_loader:\n","#     print(f\"iamge shape: {img.shape}\")\n","#     print(f\"mask shape: {mask.shape}\")\n","\n","#     img = img.to('cuda')\n","#     outputs = model(img)\n","\n","#     # bilinear interpolation\n","#     h, w = mask.size(1), mask.size(2)\n","#     ph, pw = outputs[0].size(2), outputs[0].size(3)\n","#     if ph != h or pw != w:\n","#         for i in range(len(outputs)):\n","#             outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear',\n","#                                        align_corners=True)\n","\n","#     for output in outputs:\n","#       print(output.shape)\n","#     break\n","\n","# print(\"===================== Original Image =====================\")\n","# plt.imshow(img[0].permute(1, 2, 0).cpu().numpy())\n","# plt.show()\n","\n","# print(\"===================== Ground Truth =====================\")\n","# plt.imshow(mask[0].cpu().numpy())\n","# plt.show()\n","\n","# print(\"===================== Predicted Mask =====================\")\n","# plt.imshow(torch.argmax(outputs[0][0], dim=0).cpu().numpy())\n","# plt.show()"],"metadata":{"id":"xkTB43MxcDRA","executionInfo":{"status":"ok","timestamp":1736182226216,"user_tz":-60,"elapsed":5,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}}},"execution_count":60,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3DwQcmOWpUxn"},"source":["# Training & Dataset creation"]},{"cell_type":"markdown","source":["### Ablations and Macros"],"metadata":{"id":"iToC6F28cnl5"}},{"cell_type":"code","source":["DEVICE = 'cuda' # 'cuda' or 'cpu'\n","\n","LR = 1e-3            # The initial Learning Rate -- I increased it using quadratic rule in relation with batch size\n","DISC_LR = 1e-3  #1e-4   # The initial Learning Rate for Discriminator\n","MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n","WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n","\n","NUM_EPOCHS = 20      # Total number of training epochs (iterations over dataset)\n","STEP_SIZE = 21       # How many epochs before decreasing learning rate (if using a step-down policy) -- Trying to keep a 2:3 ratio with NUM_EPOCHS\n","DISC_STEP_SIZE = STEP_SIZE\n","# STEP_SIZEs = [21, 14]\n","# DISC_STEP_SIZEs = [14, 21]\n","GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n","\n","LOG_FREQUENCY = 5\n","NUM_CLASSES = len(LABEL_MAP)\n","BATCH_SIZE = 16\n","\n","\n","# LAMBDA_SEGs = [1]\n","LAMBDA_SEG = 1\n","LAMBDA_ADV = 0.001\n","LAMBDA_DISC = 0.1 #0.5"],"metadata":{"id":"moWCCVykx1ac","executionInfo":{"status":"ok","timestamp":1736182226216,"user_tz":-60,"elapsed":5,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}}},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":["### Setup, Create Datasets and DataLoaders. With annexed transforms."],"metadata":{"id":"IwDwW9Hac6SS"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","from albumentations import Compose, HorizontalFlip, RandomRotate90, RandomScale, RandomCrop, GaussNoise, Rotate, Resize, OneOf, Normalize, ColorJitter, GaussianBlur\n","from albumentations.pytorch import ToTensorV2\n","\n","#How big should be the image that we feed to the model?\n","RESIZE = 512\n","# DEFINE TRANSFORMATIONS HERE\n","# To Tensor is not needed since its performed inside the getitem\n","\n","\n","AUGMENTATIONS = {\n","    'Resize': Compose([\n","            Resize(RESIZE, RESIZE),\n","    ], additional_targets={\"boundaries\": \"mask\"}),\n","    'Normalize': Compose([\n","            Normalize(mean=(123.675, 116.28, 103.53), std=(58.395, 57.12, 57.375), max_pixel_value=1.0, always_apply=True),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomCrop256': Compose([\n","            RandomCrop(256, 256),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomCrop512': Compose([\n","            RandomCrop(512, 512),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomCrop512+Normalization' : Compose([\n","            Normalize(mean=(123.675, 116.28, 103.53), std=(58.395, 57.12, 57.375), max_pixel_value=1.0, always_apply=True),\n","            RandomCrop(512, 512),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomCropOrResize': Compose([\n","            OneOf([\n","                RandomCrop(RESIZE, RESIZE, p=0.5),  # Random crop to resize\n","                Resize(RESIZE, RESIZE, p=0.5)\n","            ], p=1)\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomCropXXX': Compose([\n","            OneOf([\n","                RandomCrop(256, 256),  # Random crop to resize\n","                RandomCrop(512, 512),\n","            ], p=1)\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'Jitter': Compose([\n","            ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'NormalizeOnRural': Compose([\n","            Normalize(mean=(73.532, 80.017, 74.593), std=(41.493, 35.653, 33.747), max_pixel_value=1.0, always_apply=True),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'GaussianBlur': Compose([\n","            GaussianBlur(p=0.5),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","}\n","\n","CHOOSE_TRANSFORM = 'Resize'\n","AUGTRANSFORM = None\n","transforms = AUGMENTATIONS[CHOOSE_TRANSFORM]\n","\n","## Dataset and Loader\n","# Training Sets and Loaders\n","source_dataset = LoveDADataset(baseTransform=AUGMENTATIONS[CHOOSE_TRANSFORM], augTransforms=AUGTRANSFORM, split='Urban', type='Train', validation_ratio=0.2)\n","source_loader = DataLoader(source_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True, pin_memory=True)\n","\n","target_dataset = LoveDADataset(baseTransform=AUGMENTATIONS[CHOOSE_TRANSFORM], augTransforms=AUGTRANSFORM, split='Rural', type='Train', validation_ratio=0.2, useBoundaries=False)\n","target_loader = DataLoader(target_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True, pin_memory=True)\n","\n","# Validation Sets and Loaders\n","source_validation_dataset = LoveDADataset(baseTransform=AUGMENTATIONS[CHOOSE_TRANSFORM], augTransforms=AUGTRANSFORM, split='Urban', type='Validation', validation_ratio=0.2, useBoundaries=False)\n","source_validation_loader = DataLoader(source_validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True, pin_memory=True)\n","\n","target_validation_dataset = LoveDADataset(baseTransform=AUGMENTATIONS[CHOOSE_TRANSFORM], augTransforms=AUGTRANSFORM, split='Rural', type='Validation', validation_ratio=0.2, useBoundaries=False)\n","target_validation_loader = DataLoader(target_validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True, pin_memory=True)\n","\n","## Model is defined some cells above, in LoadPidNetModel\n","drive_path = '/content/drive/MyDrive/LoveDA'"],"metadata":{"id":"J8DhNU7Lc7aD","executionInfo":{"status":"ok","timestamp":1736182226216,"user_tz":-60,"elapsed":5,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2cc26e81-60eb-463c-f095-25d9780532ef"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset size: 924\n","Dataset size: 1092\n","Dataset size: 232\n","Dataset size: 274\n"]}]},{"cell_type":"markdown","source":["### Losses"],"metadata":{"id":"l18jlCwmiTZp"}},{"cell_type":"code","source":["def weighted_bce(bd_pre, target):\n","    n, c, h, w = bd_pre.size()\n","    log_p = bd_pre.permute(0,2,3,1).contiguous().view(1, -1)\n","    target_t = target.view(1, -1)\n","\n","    pos_index = (target_t == 1)\n","    neg_index = (target_t == 0)\n","\n","    weight = torch.zeros_like(log_p)\n","    pos_num = pos_index.sum()\n","    neg_num = neg_index.sum()\n","    sum_num = pos_num + neg_num\n","    weight[pos_index] = neg_num * 1.0 / sum_num\n","    weight[neg_index] = pos_num * 1.0 / sum_num\n","\n","    loss = F.binary_cross_entropy_with_logits(log_p, target_t, weight, reduction='mean')\n","\n","    return loss\n","\n","def boundary_loss(bd_pre, bd_gt):\n","    loss = 20.0 * weighted_bce(bd_pre, bd_gt)\n","    return loss\n","\n","# TODO EXTRA add weights=class_weights to nn.CrossEntropyLoss()\n","# TODO EXTRA use OHCE instead of basic one\n","def cross_entropy(score, target):\n","    compute_ce_loss = nn.CrossEntropyLoss(ignore_index=-1)\n","\n","    # See paper for weights. In order of loss index: (0.4, 20, 1, 1) # But on cfg they set everything to 0.5\n","    balance_weights = [0.4, 1]\n","    sb_weights = 1\n","\n","    # print(f\"DEBUG: inside cross_entropy: len(score) = {len(score)}\")\n","    if len(balance_weights) == len(score):\n","        return sum([w * compute_ce_loss(x, target) for (w, x) in zip(balance_weights, score)])\n","    elif len(score) == 1:\n","        return sb_weights * compute_ce_loss(score[0], target)\n","    else:\n","        raise ValueError(\"lengths of prediction and target are not identical!\")\n","\n","sem_loss = cross_entropy\n","bd_loss = boundary_loss\n","bce_loss = torch.nn.BCEWithLogitsLoss()"],"metadata":{"id":"tU1zr5WKiU0y","executionInfo":{"status":"ok","timestamp":1736182227218,"user_tz":-60,"elapsed":2,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}}},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":["### Training Loop"],"metadata":{"id":"_rRjwVYaaFG8"}},{"cell_type":"code","execution_count":64,"metadata":{"id":"Y8nlqvfeFpg0","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1OM7_ypfYusizDZmTlbi-wX8S-vf1xAPh"},"outputId":"2bff920e-53ba-4f2a-dab2-3fa606a2d730","collapsed":true,"executionInfo":{"status":"ok","timestamp":1736183829873,"user_tz":-60,"elapsed":1308008,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import torch\n","import numpy as np\n","\n","# For easier debugging\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","os.environ['TORCH_USE_CUDA_DSA'] = '1'\n","\n","# STEP_SIZEs = [21, 14]\n","# DISC_STEP_SIZEs = [14, 21]\n","\n","LOAD_BASE_MODELs = [True]\n","model_name = 'best_model_Resize.pth'\n","\n","for LOAD_BASE_MODEL in LOAD_BASE_MODELs:\n","    ## If you want to UPLOAD an existing model for re-training\n","    if LOAD_BASE_MODEL:\n","        #model_path = os.path.join(drive_path, model_name)\n","        #best_model = torch.load(model_path, map_location=DEVICE)\n","        best_model = torch.load(f'/content/{model_name}', weights_only=True)\n","        model.load_state_dict(best_model)\n","\n","    ## Optimizier and Scheduler\n","    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n","\n","    disc_optimizer = optim.Adam(disc_model.parameters(), lr=DISC_LR, betas=(0.9, 0.99))\n","    disc_scheduler = optim.lr_scheduler.StepLR(disc_optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n","\n","    ## Models Names\n","    SAVE_MODEL_AS =        f'best_DA_model_{CHOOSE_TRANSFORM}__PRETRN={LOAD_BASE_MODEL}__optepch__LS={LAMBDA_SEG}_LA={LAMBDA_ADV}_LD={LAMBDA_DISC}__LR={STEP_SIZE}_dscLR={DISC_STEP_SIZE}.pth'\n","    SAVE_DISC_MODEL_AS = f'best_disc_model_{CHOOSE_TRANSFORM}__PRETRN={LOAD_BASE_MODEL}__optepch__LS={LAMBDA_SEG}_LA={LAMBDA_ADV}_LD={LAMBDA_DISC}__LR={STEP_SIZE}_dscLR={DISC_STEP_SIZE}.pth'\n","    MODEL_NAME = SAVE_MODEL_AS\n","\n","    current_step = 0\n","\n","    ## Prepare to save model and params which proves to be best\n","    best_loss = float('inf')\n","    best_model = model.state_dict()\n","    disc_best_model = disc_model.state_dict()\n","\n","    model = model.to(DEVICE)\n","    disc_model = disc_model.to(DEVICE)\n","    print(f\"DEVICE is {DEVICE}. Model name is {SAVE_MODEL_AS}\")\n","\n","    ## Start the training\n","    if TYPE == 'Train':\n","        for epoch in range(NUM_EPOCHS):\n","            model.train()\n","            disc_model.train()\n","\n","            optimizer.zero_grad()\n","            disc_optimizer.zero_grad()\n","\n","            epoch_losses = {\n","                'loss_complete': 0.0,\n","                'amount': 0, # amt of images I think?\n","                'loss_pidnet': 0.0,\n","                'loss_adv': 0.0,\n","                'loss_disc': 0.0,\n","                'loss_pidnet_wtd': 0.0,\n","                'loss_adv_wtd': 0.0,\n","                'loss_disc_wtd': 0.0,\n","            }\n","\n","            print('Starting epoch {}/{}, LR = {}, DISC_LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr(), disc_scheduler.get_lr()))\n","            srcldr_len = len(source_loader)\n","            tgtldr_len = len(target_loader)\n","            print(f\"len of source_loader is {srcldr_len}, len of target_loader is {tgtldr_len}. WARNING: make sure source is shorter than target, or modify code to fix early stopping\")\n","\n","            # ================================== Train Epoch X ==================================\n","            for batch_i, (batch_s, batch_t) in tqdm(enumerate(zip(source_loader, target_loader)), total=len(source_loader)):\n","                ### Extract input\n","                image_list_source, masks_list_source, img_path_source, bd_gts_list_source = batch_s\n","                image_list_target, _, _, _ = batch_t\n","\n","                index = 0\n","                for images, masks, bd_gts, images_t in zip(image_list_source, masks_list_source, bd_gts_list_source, image_list_target):\n","                    ### Extract Images, Masks, Boundaries\n","                    # Source\n","                    images = images.to(DEVICE)\n","                    masks = masks.to(DEVICE)\n","                    bd_gts = bd_gts.float().to(DEVICE)\n","                    # print(f\"DEBUG: source_image batch shape: {images.shape}\")\n","                    # print(f\"DEBUG: source_mask batch shape: {masks.shape}\")\n","\n","                    # Target\n","                    images_t = images_t.to(DEVICE)\n","                    # print(f\"DEBUG: target_image batch shape: {images.shape}\")\n","\n","                    ### ReEnable optimization on backbone (***PIDNet***)\n","                    for params in model.parameters():\n","                        params.requires_grad = True\n","\n","                    ### Disable ***Discriminator*** optimization for the first part\n","                    for param in disc_model.parameters():\n","                        param.requires_grad = False\n","\n","\n","                    ### ===> Train Segmentation --- on Source\n","                    ## Forward\n","                    outputs = model(images) # in model.train() mode batch size must be > 1 I think\n","                                            # NOTE: we have 3 heads (i.e. 3 outputs) but 4 losses: 2nd head is used for both S and BAS\n","\n","                    ## Upscale (bilinear interpolation - not learned)\n","                    h, w = masks.size(1), masks.size(2)\n","                    ph, pw = outputs[0].size(2), outputs[0].size(3)\n","                    if ph != h or pw != w:\n","                        for i in range(len(outputs)):\n","                            outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n","\n","                    ## Losses\n","                    # Semantic Losses (l_0 and l_2)\n","                    loss_s = sem_loss(outputs[:-1], masks) # output #1 and #2 are segmentation predictions (i.e. low level (P) and high+low level (PI) respectively)\n","\n","                    # Boundary Loss (l_1)\n","                    loss_b = bd_loss(outputs[-1], bd_gts) # output #3 is the boundary prediction\n","\n","                    # Boundary AwareneSS (BAS) Loss (l_3)\n","                    filler = torch.ones_like(masks) * -1\n","                    bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:]) > 0.8, masks, filler)\n","                                        # REMEMBER to wrap in list, as the checks in ce use that to know what to do\n","                    loss_sb = sem_loss([outputs[-2]], bd_label) # output #2 is the PI segmentation prediction, done here in BAS mode (see `filler` variable)\n","\n","                    # Segmentation Loss w/o DA (on source domain, which has labels)\n","                    loss_pidnet = loss_s + loss_b + loss_sb  # The coefficients of the sum of the four losses (0.4, 20, 1, 1) are taken into account in the various `sem_loss` and `bd_loss`\n","                    if batch_i % LOG_FREQUENCY == 0:\n","                        print(f'\\n[losses are unscaled] loss_pidnet at batch {batch_i}: {loss_pidnet.item()}', end=\" --- \")\n","                    epoch_losses['loss_pidnet'] += loss_pidnet.item()\n","\n","                    ## Backprop\n","                    loss = loss_pidnet * LAMBDA_SEG #/ srcldr_len\n","                    epoch_losses['loss_complete'] += loss.item()\n","                    epoch_losses['loss_pidnet_wtd'] += loss.item()\n","                    loss.backward()\n","                    ### <=== Train Segmentation --- on Source\n","\n","\n","                    ### ===> Train Segmentation --- on Target\n","                    # note that we don't want to be optimizing the Discriminator yet\n","                    ## Forward\n","                    outputs_t = model(images_t)\n","                    # print(f\"DEBUG: there are {len(outputs_t)} outputs for output_t\")\n","                    # for output in outputs_t:\n","                    #     print(f\"DEBUG: output shape is {output.shape}\")\n","\n","                    ## Upscale\n","                    h, w = images_t.size(2), images_t.size(3)\n","                    ph, pw = outputs_t[0].size(2), outputs_t[0].size(3)\n","                    output_t = F.interpolate(outputs_t[-2], size=(h, w), mode='bilinear', align_corners=True)\n","                    # print(f\"DEBUG: shape of interpolated main Head output_t is {output_t.shape}, as h={h}, w={w}, ph={ph}, pw={pw}\")\n","\n","                    ## Discriminate\n","                    disc_output_t = disc_model(output_t)\n","\n","                    ## Adversarial Loss (bce between disc_output_t and source_label (which is z=0)) # TODO: check this z=0 thing, as in the paper it states to opposite than in the repo\n","                    loss_adv = bce_loss(disc_output_t, torch.zeros_like(disc_output_t)).to(DEVICE)\n","                    if batch_i % LOG_FREQUENCY == 0:\n","                        print(f'loss_adv at batch {batch_i}: {loss_adv.item()}', end=\" --- \")\n","                    epoch_losses['loss_adv'] += loss_adv.item()\n","\n","                    ## Backprop\n","                    loss = loss_adv * LAMBDA_ADV #/ srcldr_len\n","                    epoch_losses['loss_complete'] += loss.item()\n","                    epoch_losses['loss_adv_wtd'] += loss.item()\n","                    loss.backward()\n","                    ### <=== Train Segmentation --- on Target\n","\n","\n","                    ### ===> Train Discriminator\n","                    ## Enable Optimization for ***Discriminator***\n","                    for param in disc_model.parameters():\n","                        param.requires_grad = True\n","\n","                    ## Disable Optimization on backbone (***PIDNet***)\n","                    output = outputs[1].detach()\n","                    output_t = output_t.detach()\n","\n","                    ## Discriminate\n","                    disc_output = disc_model(output)\n","                    disc_output_t = disc_model(output_t)\n","\n","                    ## Discriminative Loss (z=0 for source domain z=1 for target domain) # TODO: check this z=0 and z=1 thing, as in the paper it states to opposite than in the repo\n","                    loss_disc =  bce_loss(disc_output, torch.zeros_like(disc_output)).to(DEVICE)\n","                    loss_disc += bce_loss(disc_output_t, torch.ones_like(disc_output_t)).to(DEVICE)\n","\n","                    if batch_i % LOG_FREQUENCY == 0:\n","                        print(f'loss_disc at batch {batch_i}: {loss_disc.item()}')\n","                    epoch_losses['loss_disc'] += loss_disc.item()\n","\n","                    ## Backprop\n","                    loss = loss_disc * LAMBDA_DISC #/ srcldr_len\n","                    epoch_losses['loss_complete'] += loss.item()\n","                    epoch_losses['loss_disc_wtd'] += loss.item()\n","                    loss.backward()\n","                    ### <=== Train Discriminator\n","\n","                    epoch_losses['amount'] += images.size(0)\n","\n","            optimizer.step()\n","            disc_optimizer.step()\n","\n","            # ================================== Evaluate Main Model ==================================\n","            # Evaluate model on the evaluation set and save the parameters if is better than best model\n","            model.eval()\n","            disc_model.eval()\n","            total_loss = 0.0\n","            epoch_losses_eval = {\n","            'loss_complete': 0.0,\n","            'amount': 0, # amt of images I think?\n","            'loss_pidnet': 0.0,\n","            'loss_adv': 0.0,\n","            'loss_disc': 0.0,\n","            'loss_pidnet_wtd': 0.0,\n","            'loss_adv_wtd': 0.0,\n","            'loss_disc_wtd': 0.0,\n","            }\n","\n","            outputs = []\n","            with torch.no_grad():\n","                for batch_i, (batch_s, batch_t) in tqdm(enumerate(zip(source_validation_loader, target_validation_loader))):\n","                    ### Extract input\n","                    images, masks, _, bd_gts = batch_s\n","                    images_t, _, _, _ = batch_t\n","\n","                    ### Extract Images, Masks, Boundaries\n","                    # Source\n","                    images = images.to(DEVICE)\n","                    masks = masks.to(DEVICE)\n","                    bd_gts = bd_gts.float().to(DEVICE)\n","                    # print(f\"DEBUG: source_image batch shape: {images.shape}\")\n","                    # print(f\"DEBUG: source_mask batch shape: {masks.shape}\")\n","\n","                    # Target\n","                    images_t = images_t.to(DEVICE)\n","                    # print(f\"DEBUG: target_image batch shape: {images.shape}\")\n","\n","                    ### ===> Train Segmentation --- on Source\n","                    ## Forward\n","                    outputs = model(images) # in model.train() mode batch size must be > 1 I think\n","                                            # NOTE: we have 3 heads (i.e. 3 outputs) but 4 losses: 2nd head is used for both S and BAS\n","\n","                    ## Upscale (bilinear interpolation - not learned)\n","                    h, w = masks.size(1), masks.size(2)\n","                    ph, pw = outputs[0].size(2), outputs[0].size(3)\n","                    if ph != h or pw != w:\n","                        for i in range(len(outputs)):\n","                            outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n","\n","                    ## Losses\n","                    # Semantic Losses (l_0 and l_2)\n","                    loss_s = sem_loss(outputs[:-1], masks) # output #1 and #2 are segmentation predictions (i.e. low level (P) and high+low level (PI) respectively)\n","\n","                    # Boundary Loss (l_1)\n","                    loss_b = bd_loss(outputs[-1], bd_gts) # output #3 is the boundary prediction\n","\n","                    # Boundary AwareneSS (BAS) Loss (l_3)\n","                    filler = torch.ones_like(masks) * -1\n","                    bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:]) > 0.8, masks, filler)\n","                                        # REMEMBER to wrap in list, as the checks in ce use that to know what to do\n","                    loss_sb = sem_loss([outputs[-2]], bd_label) # output #2 is the PI segmentation prediction, done here in BAS mode (see `filler` variable)\n","\n","                    # Segmentation Loss w/o DA (on source domain, which has labels)\n","                    loss_pidnet = loss_s + loss_b + loss_sb  # The coefficients of the sum of the four losses (0.4, 20, 1, 1) are taken into account in the various `sem_loss` and `bd_loss`\n","                    if batch_i % LOG_FREQUENCY == 0:\n","                        print(f'\\n[losses are unscaled] loss_pidnet at batch {batch_i}: {loss_pidnet.item()}', end=\" --- \")\n","                    epoch_losses_eval['loss_pidnet'] += loss_pidnet.item()\n","\n","                    ## Log losses\n","                    loss = loss_pidnet * LAMBDA_SEG\n","                    epoch_losses_eval['loss_complete'] += loss.item()\n","                    epoch_losses_eval['loss_pidnet_wtd'] += loss.item()\n","\n","                    ### <=== Train Segmentation --- on Source\n","\n","\n","                    ### ===> Train Segmentation --- on Target\n","                    # note that we don't want to be optimizing the Discriminator yet\n","                    ## Forward\n","                    outputs_t = model(images_t)\n","                    # print(f\"DEBUG: there are {len(outputs_t)} outputs for output_t\")\n","                    # for output in outputs_t:\n","                    #     print(f\"DEBUG: output shape is {output.shape}\")\n","\n","                    ## Upscale\n","                    h, w = images_t.size(2), images_t.size(3)\n","                    ph, pw = outputs_t[0].size(2), outputs_t[0].size(3)\n","                    output_t = F.interpolate(outputs_t[-2], size=(h, w), mode='bilinear', align_corners=True)\n","                    # print(f\"DEBUG: shape of interpolated main Head output_t is {output_t.shape}, as h={h}, w={w}, ph={ph}, pw={pw}\")\n","\n","                    ## Discriminate\n","                    disc_output_t = disc_model(output_t)\n","\n","                    ## Adversarial Loss (bce between disc_output_t and source_label (which is z=0)) # TODO: check this z=0 thing, as in the paper it states to opposite than in the repo\n","                    loss_adv = bce_loss(disc_output_t, torch.zeros_like(disc_output_t)).to(DEVICE)\n","                    if batch_i % LOG_FREQUENCY == 0:\n","                        print(f'loss_adv at batch {batch_i}: {loss_adv.item()}', end=\" --- \")\n","                    epoch_losses_eval['loss_adv'] += loss_adv.item()\n","\n","                    ## Log losses\n","                    loss = loss_adv * LAMBDA_ADV\n","                    epoch_losses_eval['loss_complete'] += loss.item()\n","                    epoch_losses_eval['loss_adv_wtd'] += loss.item()\n","\n","                    ### <=== Train Segmentation --- on Target\n","\n","\n","                    ### ===> Train Discriminator\n","                    ## Discriminate\n","                    disc_output = disc_model(output)\n","                    disc_output_t = disc_model(output_t)\n","\n","                    ## Discriminative Loss (z=0 for source domain z=1 for target domain) # TODO: check this z=0 and z=1 thing, as in the paper it states to opposite than in the repo\n","                    loss_disc =  bce_loss(disc_output, torch.zeros_like(disc_output)).to(DEVICE)\n","                    loss_disc += bce_loss(disc_output_t, torch.ones_like(disc_output_t)).to(DEVICE)\n","                    if batch_i % LOG_FREQUENCY == 0:\n","                        print(f'loss_disc at batch {batch_i}: {loss_disc.item()}')\n","                    epoch_losses_eval['loss_disc'] += loss_disc.item()\n","\n","                    ## Log losses\n","                    loss = loss_disc * LAMBDA_DISC\n","                    epoch_losses_eval['loss_complete'] += loss.item()\n","                    epoch_losses_eval['loss_disc_wtd'] += loss.item()\n","\n","                    ### <=== Train Discriminator\n","\n","                    epoch_losses_eval['amount'] += images.size(0)\n","                    # Complete loss (loss_domain contains Loss adv and Loss discrimnation)\n","                    loss_complete = loss_pidnet + loss_adv + loss_disc\n","\n","                    total_loss += loss_complete.item()\n","\n","            print('Epoch {}, Loss {}'.format(epoch+1, total_loss))\n","            if total_loss < best_loss:\n","                best_loss = total_loss\n","                best_model = model.state_dict()\n","                #Save in Drive and local\n","                torch.save(best_model, SAVE_MODEL_AS)\n","                print(\"===========================================\")\n","                print(\"BEST MODEL ON VALIDATION SET, SAVING\")\n","                if SAVE_ON_DRIVE:\n","                    !cp {SAVE_MODEL_AS} /content/drive/MyDrive/LoveDA/{SAVE_MODEL_AS}\n","                    print(f\"{SAVE_MODEL_AS} model succesfully saved on drive. loss went down to {best_loss}\")\n","\n","            if epoch >= 15:\n","                torch.save(best_model, f'{MODEL_NAME}_{epoch+1}.pth')# Save model\n","                if SAVE_ON_DRIVE:\n","                    EPOCH_MODEL_PATH = f'{SAVE_MODEL_AS}_{epoch+1}.pth'\n","                    !cp {EPOCH_MODEL_PATH} /content/drive/MyDrive/LoveDA/{EPOCH_MODEL_PATH}\n","                    print(f\"{EPOCH_MODEL_PATH} succesfully saved on drive. loss went down to {best_loss}\")\n","\n","            current_step += 1\n","            scheduler.step()\n","            disc_scheduler.step()\n","\n","            amt = epoch_losses['amount']\n","            print(f'[EPOCH {epoch+1:02}] Avg. Losses:')\n","            print(f'{\"loss_complete\":<20} {epoch_losses[\"loss_complete\"] / amt:.6f}')\n","            print(f'{\"loss_pidnet\":<20} {epoch_losses[\"loss_pidnet\"] / amt:.6f}')\n","            print(f'{\"loss_adv\":<20} {epoch_losses[\"loss_adv\"] / amt:.6f}')\n","            print(f'{\"loss_disc\":<20} {epoch_losses[\"loss_disc\"] / amt:.6f}')\n","            print(f'{\"loss_pidnet_wtd\":<20} {epoch_losses[\"loss_pidnet_wtd\"] / amt:.6f}')\n","            print(f'{\"loss_adv_wtd\":<20} {epoch_losses[\"loss_adv_wtd\"] / amt:.6f}')\n","            print(f'{\"loss_disc_wtd\":<20} {epoch_losses[\"loss_disc_wtd\"] / amt:.6f}')\n","            print(\"========================================================================\")\n","            print(\"Validation\")\n","            amt = epoch_losses_eval['amount']\n","            print(f'[EPOCH {epoch+1:02}] Avg. Losses:')\n","            print(f'{\"loss_complete\":<20} {epoch_losses_eval[\"loss_complete\"] / amt:.6f}')\n","            print(f'{\"loss_pidnet\":<20} {epoch_losses_eval[\"loss_pidnet\"] / amt:.6f}')\n","            print(f'{\"loss_adv\":<20} {epoch_losses_eval[\"loss_adv\"] / amt:.6f}')\n","            print(f'{\"loss_disc\":<20} {epoch_losses_eval[\"loss_disc\"] / amt:.6f}')\n","            print(f'{\"loss_pidnet_wtd\":<20} {epoch_losses_eval[\"loss_pidnet_wtd\"] / amt:.6f}')\n","            print(f'{\"loss_adv_wtd\":<20} {epoch_losses_eval[\"loss_adv_wtd\"] / amt:.6f}')\n","            print(f'{\"loss_disc_wtd\":<20} {epoch_losses_eval[\"loss_disc_wtd\"] / amt:.6f}')\n","            print(\"========================================================================\")\n","\n","            # Create a figure with 1 row and 3 columns\n","            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","            # Plot the original image\n","            axes[0].imshow(images[0].permute(1, 2, 0).cpu().numpy())\n","            axes[0].set_title(\"Original Image\")\n","            axes[0].axis('off')\n","\n","            # Plot the ground truth mask\n","            # axes[1].imshow(masks[0].cpu().numpy())\n","            # axes[1].set_title(\"Ground Truth\")\n","            # axes[1].axis('off')\n","\n","            # # Show mask\n","            adapted_mask = new_colors_mask(masks[0].cpu().numpy())\n","            plt.legend(handles=getLegendHandles(), loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n","            axes[1].imshow(adapted_mask)\n","            axes[1].set_title(\"Ground Truth\")\n","            axes[1].axis('off')\n","\n","            # Plot the predicted mask\n","            # axes[2].imshow(torch.argmax(outputs[0][0], dim=0).cpu().numpy())\n","            # axes[2].set_title(\"Predicted Mask\")\n","            # axes[2].axis('off')\n","\n","            adapted_mask_2 = new_colors_mask(torch.argmax(outputs[0][0], dim=0).cpu().numpy())\n","            plt.legend(handles=getLegendHandles(), loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n","            axes[2].imshow(adapted_mask_2)\n","            axes[2].set_title(\"Predicted Mask\")\n","            axes[2].axis('off')\n","\n","            # Display the figure\n","            plt.tight_layout()\n","            plt.show()\n","\n","            # ================================== Evaluate Discriminator Model ==================================\n","            # TODO !!!!!!!!!!!!\n","            # just run the discriminator on couples of source/target images and evaluate the prediction being closer to 0/1 (0=source, 1=target)\n","\n"]},{"cell_type":"code","source":["# # Save model\n","# epoch = 19\n","\n","# LAMBDA_SEGs = [1]\n","\n","# for LAMBDA_SEG in LAMBDA_SEGs:\n","#     SAVE_MODEL_AS = f'best_DA_model_{CHOOSE_TRANSFORM}_LS={LAMBDA_SEG}_LA={LAMBDA_ADV}_LD={LAMBDA_DISC}.pth'\n","#     EPOCH_MODEL_PATH = f'{SAVE_MODEL_AS}_{epoch+1}.pth'\n","\n","#     !cp {EPOCH_MODEL_PATH} /content/drive/MyDrive/LoveDA/{EPOCH_MODEL_PATH}\n","#     print(f\"{EPOCH_MODEL_PATH} succesfully saved on drive. loss went down to {best_loss}\")"],"metadata":{"id":"0pRuSB9wcI_G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736183829887,"user_tz":-60,"elapsed":0,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}},"outputId":"ce13b43a-3452-4f41-a786-a11ebbb8fcd0"},"execution_count":65,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["cp: cannot stat 'best_DA_model_Resize_LS=1_LA=0.001_LD=0.1.pth_20.pth': No such file or directory\n","best_DA_model_Resize_LS=1_LA=0.001_LD=0.1.pth_20.pth succesfully saved on drive. loss went down to 144.99949264526367\n"]}]},{"cell_type":"markdown","source":["# TEST"],"metadata":{"id":"soROYpcYyajp"}},{"cell_type":"code","source":["!pip install torchmetrics ptflops"],"metadata":{"id":"7DKgjGPKydLH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736183829874,"user_tz":-60,"elapsed":8,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}},"outputId":"ab3bc758-9757-4108-b209-dbab7d895548"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.6.1)\n","Requirement already satisfied: ptflops in /usr/local/lib/python3.10/dist-packages (0.7.4)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"]}]},{"cell_type":"code","source":["from torchmetrics import Accuracy\n","from tqdm import tqdm\n","import time\n","import ptflops\n","import os\n","\n","DEVICE = 'cuda'\n","TYPE = 'Test'\n","# WARNING: YOU MAY DECIDE TO OVVERIDE THE CHOOSE_TRANSFORM, BUT BE CAREFUL. BE SURE THE MODEL YOU'RE USING IS THE CORRECT ONE\n","CHOOSE_TRANSFORM = 'Resize'\n","\n","# Create unweighted models\n","model = get_seg_model(cfg, imgnet_pretrained=False)\n","# disc_model = Discriminator(len(LABEL_MAP))\n","\n","# Load weights\n","model_name_pattern = 'best_DA_model' # for starters, use 'best_DA_model' for this step, and just 'best_model' for non DA models\n","disc_model_name_pattern = 'best_disc_model'\n","\n","# find filenames that start with model_name_pattern using os\n","model_files_paths = [f for f in os.listdir('.') if f.startswith(model_name_pattern)]\n","# disc_model_files_paths = [f for f in os.listdir('.') if f.startswith(disc_model_name_pattern)]\n","print(model_files_paths)\n","\n","TEST_ONLY_ON_BEST = False\n","\n","for model_file_path in model_files_paths:\n","    best_model = torch.load(model_file_path, weights_only=True)\n","    # best_disc_model = torch.load(disc_model_file_path, weights_only=True)\n","\n","    model.load_state_dict(best_model)\n","    model = model.to(DEVICE)\n","    # disc_model.load_state_dict(best_disc_model)\n","    # disc_model = disc_model.to(DEVICE)\n","\n","    accuracy, mIoU = True, True\n","\n","    TARGETs = ['Urban', 'Rural']\n","\n","    for TARGET in TARGETs:\n","        if TARGET == 'Urban': # Here we just validate on less images if URBAN, as it's not the focus of step 3b. Use Train instead of Validation for a 0.8 split instead of 0.2\n","            target_type = 'Validation'\n","        elif TARGET == 'Rural': # While we take the entirety of the Rural folder in case of Rural\n","            #target_type = 'ActualTest'\n","            target_type = 'Validation'\n","        else:\n","            raise ValueError(\"TARGET must be 'Urban' or 'Rural'\")\n","\n","\n","\n","        if CHOOSE_TRANSFORM == 'RandomCropOrResize':\n","            test_augmentation = AUGMENTATIONS['RandomCrop512']\n","        elif CHOOSE_TRANSFORM == 'RandomCropXXX':\n","            test_augmentation = AUGMENTATIONS['None']\n","        elif CHOOSE_TRANSFORM == 'Jitter':\n","            test_augmentation = AUGMENTATIONS['None']\n","        elif CHOOSE_TRANSFORM == 'GaussianBlur':\n","            test_augmentation = AUGMENTATIONS['None']\n","        else:\n","            test_augmentation = AUGMENTATIONS[CHOOSE_TRANSFORM]\n","\n","        test_dataset = LoveDADataset(baseTransform=test_augmentation, augTransforms=None, split=TARGET, type=target_type, useBoundaries=False)\n","        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True, pin_memory=True)\n","\n","        #### TEST LOOP\n","        model.eval()\n","        print(f\"Testing model={model_file_path} on domain={TARGET} on a {target_type} split\")\n","\n","        # Latency\n","        with torch.no_grad():\n","            start_time = time.time()\n","            for _ in range(100):\n","                _ = model(torch.randn(1, 3, RESIZE, RESIZE).to(DEVICE))\n","            end_time = time.time()\n","        latency = (end_time - start_time) / 100\n","        print(f\"Latency: {latency:.4f} seconds\")\n","\n","        # FLOPs\n","        macs, _ = ptflops.get_model_complexity_info(model,\n","            (3, RESIZE, RESIZE), as_strings=False,\n","            print_per_layer_stat=False, verbose=False)\n","        flops = macs * 2  # MACs perform two FLOPs\n","        print(\"FLOPs:\", flops)\n","\n","        # Number of parameters\n","        total_params = sum(p.numel() for p in model.parameters())\n","        print(f\"Total number of parameters: {total_params}\")\n","\n","        if TYPE == 'Test':\n","            with torch.no_grad():\n","                total_union = torch.zeros(NUM_CLASSES).to(DEVICE)\n","                total_intersection = torch.zeros(NUM_CLASSES).to(DEVICE)\n","                meter = Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(DEVICE)\n","                for (batch) in tqdm(test_loader):\n","                    ### Extract input\n","                    images, masks, img_path, bd_gts = batch\n","                    images = images.float().to(DEVICE)\n","                    masks = masks.to(DEVICE)\n","\n","                    ### ===> Forward, Upscale, Compute Losses\n","                    ## Forward\n","                    outputs = model(images)\n","\n","                    ## Upscale (bilinear interpolation - not learned)\n","                    h, w = masks.size(1), masks.size(2)\n","                    ph, pw = outputs[0].size(2), outputs[0].size(3)\n","                    if ph != h or pw != w:\n","                        for i in range(len(outputs)):\n","                            outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n","\n","                    # Output 1 is the prediction\n","\n","                    # Shape: NBATCHES x classes x h x w\n","                    class_indices = torch.argmax(outputs[1], dim=1)  # Shape: NBATCHES x h x w\n","\n","                    if accuracy:\n","                    # Create a mask for valid targets (where target is not -1)\n","                        valid_mask = (masks != -1)  # Mask of shape: NBATCHES x h x w\n","                        # Apply the mask to ignore -1 targets when updating the accuracy metric\n","                        meter.update(class_indices[valid_mask], masks[valid_mask])\n","\n","                    if mIoU:\n","                        for predicted, target in zip(class_indices, masks):\n","                            for i in range(NUM_CLASSES):\n","                                total_intersection[i] += torch.sum(torch.logical_and(predicted == i, target == i))\n","                                total_union[i] += torch.sum(torch.logical_or(predicted == i, target == i))\n","\n","        if accuracy:\n","            accuracy = meter.compute()\n","            print(f'\\nAccuracy on the target domain: {100 * accuracy:.2f}%')\n","\n","        if mIoU:\n","            intersection_over_union = total_intersection / total_union\n","\n","            # Per class IoU\n","            for i, iou in enumerate(intersection_over_union):\n","                class_name = list(LABEL_MAP.keys())[list(LABEL_MAP.values()).index(i)]  # Get the class name from LABEL_MAP\n","                print(f'{class_name} IoU: {iou:.4f}')\n","\n","            mIoU = torch.mean(intersection_over_union)\n","            print(f'\\nmIoU on the {TARGET} domain: {mIoU}')\n","\n","        print(\"========================================================================\")\n","\n","    if TEST_ONLY_ON_BEST:\n","        break\n"],"metadata":{"id":"M3c7DFDRymnI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736184465570,"user_tz":-60,"elapsed":79621,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}},"outputId":"75efd9ca-379f-4d44-ec4c-add9bda79574"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-57-c45f6e9ccf7a>:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  pretrained_dict = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')\n"]},{"output_type":"stream","name":"stdout","text":["['best_DA_model_Resize__PRETRN=False__optepch__LS=1_LA=0.001_LD=0.1__LR=21_dscLR=21.pth_20.pth', 'best_DA_model_Resize__PRETRN=False__optepch__LS=1_LA=0.001_LD=0.1__LR=21_dscLR=21.pth', 'best_DA_model_Resize__PRETRN=True__optepch__LS=1_LA=0.001_LD=0.1__LR=21_dscLR=21.pth_20.pth', 'best_DA_model_Resize__PRETRN=True__optepch__LS=1_LA=0.001_LD=0.1__LR=21_dscLR=21.pth']\n","Dataset size: 232\n","Testing model=best_DA_model_Resize__PRETRN=False__optepch__LS=1_LA=0.001_LD=0.1__LR=21_dscLR=21.pth_20.pth on domain=Urban on a Validation split\n","Latency: 0.0295 seconds\n","FLOPs: 12682936320\n","Total number of parameters: 7717839\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 14/14 [00:06<00:00,  2.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy on the target domain: 59.55%\n","Background IoU: 0.4805\n","Building IoU: 0.3489\n","Road IoU: 0.3778\n","Water IoU: 0.3355\n","Barren IoU: 0.1899\n","Forest IoU: 0.2038\n","Agricultural IoU: 0.0000\n","\n","mIoU on the Urban domain: 0.2766312062740326\n","========================================================================\n","Dataset size: 274\n","Testing model=best_DA_model_Resize__PRETRN=False__optepch__LS=1_LA=0.001_LD=0.1__LR=21_dscLR=21.pth_20.pth on domain=Rural on a Validation split\n","Latency: 0.0251 seconds\n","FLOPs: 12682936320\n","Total number of parameters: 7717839\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 17/17 [00:07<00:00,  2.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy on the target domain: 39.47%\n","Background IoU: 0.2467\n","Building IoU: 0.3089\n","Road IoU: 0.1392\n","Water IoU: 0.3098\n","Barren IoU: 0.0343\n","Forest IoU: 0.4586\n","Agricultural IoU: 0.0000\n","\n","mIoU on the Rural domain: 0.2139197736978531\n","========================================================================\n","Dataset size: 232\n","Testing model=best_DA_model_Resize__PRETRN=False__optepch__LS=1_LA=0.001_LD=0.1__LR=21_dscLR=21.pth on domain=Urban on a Validation split\n","Latency: 0.0248 seconds\n","FLOPs: 12682936320\n","Total number of parameters: 7717839\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 14/14 [00:06<00:00,  2.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy on the target domain: 56.13%\n","Background IoU: 0.4483\n","Building IoU: 0.3396\n","Road IoU: 0.3489\n","Water IoU: 0.4361\n","Barren IoU: 0.1950\n","Forest IoU: 0.1274\n","Agricultural IoU: 0.0000\n","\n","mIoU on the Urban domain: 0.2707536518573761\n","========================================================================\n","Dataset size: 274\n","Testing model=best_DA_model_Resize__PRETRN=False__optepch__LS=1_LA=0.001_LD=0.1__LR=21_dscLR=21.pth on domain=Rural on a Validation split\n","Latency: 0.0242 seconds\n","FLOPs: 12682936320\n","Total number of parameters: 7717839\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 17/17 [00:07<00:00,  2.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy on the target domain: 39.75%\n","Background IoU: 0.2586\n","Building IoU: 0.2750\n","Road IoU: 0.1389\n","Water IoU: 0.2979\n","Barren IoU: 0.0439\n","Forest IoU: 0.4732\n","Agricultural IoU: 0.0000\n","\n","mIoU on the Rural domain: 0.21250292658805847\n","========================================================================\n","Dataset size: 232\n","Testing model=best_DA_model_Resize__PRETRN=True__optepch__LS=1_LA=0.001_LD=0.1__LR=21_dscLR=21.pth_20.pth on domain=Urban on a Validation split\n","Latency: 0.0253 seconds\n","FLOPs: 12682936320\n","Total number of parameters: 7717839\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 14/14 [00:06<00:00,  2.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy on the target domain: 68.76%\n","Background IoU: 0.5548\n","Building IoU: 0.4899\n","Road IoU: 0.5014\n","Water IoU: 0.4315\n","Barren IoU: 0.3752\n","Forest IoU: 0.3024\n","Agricultural IoU: 0.2615\n","\n","mIoU on the Urban domain: 0.4166805148124695\n","========================================================================\n","Dataset size: 274\n","Testing model=best_DA_model_Resize__PRETRN=True__optepch__LS=1_LA=0.001_LD=0.1__LR=21_dscLR=21.pth_20.pth on domain=Rural on a Validation split\n","Latency: 0.0239 seconds\n","FLOPs: 12682936320\n","Total number of parameters: 7717839\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 17/17 [00:07<00:00,  2.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy on the target domain: 44.84%\n","Background IoU: 0.3353\n","Building IoU: 0.2215\n","Road IoU: 0.2109\n","Water IoU: 0.3293\n","Barren IoU: 0.0878\n","Forest IoU: 0.3673\n","Agricultural IoU: 0.2029\n","\n","mIoU on the Rural domain: 0.2507210075855255\n","========================================================================\n","Dataset size: 232\n","Testing model=best_DA_model_Resize__PRETRN=True__optepch__LS=1_LA=0.001_LD=0.1__LR=21_dscLR=21.pth on domain=Urban on a Validation split\n","Latency: 0.0248 seconds\n","FLOPs: 12682936320\n","Total number of parameters: 7717839\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 14/14 [00:06<00:00,  2.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy on the target domain: 69.01%\n","Background IoU: 0.5707\n","Building IoU: 0.4911\n","Road IoU: 0.4896\n","Water IoU: 0.2742\n","Barren IoU: 0.4280\n","Forest IoU: 0.3153\n","Agricultural IoU: 0.3252\n","\n","mIoU on the Urban domain: 0.41345399618148804\n","========================================================================\n","Dataset size: 274\n","Testing model=best_DA_model_Resize__PRETRN=True__optepch__LS=1_LA=0.001_LD=0.1__LR=21_dscLR=21.pth on domain=Rural on a Validation split\n","Latency: 0.0244 seconds\n","FLOPs: 12682936320\n","Total number of parameters: 7717839\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 17/17 [00:07<00:00,  2.14it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy on the target domain: 43.32%\n","Background IoU: 0.3406\n","Building IoU: 0.2064\n","Road IoU: 0.1966\n","Water IoU: 0.3102\n","Barren IoU: 0.0847\n","Forest IoU: 0.3579\n","Agricultural IoU: 0.1701\n","\n","mIoU on the Rural domain: 0.23808510601520538\n","========================================================================\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["xSaOfED0-zoc","IrsSERdV_xZc","V1PldejM1ZCB","uiBwR_Yg1dVO","BPXkqWOtzUOj","isRcFkZ3CxpI","RtcmrSGDcA4w","l18jlCwmiTZp"],"gpuType":"T4","machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}