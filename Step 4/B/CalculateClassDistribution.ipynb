{"cells":[{"cell_type":"markdown","source":["### Download Train Set"],"metadata":{"id":"PD2KBwVnZV7m"}},{"cell_type":"code","source":["SAVE_ON_DRIVE = True"],"metadata":{"id":"Ai7eAAlUl9hq","executionInfo":{"status":"ok","timestamp":1737974113953,"user_tz":-60,"elapsed":215,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","import shutil\n","\n","drive.mount('/content/drive')\n","\n","# Set paths for Validation and Test datasets\n","val_dataset_path = '/content/drive/MyDrive/LoveDA/Val'\n","test_dataset_path = '/content/drive/MyDrive/LoveDA/Test'\n","\n","\n","# Function to handle dataset download and extraction\n","def handle_dataset(dataset_name, zip_url, local_path, drive_path, save_on_drive):\n","    if not os.path.exists(local_path):\n","        if os.path.exists(f\"{drive_path}.zip\"):\n","            print(f\"{dataset_name} dataset available on own drive, unzipping...\")\n","            !unzip -q {drive_path}.zip -d ./\n","        else:\n","            print(f\"Downloading {dataset_name} dataset...\")\n","            !wget -O {dataset_name}.zip \"{zip_url}\"\n","            if save_on_drive:\n","                print(f\"Saving {dataset_name} dataset on drive...\")\n","                !cp {dataset_name}.zip {drive_path}.zip\n","                print(f\"{dataset_name} dataset saved on drive\")\n","            print(f\"Unzipping {dataset_name} dataset...\")\n","            !unzip -q {dataset_name}.zip -d ./\n","    else:\n","        print(f\"{dataset_name} dataset already in local\")\n","\n","# # Handle Validation dataset\n","# handle_dataset(\"Validation\", \"https://zenodo.org/records/5706578/files/Val.zip?download=1\", \"./Val\", \"/content/drive/MyDrive/LoveDA/Val\", SAVE_VAL_TOO)\n","\n","# # Handle Test dataset\n","# handle_dataset(\"Test\", \"https://zenodo.org/records/5706578/files/Test.zip?download=1\", \"./Test\", \"/content/drive/MyDrive/LoveDA/Test\", False)\n","\n","# Handle Train dataset\n","handle_dataset(\"Train\", \"https://zenodo.org/records/5706578/files/Train.zip?download=1\", \"./Train\", \"/content/drive/MyDrive/LoveDA/Train\", SAVE_ON_DRIVE)"],"metadata":{"id":"tMlLwxBrZhOP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737974115626,"user_tz":-60,"elapsed":1479,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}},"outputId":"34d7cd4a-6143-4853-85e5-12bf5bb9a56d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Train dataset already in local\n"]}]},{"cell_type":"markdown","source":["### Dirs"],"metadata":{"id":"YhWChnHNanrY"}},{"cell_type":"code","source":["from PIL import Image\n","import os\n","import torch\n","from tqdm import tqdm\n","\n","def pil_loader(path, color_type):\n","    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n","    with open(path, 'rb') as f:\n","        img = Image.open(f)\n","        return img.convert(color_type)\n","\n","urban_masks_path = \"./Train/Urban/masks_png\"\n","rural_masks_path = \"./Train/Rural/masks_png\"\n"],"metadata":{"id":"dBYCQgFGapUF","executionInfo":{"status":"ok","timestamp":1737974115626,"user_tz":-60,"elapsed":5,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### Calculation"],"metadata":{"id":"1aq3ckWZb94J"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","idx_to_class = {\n","    0: 'Unclassified',\n","    1: 'Background',\n","    2: 'Building',\n","    3: 'Road',\n","    4: 'Water',\n","    5: 'Barren',\n","    6: 'Forest',\n","    7: 'Agricultural',\n","}\n","\n","class_count = {\n","    0: 0.0,\n","    1: 0.0,\n","    2: 0.0,\n","    3: 0.0,\n","    4: 0.0,\n","    5: 0.0,\n","    6: 0.0,\n","    7: 0.0\n","}\n","\n","class_ids = [0, 1, 2, 3, 4, 5, 6, 7]\n","\n","nof_imgs_that_have_unclassified = 0\n","\n","total = 0\n","for masks_path in [urban_masks_path, rural_masks_path]:\n","    print(f\"processing {masks_path}\")\n","\n","    loop = tqdm(os.listdir(masks_path))\n","    for filename in loop:\n","        loop.set_description(f\"Processing {filename} from \") # update desc with filename\n","\n","        mask = pil_loader(os.path.join(masks_path, filename), \"L\")\n","\n","        # Convert the PIL Image to a PyTorch tensor\n","        mask_tensor = torch.from_numpy(np.array(mask))\n","\n","        # Count the occurrences of each class using torch.unique\n","        unique_classes, counts = torch.unique(mask_tensor, return_counts=True)\n","\n","        if 7 in unique_classes:\n","            nof_imgs_that_have_unclassified += 1\n","\n","        # Update the class_count dictionary\n","        for class_idx, count in zip(unique_classes.tolist(), counts.tolist()):\n","            if class_idx in class_count:\n","                class_count[class_idx] += count\n","                total += count\n","\n","    class_ratio = {}\n","    for key in class_count.keys():\n","        class_ratio[key] = class_count[key] / total\n","\n","    for key in class_ratio.keys():\n","      print(f\"{idx_to_class[key]}: {class_ratio[key]:.4f}\")\n","\n","    print()\n","\n","    print(\"class_ratio = {\")  # Start with the opening brace\n","    for key in class_ratio.keys():\n","        print(f\"    {key}: {class_ratio[key]:.4f},\")  # Print each key-value pair with indentation and comma\n","    print(\"}\")  # End with the closing brace\n","\n","    print()\n","    print(f\"There are {nof_imgs_that_have_unclassified} images that contain the unclassified class\")"],"metadata":{"id":"rcZCALsvaKOW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737974206692,"user_tz":-60,"elapsed":91071,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}},"outputId":"c5b11c3d-cb4d-4151-b81d-d1b38802928f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["processing ./Train/Urban/masks_png\n"]},{"output_type":"stream","name":"stderr","text":["Processing 2294.png from : 100%|██████████| 1156/1156 [00:41<00:00, 27.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Unclassified: 0.0482\n","Background: 0.4612\n","Building: 0.2018\n","Road: 0.0883\n","Water: 0.0355\n","Barren: 0.0720\n","Forest: 0.0754\n","Agricultural: 0.0177\n","\n","class_ratio = {\n","    0: 0.0482,\n","    1: 0.4612,\n","    2: 0.2018,\n","    3: 0.0883,\n","    4: 0.0355,\n","    5: 0.0720,\n","    6: 0.0754,\n","    7: 0.0177,\n","}\n","\n","There are 187 images that contain the unclassified class\n","processing ./Train/Rural/masks_png\n"]},{"output_type":"stream","name":"stderr","text":["Processing 222.png from : 100%|██████████| 1366/1366 [00:49<00:00, 27.56it/s]"]},{"output_type":"stream","name":"stdout","text":["Unclassified: 0.0377\n","Background: 0.3444\n","Building: 0.1064\n","Road: 0.0507\n","Water: 0.0615\n","Barren: 0.0502\n","Forest: 0.1547\n","Agricultural: 0.1944\n","\n","class_ratio = {\n","    0: 0.0377,\n","    1: 0.3444,\n","    2: 0.1064,\n","    3: 0.0507,\n","    4: 0.0615,\n","    5: 0.0502,\n","    6: 0.1547,\n","    7: 0.1944,\n","}\n","\n","There are 1192 images that contain the unclassified class\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["### Output"],"metadata":{"id":"Qus50ARJX5rp"}},{"cell_type":"code","source":["# processing ./Train/Urban/masks_png\n","# Processing 2294.png from : 100%|██████████| 1156/1156 [00:41<00:00, 27.83it/s]\n","# Unclassified: 0.0482\n","# Background: 0.4612\n","# Building: 0.2018\n","# Road: 0.0883\n","# Water: 0.0355\n","# Barren: 0.0720\n","# Forest: 0.0754\n","# Agricultural: 0.0177\n","\n","# class_ratio = {\n","#     0: 0.0482,\n","#     1: 0.4612,\n","#     2: 0.2018,\n","#     3: 0.0883,\n","#     4: 0.0355,\n","#     5: 0.0720,\n","#     6: 0.0754,\n","#     7: 0.0177,\n","# }\n","\n","# There are 187 images that contain the unclassified class\n","# processing ./Train/Rural/masks_png\n","# Processing 222.png from : 100%|██████████| 1366/1366 [00:49<00:00, 27.56it/s]Unclassified: 0.0377\n","# Background: 0.3444\n","# Building: 0.1064\n","# Road: 0.0507\n","# Water: 0.0615\n","# Barren: 0.0502\n","# Forest: 0.1547\n","# Agricultural: 0.1944\n","\n","# class_ratio = {\n","#     0: 0.0377,\n","#     1: 0.3444,\n","#     2: 0.1064,\n","#     3: 0.0507,\n","#     4: 0.0615,\n","#     5: 0.0502,\n","#     6: 0.1547,\n","#     7: 0.1944,\n","# }\n","\n","# There are 1192 images that contain the unclassified class\n"],"metadata":{"id":"CI2bI-lnea3Y","executionInfo":{"status":"ok","timestamp":1737974206692,"user_tz":-60,"elapsed":5,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Pixel Weighting Things"],"metadata":{"id":"P1Sc1TyzapQF"}},{"cell_type":"markdown","source":["What I would like to do is to weigh the training inversely to the frequency of occurrence of classes in Urban, so that if the model already learned to classify buildings let's say, then it will not focus on that, but on other things like agriculture (which btw it's probably much more prevalent in Rural)"],"metadata":{"id":"96Y-99CSV5Hz"}},{"cell_type":"code","source":["# Urban class ratio\n","class_ratio = {\n","    0: 0.0482,\n","    1: 0.4612,\n","    2: 0.2018,\n","    3: 0.0883,\n","    4: 0.0355,\n","    5: 0.0720,\n","    6: 0.0754,\n","    7: 0.0177,\n","}\n","\n","print(\"urban stats\")\n","class_ratio_inverse = {}\n","for key, value in class_ratio.items():\n","    class_ratio_inverse[key] = 1 - value\n","\n","print(class_ratio_inverse)\n","\n","class_ratio = {\n","    0: 0.0377,\n","    1: 0.3444,\n","    2: 0.1064,\n","    3: 0.0507,\n","    4: 0.0615,\n","    5: 0.0502,\n","    6: 0.1547,\n","    7: 0.1944,\n","}\n","\n","print(\"rural stats\")\n","class_ratio_inverse = {}\n","for key, value in class_ratio.items():\n","    class_ratio_inverse[key] = 1 - value\n","\n","print(class_ratio_inverse)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hOU_LdG7arEs","executionInfo":{"status":"ok","timestamp":1737974401047,"user_tz":-60,"elapsed":188,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}},"outputId":"6ca88810-be41-44ca-f1cc-a772a63fe75a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["urban stats\n","{0: 0.9518, 1: 0.5388, 2: 0.7982, 3: 0.9117, 4: 0.9645, 5: 0.928, 6: 0.9246, 7: 0.9823}\n","rural stats\n","{0: 0.9623, 1: 0.6556, 2: 0.8936, 3: 0.9493, 4: 0.9385, 5: 0.9498, 6: 0.8452999999999999, 7: 0.8056}\n"]}]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["PD2KBwVnZV7m","x92nAjzuZmSp"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}