{"cells":[{"cell_type":"markdown","metadata":{"id":"cbIwgK7-uZ0l"},"source":["**WARNING**: Remember to run the `ExtractBoundaries` notebook present in step2b folder before running all (or specifically the `Import Boundaries` cell)\n","\n","**WARNING2**: You should also run `Generate Class Masks` from step 4b"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":543,"status":"ok","timestamp":1737980530877,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"qeY1VfJ5T8kV"},"outputs":[],"source":["SAVE_ON_DRIVE = True # Set to True if you want to save the datasets and trained models to your Google Drive"]},{"cell_type":"markdown","metadata":{"id":"YwDm553BzPlB"},"source":["# Dataset initialization\n"]},{"cell_type":"markdown","metadata":{"id":"7zeYu_g-bpip"},"source":["### Download Data"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111487,"status":"ok","timestamp":1737980643141,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"cjuW8Yf0zO7K","outputId":"f1c7dbdc-0704-45b9-f7d8-4a93cf9390bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Validation dataset available on own drive, unzipping...\n","Train dataset available on own drive, unzipping...\n"]}],"source":["from google.colab import drive\n","import os\n","import shutil\n","\n","drive.mount('/content/drive')\n","\n","# Set paths for Validation and Test datasets\n","val_dataset_path = '/content/drive/MyDrive/LoveDA/Val'\n","test_dataset_path = '/content/drive/MyDrive/LoveDA/Test'\n","\n","\n","# Function to handle dataset download and extraction\n","def handle_dataset(dataset_name, zip_url, local_path, drive_path, save_on_drive):\n","    if not os.path.exists(local_path):\n","        if os.path.exists(f\"{drive_path}.zip\"):\n","            print(f\"{dataset_name} dataset available on own drive, unzipping...\")\n","            !unzip -q {drive_path}.zip -d ./\n","        else:\n","            print(f\"Downloading {dataset_name} dataset...\")\n","            !wget -O {dataset_name}.zip \"{zip_url}\"\n","            if save_on_drive:\n","                print(f\"Saving {dataset_name} dataset on drive...\")\n","                !cp {dataset_name}.zip {drive_path}.zip\n","                print(f\"{dataset_name} dataset saved on drive\")\n","            print(f\"Unzipping {dataset_name} dataset...\")\n","            !unzip -q {dataset_name}.zip -d ./\n","    else:\n","        print(f\"{dataset_name} dataset already in local\")\n","\n","# Handle Validation dataset\n","handle_dataset(\"Validation\", \"https://zenodo.org/records/5706578/files/Val.zip?download=1\", \"./Val\", \"/content/drive/MyDrive/LoveDA/Val\", False)\n","\n","# Handle Test dataset\n","#handle_dataset(\"Test\", \"https://zenodo.org/records/5706578/files/Test.zip?download=1\", \"./Test\", \"/content/drive/MyDrive/LoveDA/Test\", False)\n","\n","# Handle Train dataset\n","handle_dataset(\"Train\", \"https://zenodo.org/records/5706578/files/Train.zip?download=1\", \"./Train\", \"/content/drive/MyDrive/LoveDA/Train\", SAVE_ON_DRIVE)"]},{"cell_type":"markdown","metadata":{"id":"OftJbvlhH725"},"source":["### Import Boundaries"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22583,"status":"ok","timestamp":1737980665719,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"XObla4TbH9Y7","outputId":"cb34f559-9de6-4504-fa52-6afc7e4936c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating ./Train/Rural/boundaries_png...\n","Creating ./Train/Urban/boundaries_png...\n","Importing boundaries, as we have 0 urban boundaries as of now...\n"]}],"source":["# Paths\n","rural_boundaries_path = \"./Train/Rural/boundaries_png\"\n","rural_masks_path = './Train/Rural/masks_png'\n","\n","urban_boundaries_path = \"./Train/Urban/boundaries_png\"\n","urban_masks_path = './Train/Urban/masks_png'\n","drive_rural_boundaries_path = '/content/drive/MyDrive/LoveDA/boundaries/Rural/boundaries_png'\n","drive_urban_boundaries_path = '/content/drive/MyDrive/LoveDA/boundaries/Urban/boundaries_png'\n","\n","boundaries_paths = [rural_boundaries_path, urban_boundaries_path]\n","\n","# Make dir inside ./Train/...\n","for boundaries_path in boundaries_paths:\n","    if (os.path.exists(boundaries_path) == False):\n","        print(f\"Creating {boundaries_path}...\")\n","        os.makedirs(boundaries_path)\n","    else:\n","        print(f\"{boundaries_path} exists...\")\n","\n","\n","# Check if files are already present\n","# rural_file_count = len([name for name in os.listdir(rural_boundaries_path) if os.path.isfile(os.path.join(rural_boundaries_path, name))])\n","# rural_mask_file_count = len([name for name in os.listdir(rural_masks_path) if os.path.isfile(os.path.join(rural_masks_path, name))])\n","urban_file_count = len([name for name in os.listdir(urban_boundaries_path) if os.path.isfile(os.path.join(urban_boundaries_path, name))])\n","urban_mask_file_count = len([name for name in os.listdir(urban_masks_path) if os.path.isfile(os.path.join(urban_masks_path, name))])\n","\n","# if (rural_file_count != rural_mask_file_count):\n","#     print(f\"Importing boundaries, as we have {rural_file_count} rural boundaries as of now...\")\n","#     shutil.copytree(drive_rural_boundaries_path, rural_boundaries_path, dirs_exist_ok=True)\n","# else:\n","#     print(f\"Rural boundaries already present, {rural_file_count} files...\")\n","\n","if (urban_file_count != urban_mask_file_count):\n","    print(f\"Importing boundaries, as we have {urban_file_count} urban boundaries as of now...\")\n","    shutil.copytree(drive_urban_boundaries_path, urban_boundaries_path, dirs_exist_ok=True)\n","else:\n","    print(f\"Urban boundaries already present, {urban_file_count} files...\")"]},{"cell_type":"markdown","metadata":{"id":"68BKhuGh2vJH"},"source":["### Dataset Definition"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7113,"status":"ok","timestamp":1737980672827,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"iUsQczlZ3Wx0"},"outputs":[],"source":["import os\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import torchvision.transforms as T\n","import random\n","import cv2\n","\n","\n","def pil_loader(path, color_type):\n","    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n","    with open(path, 'rb') as f:\n","        img = Image.open(f)\n","        return img.convert(color_type)\n","\n","class LoveDADataset(Dataset):\n","    def __init__(self, baseTransform, augTransforms, split = 'Urban', type = 'Train', useBoundaries=True, validation_ratio=0.2, seed=265637):\n","        # Validate type input\n","        if type not in ['Train', 'Validation', 'Total', 'ActualTest']:\n","            raise ValueError(\"Invalid type. Expected 'Train' or 'Validation' or 'Total' or 'ActualTest'.\")\n","        self.directory = []\n","        if type == 'ActualTest':\n","            directory_path = os.path.join('./Val', split, 'images_png')\n","        else:\n","            directory_path = os.path.join('./Train', split, 'images_png')\n","        # Check if the directory exists\n","        if not os.path.exists(directory_path):\n","            raise FileNotFoundError(f\"Directory not found: {directory_path}\")\n","        # Get all image paths\n","        all_images = [os.path.join(directory_path, entry) for entry in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, entry))]\n","        # Shuffle images for random splitting\n","        random.seed(seed)\n","        random.shuffle(all_images)\n","        # Split into training and validation sets\n","        split_idx = int(len(all_images) * (1 - validation_ratio))\n","        if type == 'Train':\n","            self.directory = all_images[:split_idx]\n","        elif type == 'Validation':\n","            self.directory = all_images[split_idx:]\n","        elif type == 'Total':\n","            self.directory = all_images\n","        elif type == 'ActualTest':\n","            self.directory = all_images\n","        else:\n","            raise ValueError(\"Invalid type. Expected 'Train' or 'Validation' or 'Total' or 'ActualTest.\")\n","        self.baseTransforms = baseTransform\n","        self.augTransforms = augTransforms\n","        self.useBoundaries = useBoundaries\n","        self.typeDataset = type\n","        # Print dataset size\n","        print(f\"Dataset size: {len(self.directory)}\")\n","\n","    def __len__(self):\n","        return len(self.directory)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.directory[idx]\n","        image = pil_loader(image_path, 'RGB')\n","        mask_path = image_path.replace('images_png', 'masks_png')\n","        boundaries_path = image_path.replace('images_png', 'boundaries_png')\n","\n","        mask = pil_loader(mask_path, 'L')\n","\n","        if self.useBoundaries:\n","          boundaries = pil_loader(boundaries_path, 'L')\n","        else:\n","          boundaries = mask\n","\n","        base_transformed = self.baseTransforms(image=np.array(image), mask=np.array(mask), boundaries=np.array(boundaries))\n","        base_image = base_transformed['image']\n","        base_mask = base_transformed['mask']\n","        base_boundaries = base_transformed['boundaries']\n","\n","        base_image = T.Compose([T.ToTensor()])(base_image)\n","        base_mask = torch.from_numpy(base_mask).long()\n","        base_mask -= 1\n","        base_boundaries = torch.from_numpy(base_boundaries)\n","\n","        if(self.typeDataset != 'Train'):\n","          return base_image, base_mask, image_path, base_boundaries\n","\n","\n","        if self.augTransforms == None:\n","          return [base_image], [base_mask], image_path, [base_boundaries]\n","        # Apply transformations\n","        augmented = self.augTransforms(image=np.array(image), mask=np.array(mask), boundaries=np.array(boundaries))\n","        augmented_image = T.Compose([T.ToTensor()])(augmented['image'])\n","\n","        mask = augmented['mask']\n","        mask = torch.from_numpy(mask).long()\n","        mask = mask-1\n","        boundaries = augmented['boundaries']\n","        boundaries = torch.from_numpy(boundaries)\n","\n","        image_list = [base_image, augmented_image]\n","        mask_list = [base_mask, mask]\n","        boundaries_list = [base_boundaries, boundaries]\n","\n","        return image_list, mask_list, image_path, boundaries_list"]},{"cell_type":"markdown","metadata":{"id":"xSaOfED0-zoc"},"source":["### Dataset Utils"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1737980672828,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"tfi1pG3P-2H1"},"outputs":[],"source":["import matplotlib.patches as mpatches\n","\n","from collections import OrderedDict\n","COLOR_MAP = OrderedDict(\n","    Background=(255, 255, 255),\n","    Building=(255, 0, 0),\n","    Road=(255, 255, 0),\n","    Water=(0, 0, 255),\n","    Barren=(159, 129, 183),\n","    Forest=(34, 139, 34),\n","    Agricultural=(255, 195, 128),\n",")\n","\n","LABEL_MAP = OrderedDict(\n","    Background=0,\n","    Building=1,\n","    Road=2,\n","    Water=3,\n","    Barren=4,\n","    Forest=5,\n","    Agricultural=6,\n",")\n","inverted_label_map = OrderedDict((v, k) for k, v in LABEL_MAP.items())\n","\n","\n","def getLabelColor(label):\n","    # Default color for unclassified labels\n","    default_color = np.array([128, 128, 128])  # Gray\n","\n","    # Check if label exists in inverted_label_map\n","    label_name = inverted_label_map.get(label, None)\n","    if label_name is None or label_name not in COLOR_MAP:\n","        return default_color  # Return default color for unclassified\n","\n","    # Return the mapped color\n","    label_color = np.array(COLOR_MAP[label_name])\n","    return label_color\n","\n","\n","def getLegendHandles():\n","  handles = [mpatches.Patch(color=getLabelColor(i)/255, label=inverted_label_map[i]) for i in range(0, len(LABEL_MAP))]\n","  handles.append(mpatches.Patch(color=getLabelColor(-1)/255, label='Unclassified'))\n","  return handles\n","\n","def new_colors_mask(mask):\n","  new_image = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n","  for i, row in enumerate(mask):\n","    for j, cell in enumerate(row):\n","      new_image[i][j] = getLabelColor(cell.item())\n","  return new_image\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IrsSERdV_xZc"},"source":["### Dataset Debug"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1737980672829,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"l5iBVMMn6-3-"},"outputs":[],"source":["# # Comment this cell to save GPU time\n","\n","# import matplotlib.pyplot as plt\n","# import torch\n","# from torch.utils.data import DataLoader\n","# import matplotlib.patches as mpatches\n","\n","# train_dataset = LoveDADataset(type='Train', seed=222)\n","# print(train_dataset.__len__())\n","\n","# # Get item\n","# image, mask, path, bd = train_dataset.__getitem__(88)\n","\n","# # Show path\n","# print(f\"Image is at {path}\")\n","\n","# # Show image\n","# image = image.permute(1, 2, 0)\n","# image = image.numpy()\n","# plt.imshow(image)\n","\n","# # Show mask\n","# new_image = new_colors_mask(mask)\n","# plt.imshow(image)\n","# plt.show()\n","# plt.legend(handles=getLegendHandles(), loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n","# plt.imshow(new_image)\n","# plt.show()\n","\n","# # Show boundaries\n","# # for row in bd:\n","# #     for col in row:\n","# #         if col != 0 and col != 1:\n","# #             print(col)\n","# bd = bd.numpy()\n","# plt.imshow(bd)\n"]},{"cell_type":"markdown","metadata":{"id":"uZwN55AlCRZc"},"source":["# Initialize model"]},{"cell_type":"markdown","metadata":{"id":"V1PldejM1ZCB"},"source":["### PIDNet Util Modules"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1737980672829,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"8lOLZdcA1W04"},"outputs":[],"source":["# ------------------------------------------------------------------------------\n","# Written by Jiacong Xu (jiacong.xu@tamu.edu)\n","# ------------------------------------------------------------------------------\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","BatchNorm2d = nn.BatchNorm2d\n","bn_mom = 0.1\n","algc = False\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=False):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.no_relu = no_relu\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","\n","        if self.no_relu:\n","            return out\n","        else:\n","            return self.relu(out)\n","\n","class Bottleneck(nn.Module):\n","    expansion = 2\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=True):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n","                               bias=False)\n","        self.bn3 = BatchNorm2d(planes * self.expansion, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.no_relu = no_relu\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        if self.no_relu:\n","            return out\n","        else:\n","            return self.relu(out)\n","\n","class segmenthead(nn.Module):\n","\n","    def __init__(self, inplanes, interplanes, outplanes, scale_factor=None):\n","        super(segmenthead, self).__init__()\n","        self.bn1 = BatchNorm2d(inplanes, momentum=bn_mom)\n","        self.conv1 = nn.Conv2d(inplanes, interplanes, kernel_size=3, padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(interplanes, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(interplanes, outplanes, kernel_size=1, padding=0, bias=True)\n","        self.scale_factor = scale_factor\n","\n","    def forward(self, x):\n","\n","        x = self.conv1(self.relu(self.bn1(x)))\n","        out = self.conv2(self.relu(self.bn2(x)))\n","\n","        if self.scale_factor is not None:\n","            height = x.shape[-2] * self.scale_factor\n","            width = x.shape[-1] * self.scale_factor\n","            out = F.interpolate(out,\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)\n","\n","        return out\n","\n","class DAPPM(nn.Module):\n","    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n","        super(DAPPM, self).__init__()\n","        bn_mom = 0.1\n","        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale0 = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.process1 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.process2 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.process3 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.process4 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.compression = nn.Sequential(\n","                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n","                                    )\n","        self.shortcut = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n","                                    )\n","\n","    def forward(self, x):\n","        width = x.shape[-1]\n","        height = x.shape[-2]\n","        x_list = []\n","\n","        x_list.append(self.scale0(x))\n","        x_list.append(self.process1((F.interpolate(self.scale1(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[0])))\n","        x_list.append((self.process2((F.interpolate(self.scale2(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[1]))))\n","        x_list.append(self.process3((F.interpolate(self.scale3(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[2])))\n","        x_list.append(self.process4((F.interpolate(self.scale4(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[3])))\n","\n","        out = self.compression(torch.cat(x_list, 1)) + self.shortcut(x)\n","        return out\n","\n","class PAPPM(nn.Module):\n","    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n","        super(PAPPM, self).__init__()\n","        bn_mom = 0.1\n","        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","\n","        self.scale0 = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","\n","        self.scale_process = nn.Sequential(\n","                                    BatchNorm(branch_planes*4, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes*4, branch_planes*4, kernel_size=3, padding=1, groups=4, bias=False),\n","                                    )\n","\n","\n","        self.compression = nn.Sequential(\n","                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n","                                    )\n","\n","        self.shortcut = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n","                                    )\n","\n","\n","    def forward(self, x):\n","        width = x.shape[-1]\n","        height = x.shape[-2]\n","        scale_list = []\n","\n","        x_ = self.scale0(x)\n","        scale_list.append(F.interpolate(self.scale1(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","        scale_list.append(F.interpolate(self.scale2(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","        scale_list.append(F.interpolate(self.scale3(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","        scale_list.append(F.interpolate(self.scale4(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","\n","        scale_out = self.scale_process(torch.cat(scale_list, 1))\n","\n","        out = self.compression(torch.cat([x_,scale_out], 1)) + self.shortcut(x)\n","        return out\n","\n","\n","class PagFM(nn.Module):\n","    def __init__(self, in_channels, mid_channels, after_relu=False, with_channel=False, BatchNorm=nn.BatchNorm2d):\n","        super(PagFM, self).__init__()\n","        self.with_channel = with_channel\n","        self.after_relu = after_relu\n","        self.f_x = nn.Sequential(\n","                                nn.Conv2d(in_channels, mid_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(mid_channels)\n","                                )\n","        self.f_y = nn.Sequential(\n","                                nn.Conv2d(in_channels, mid_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(mid_channels)\n","                                )\n","        if with_channel:\n","            self.up = nn.Sequential(\n","                                    nn.Conv2d(mid_channels, in_channels,\n","                                              kernel_size=1, bias=False),\n","                                    BatchNorm(in_channels)\n","                                   )\n","        if after_relu:\n","            self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x, y):\n","        input_size = x.size()\n","        if self.after_relu:\n","            y = self.relu(y)\n","            x = self.relu(x)\n","\n","        y_q = self.f_y(y)\n","        y_q = F.interpolate(y_q, size=[input_size[2], input_size[3]],\n","                            mode='bilinear', align_corners=False)\n","        x_k = self.f_x(x)\n","\n","        if self.with_channel:\n","            sim_map = torch.sigmoid(self.up(x_k * y_q))\n","        else:\n","            sim_map = torch.sigmoid(torch.sum(x_k * y_q, dim=1).unsqueeze(1))\n","\n","        y = F.interpolate(y, size=[input_size[2], input_size[3]],\n","                            mode='bilinear', align_corners=False)\n","        x = (1-sim_map)*x + sim_map*y\n","\n","        return x\n","\n","class Light_Bag(nn.Module):\n","    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n","        super(Light_Bag, self).__init__()\n","        self.conv_p = nn.Sequential(\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","        self.conv_i = nn.Sequential(\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","\n","    def forward(self, p, i, d):\n","        edge_att = torch.sigmoid(d)\n","\n","        p_add = self.conv_p((1-edge_att)*i + p)\n","        i_add = self.conv_i(i + edge_att*p)\n","\n","        return p_add + i_add\n","\n","\n","class DDFMv2(nn.Module):\n","    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n","        super(DDFMv2, self).__init__()\n","        self.conv_p = nn.Sequential(\n","                                BatchNorm(in_channels),\n","                                nn.ReLU(inplace=True),\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","        self.conv_i = nn.Sequential(\n","                                BatchNorm(in_channels),\n","                                nn.ReLU(inplace=True),\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","\n","    def forward(self, p, i, d):\n","        edge_att = torch.sigmoid(d)\n","\n","        p_add = self.conv_p((1-edge_att)*i + p)\n","        i_add = self.conv_i(i + edge_att*p)\n","\n","        return p_add + i_add\n","\n","class Bag(nn.Module):\n","    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n","        super(Bag, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","                                BatchNorm(in_channels),\n","                                nn.ReLU(inplace=True),\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=3, padding=1, bias=False)\n","                                )\n","\n","\n","    def forward(self, p, i, d):\n","        edge_att = torch.sigmoid(d)\n","        return self.conv(edge_att*p + (1-edge_att)*i)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uiBwR_Yg1dVO"},"source":["### PIDNet Definition"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1737980672829,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"dcHkfgpmCUys"},"outputs":[],"source":["# ------------------------------------------------------------------------------\n","# Written by Jiacong Xu (jiacong.xu@tamu.edu)\n","# ------------------------------------------------------------------------------\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","import logging\n","\n","BatchNorm2d = nn.BatchNorm2d\n","bn_mom = 0.1\n","algc = False\n","\n","INPUT_SIZE = (512, 512)\n","\n","class PIDNet(nn.Module):\n","\n","    def __init__(self, m=2, n=3, num_classes=19, planes=64, ppm_planes=96, head_planes=128, augment=True):\n","        super(PIDNet, self).__init__()\n","        self.augment = augment\n","\n","        # I Branch\n","        self.conv1 =  nn.Sequential(\n","                          nn.Conv2d(3,planes,kernel_size=3, stride=2, padding=1),\n","                          BatchNorm2d(planes, momentum=bn_mom),\n","                          nn.ReLU(inplace=True),\n","                          nn.Conv2d(planes,planes,kernel_size=3, stride=2, padding=1),\n","                          BatchNorm2d(planes, momentum=bn_mom),\n","                          nn.ReLU(inplace=True),\n","                      )\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.layer1 = self._make_layer(BasicBlock, planes, planes, m)\n","        self.layer2 = self._make_layer(BasicBlock, planes, planes * 2, m, stride=2)\n","        self.layer3 = self._make_layer(BasicBlock, planes * 2, planes * 4, n, stride=2)\n","        self.layer4 = self._make_layer(BasicBlock, planes * 4, planes * 8, n, stride=2)\n","        self.layer5 =  self._make_layer(Bottleneck, planes * 8, planes * 8, 2, stride=2)\n","\n","        # P Branch\n","        self.compression3 = nn.Sequential(\n","                                          nn.Conv2d(planes * 4, planes * 2, kernel_size=1, bias=False),\n","                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                          )\n","\n","        self.compression4 = nn.Sequential(\n","                                          nn.Conv2d(planes * 8, planes * 2, kernel_size=1, bias=False),\n","                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                          )\n","        self.pag3 = PagFM(planes * 2, planes)\n","        self.pag4 = PagFM(planes * 2, planes)\n","\n","        self.layer3_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n","        self.layer4_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n","        self.layer5_ = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n","\n","        # D Branch\n","        if m == 2:\n","            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes)\n","            self.layer4_d = self._make_layer(Bottleneck, planes, planes, 1)\n","            self.diff3 = nn.Sequential(\n","                                        nn.Conv2d(planes * 4, planes, kernel_size=3, padding=1, bias=False),\n","                                        BatchNorm2d(planes, momentum=bn_mom),\n","                                        )\n","            self.diff4 = nn.Sequential(\n","                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n","                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                     )\n","            self.spp = PAPPM(planes * 16, ppm_planes, planes * 4)\n","            self.dfm = Light_Bag(planes * 4, planes * 4)\n","        else:\n","            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n","            self.layer4_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n","            self.diff3 = nn.Sequential(\n","                                        nn.Conv2d(planes * 4, planes * 2, kernel_size=3, padding=1, bias=False),\n","                                        BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                        )\n","            self.diff4 = nn.Sequential(\n","                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n","                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                     )\n","            self.spp = DAPPM(planes * 16, ppm_planes, planes * 4)\n","            self.dfm = Bag(planes * 4, planes * 4)\n","\n","        self.layer5_d = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n","\n","        # Prediction Head\n","        if self.augment:\n","            self.seghead_p = segmenthead(planes * 2, head_planes, num_classes)\n","            self.seghead_d = segmenthead(planes * 2, planes, 1)\n","\n","        self.final_layer = segmenthead(planes * 4, head_planes, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","\n","    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n","            )\n","\n","        layers = []\n","        layers.append(block(inplanes, planes, stride, downsample))\n","        inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            if i == (blocks-1):\n","                layers.append(block(inplanes, planes, stride=1, no_relu=True))\n","            else:\n","                layers.append(block(inplanes, planes, stride=1, no_relu=False))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _make_single_layer(self, block, inplanes, planes, stride=1):\n","        downsample = None\n","        if stride != 1 or inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n","            )\n","\n","        layer = block(inplanes, planes, stride, downsample, no_relu=True)\n","\n","        return layer\n","\n","    def forward(self, x):\n","\n","        width_output = x.shape[-1] // 8\n","        height_output = x.shape[-2] // 8\n","\n","        h, w = x.size(2), x.size(3)\n","\n","        x = self.conv1(x)\n","        x = self.layer1(x)\n","        x = self.relu(self.layer2(self.relu(x)))\n","        x_ = self.layer3_(x)\n","        x_d = self.layer3_d(x)\n","\n","        x = self.relu(self.layer3(x))\n","        x_ = self.pag3(x_, self.compression3(x))\n","        x_d = x_d + F.interpolate(\n","                        self.diff3(x),\n","                        size=[height_output, width_output],\n","                        mode='bilinear', align_corners=algc)\n","        if self.augment:\n","            temp_p = x_\n","\n","        x = self.relu(self.layer4(x))\n","        x_ = self.layer4_(self.relu(x_))\n","        x_d = self.layer4_d(self.relu(x_d))\n","\n","        x_ = self.pag4(x_, self.compression4(x))\n","        x_d = x_d + F.interpolate(\n","                        self.diff4(x),\n","                        size=[height_output, width_output],\n","                        mode='bilinear', align_corners=algc)\n","        if self.augment:\n","            temp_d = x_d\n","\n","        x_ = self.layer5_(self.relu(x_))\n","        x_d = self.layer5_d(self.relu(x_d))\n","        x = F.interpolate(\n","                        self.spp(self.layer5(x)),\n","                        size=[height_output, width_output],\n","                        mode='bilinear', align_corners=algc)\n","\n","        x_ = self.final_layer(self.dfm(x_, x, x_d))\n","\n","        if self.augment:\n","            x_extra_p = self.seghead_p(temp_p)\n","            x_extra_d = self.seghead_d(temp_d)\n","            return [x_extra_p, x_, x_extra_d]\n","        else:\n","            return x_\n","\n","def get_seg_model(cfg, imgnet_pretrained):\n","\n","    if 's' in cfg.MODEL.NAME:\n","        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=32, ppm_planes=96, head_planes=128, augment=True)\n","    elif 'm' in cfg.MODEL.NAME:\n","        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=96, head_planes=128, augment=True)\n","    else:\n","        model = PIDNet(m=3, n=4, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=112, head_planes=256, augment=True)\n","\n","    if imgnet_pretrained:\n","        pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n","        model_dict = model.state_dict()\n","        pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n","        model_dict.update(pretrained_state)\n","        msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n","        logging.info('Attention!!!')\n","        logging.info(msg)\n","        logging.info('Over!!!')\n","        model.load_state_dict(model_dict, strict = False)\n","    else:\n","        pretrained_dict = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')\n","        if 'state_dict' in pretrained_dict:\n","            pretrained_dict = pretrained_dict['state_dict']\n","        model_dict = model.state_dict()\n","        pretrained_dict = {k[6:]: v for k, v in pretrained_dict.items() if (k[6:] in model_dict and v.shape == model_dict[k[6:]].shape)}\n","        msg = 'Loaded {} parameters!'.format(len(pretrained_dict))\n","        logging.info('Attention!!!')\n","        logging.info(msg)\n","        logging.info('Over!!!')\n","        model_dict.update(pretrained_dict)\n","        model.load_state_dict(model_dict, strict = False)\n","\n","    return model\n","\n","def get_pred_model(name, num_classes):\n","\n","    if 's' in name:\n","        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=False)\n","    elif 'm' in name:\n","        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=False)\n","    else:\n","        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=False)\n","\n","    return model"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1737980672830,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"XGBkU3KzzTdK"},"outputs":[],"source":["# Remember to upsample the input x before running it through this, as the paper says\n","import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, num_classes):\n","        super(Discriminator, self).__init__()\n","        self.domain_classifier = nn.Sequential(\n","            nn.Conv2d(num_classes, 64, kernel_size=4, stride=2, padding=1),  # Conv1\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # Conv2\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # Conv3\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),  # Conv4\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(512, 1, kernel_size=4, stride=2, padding=1),  # Conv5\n","        )\n","\n","    def forward(self, x):\n","        return self.domain_classifier(x)\n","\n","# Initialize the model with Kaiming initialization\n","def initialize_weights_kaiming(m):\n","    if isinstance(m, nn.Conv2d):\n","        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n","        if m.bias is not None:\n","            init.zeros_(m.bias)"]},{"cell_type":"markdown","metadata":{"id":"isRcFkZ3CxpI"},"source":["### Load PIDNet Model"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":5453,"status":"ok","timestamp":1737980678275,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"ujP_2PffskRk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b24d47f5-b445-408d-cbef-e0722a0dd9cb"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-\n","To: /content/PIDNet_S_ImageNet.pth.tar\n","100%|██████████| 38.1M/38.1M [00:00<00:00, 110MB/s]\n"]}],"source":["import gdown\n","import tarfile\n","\n","if (os.path.exists(\"./PIDNet_S_ImageNet.pth.tar\") == False):\n","  url = \"https://drive.google.com/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-\"\n","  output = \"./\"\n","  gdown.download(url, output, quiet=False)\n","# Then keep as tar, as it's already the correct format to feed the model\n","\n","# Create a config object with required parameters\n","class Config:\n","    class MODEL:\n","        NAME = 'pidnet_s'  # or 'pidnet_m' or 'pidnet_l'\n","        PRETRAINED = 'PIDNet_S_ImageNet.pth.tar'\n","    class DATASET:\n","        NUM_CLASSES = len(LABEL_MAP)\n","\n","cfg = Config()\n","\n","# model = get_pred_model('s', len(LABEL_MAP))\n"]},{"cell_type":"markdown","metadata":{"id":"3DwQcmOWpUxn"},"source":["# Training & Dataset creation"]},{"cell_type":"markdown","metadata":{"id":"iToC6F28cnl5"},"source":["### Ablations and Macros"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1737980678276,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"moWCCVykx1ac"},"outputs":[],"source":["DEVICE = 'cuda' # 'cuda' or 'cpu'\n","\n","\n","LR = 1*1e-6 # The initial Learning Rate -- I increased it using quadratic rule in relation with batch size:: new_LR = old_LR * sqrt(batch_size_new / batch_size_old)\n","MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n","WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n","\n","\n","NUM_EPOCHS = 20      # Total number of training epochs (iterations over dataset)\n","STEP_SIZE = 21       # How many epochs before decreasing learning rate (if using a step-down policy) -- Trying to keep a 2:3 ratio with NUM_EPOCHS\n","GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n","\n","\n","NUM_CLASSES = len(LABEL_MAP)\n","BATCH_SIZE = 16 # Leave to two if using do_mixing() instead of do_mixing_fullbatch()\n","if DEVICE == 'cpu':\n","  BATCH_SIZE = 2\n","\n","\n","### Main DACS tweaks (+ the extra ones from the repo that seem to be coming from DAFormer)\n","TARGET_TRANSFORMS = 'None' # 'weak' 'strong' or 'None'\n","LAMBDA_MIXED = 18\n","PIXEL_WEIGHTED = True # If set to False, PIXEL_WEIGHT_SOFT from below will have no utility\n","DETACH_PSEUDO_MASKS = False\n","\n","\n","### Extra DACS Tweaks\n","USE_NORMALIZE_ON_RURAL = False  # Originally we use just Resize512\n","PIXEL_WEIGHT_SOFT = False   # (default on False) hard threshold turns the pixel on/off for the loss, instead soft weighs based on confidence\n","WEIGH_LOSS_BY_CLASS = True\n","TAKE_ONLY_BUILDINGS_AND_ROADS = 'No'  # 'Hard' or 'Soft or 'No'\n","                                        # 'Hard' only takes just buildings and roads from Urban, and rest from Rural. With 'Soft' there's a chance to have others too. 'No' takes randomly\n","REV_TAKE_ONLY_BUILDINGS_AND_ROADS = 'No'    # 'Buildings' or 'BuildingsAndRoads' or 'No'\n","\n","### Using EMA-teacher model for pseudo-labels\n","USE_EMA_FOR_TARGET = False\n","# if USE_EMA_FOR_TARGET:\n","#   DETACH_PSEUDO_MASKS = True # Set to true in Training Loop if USE_EMA_FOR_TARGET\n","ALPHA_TEACHER = 0.99\n","\n","\n","### Loading Pretrained Model\n","drive_path = '/content/drive/MyDrive/LoveDA'\n","LOAD_BASE_MODEL = True\n","# model_name moved in training loop\n","\n","### Other\n","DONT_SAVE_BEFORE_EPOCH = 10\n","SAVE_AFTER_EPOCH = 20 # X means starts at epoch X and forward, e.g. 20 is the highest\n","LOG_FREQUENCY = 1000\n","\n","### NEXT THING TO TRY: augment also during DA with the same aug used for the base model\n","# try class_wtd, then +rev_take_buldings, then without class_wtd"]},{"cell_type":"markdown","metadata":{"id":"XlPJ2YY9Esas"},"source":["### EMA Utils"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1737980678276,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"nJoaJEUlEsvM"},"outputs":[],"source":["def create_ema_model(model):\n","    ema_model = get_seg_model(cfg, imgnet_pretrained=True)\n","    for param in ema_model.parameters():\n","        param.detach_()\n","\n","    original_parameters = list(model.parameters())\n","    ema_parameters = list(ema_model.parameters())\n","    n = len(original_parameters)\n","    for i in range(0, n):\n","        ema_parameters[i].data[:] = original_parameters[i].data[:].clone()\n","\n","    return ema_model\n","\n","def update_ema_variables(ema_model, model):\n","    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n","        ema_param.data[:] = ALPHA_TEACHER * ema_param[:].data[:] + (1 - ALPHA_TEACHER) * param[:].data[:]\n","\n","    return ema_model"]},{"cell_type":"markdown","metadata":{"id":"o_JWq-xGE4iq"},"source":["### Setup, Create Datasets and DataLoaders. With annexed transforms."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1700,"status":"ok","timestamp":1737980679968,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"WMZLgyYsE63O","outputId":"75c76214-484f-4246-b1cc-d6e9b22efb05"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset size: 924\n","Dataset size: 1092\n","Dataset size: 232\n","Dataset size: 274\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.1 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","  check_for_updates()\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","from albumentations import Compose, HorizontalFlip, RandomRotate90, RandomScale, RandomCrop, GaussNoise, Rotate, Resize, OneOf, Normalize, ColorJitter, GaussianBlur\n","from albumentations.pytorch import ToTensorV2\n","\n","#How big should be the image that we feed to the model?\n","RESIZE = 512\n","# DEFINE TRANSFORMATIONS HERE\n","# To Tensor is not needed since its performed inside the getitem\n","\n","\n","AUGMENTATIONS = {\n","    'Resize512': Compose([\n","            Resize(512, 512),\n","    ], additional_targets={\"boundaries\": \"mask\"}),\n","    'Normalize': Compose([\n","            Normalize(mean=(123.675, 116.28, 103.53), std=(58.395, 57.12, 57.375), max_pixel_value=1.0, always_apply=True),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomCrop256': Compose([\n","            RandomCrop(256, 256),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomCrop512': Compose([\n","            RandomCrop(512, 512),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomCrop512+Normalization' : Compose([\n","            Normalize(mean=(123.675, 116.28, 103.53), std=(58.395, 57.12, 57.375), max_pixel_value=1.0, always_apply=True),\n","            RandomCrop(512, 512),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomCropOrResize': Compose([\n","            OneOf([\n","                RandomCrop(RESIZE, RESIZE, p=0.5),  # Random crop to resize\n","                Resize(RESIZE, RESIZE, p=0.5)\n","            ], p=1)\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomCropXXX': Compose([\n","            OneOf([\n","                RandomCrop(256, 256),  # Random crop to resize\n","                RandomCrop(512, 512),\n","            ], p=1)\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'Jitter': Compose([\n","            ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'NormalizeOnRural+Resize512': Compose([\n","            Normalize(mean=(73.532, 80.017, 74.593), std=(41.493, 35.653, 33.747), max_pixel_value=1.0, always_apply=True),\n","            Resize(512, 512),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'GaussianBlur': Compose([\n","            GaussianBlur(p=0.5),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","}\n","\n","CHOOSE_TRANSFORM = 'Resize512'\n","if USE_NORMALIZE_ON_RURAL:\n","    CHOOSE_TRANSFORM = 'NormalizeOnRural+Resize512'\n","VALIDATION_TRANSFORM = CHOOSE_TRANSFORM\n","AUGTRANSFORM = None\n","\n","## Dataset and Loader\n","# Training Sets and Loaders\n","source_dataset = LoveDADataset(baseTransform=AUGMENTATIONS[CHOOSE_TRANSFORM], augTransforms=AUGTRANSFORM, split='Urban', type='Train', validation_ratio=0.2)\n","source_loader = DataLoader(source_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True, pin_memory=True)\n","\n","target_dataset = LoveDADataset(baseTransform=AUGMENTATIONS[CHOOSE_TRANSFORM], augTransforms=AUGTRANSFORM, split='Rural', type='Train', validation_ratio=0.2, useBoundaries=False)\n","target_loader = DataLoader(target_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True, pin_memory=True)\n","\n","# Validation Sets and Loaders\n","source_validation_dataset = LoveDADataset(baseTransform=AUGMENTATIONS[VALIDATION_TRANSFORM], augTransforms=AUGTRANSFORM, split='Urban', type='Validation', validation_ratio=0.2, useBoundaries=False)\n","source_validation_loader = DataLoader(source_validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True, pin_memory=True)\n","\n","target_validation_dataset = LoveDADataset(baseTransform=AUGMENTATIONS[VALIDATION_TRANSFORM], augTransforms=AUGTRANSFORM, split='Rural', type='Validation', validation_ratio=0.2, useBoundaries=False)\n","target_validation_loader = DataLoader(target_validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True, pin_memory=True)"]},{"cell_type":"markdown","metadata":{"id":"l18jlCwmiTZp"},"source":["### Losses"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1737980679968,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"tU1zr5WKiU0y"},"outputs":[],"source":["def weighted_bce(bd_pre, target):\n","    n, c, h, w = bd_pre.size()\n","    log_p = bd_pre.permute(0,2,3,1).contiguous().view(1, -1)\n","    target_t = target.view(1, -1)\n","\n","    pos_index = (target_t == 1)\n","    neg_index = (target_t == 0)\n","\n","    weight = torch.zeros_like(log_p)\n","    pos_num = pos_index.sum()\n","    neg_num = neg_index.sum()\n","    sum_num = pos_num + neg_num\n","    weight[pos_index] = neg_num * 1.0 / sum_num\n","    weight[neg_index] = pos_num * 1.0 / sum_num\n","\n","    loss = F.binary_cross_entropy_with_logits(log_p, target_t, weight, reduction='mean')\n","\n","    return loss\n","\n","def boundary_loss(bd_pre, bd_gt):\n","    loss = 20.0 * weighted_bce(bd_pre, bd_gt)\n","    return loss\n","\n","# TODO EXTRA add weights=class_weights to nn.CrossEntropyLoss()\n","# TODO EXTRA use OHCE instead of basic one\n","def cross_entropy(score, target):\n","    compute_ce_loss = nn.CrossEntropyLoss(ignore_index=-1)\n","\n","    # See paper for weights. In order of loss index: (0.4, 20, 1, 1) # But on cfg they set everything to 0.5\n","    balance_weights = [0.4, 1]\n","    sb_weights = 1 # WARNING: BY CHANGING THIS WEIGHT YOU'RE ACTUALLY AFFECTING ALSO SOME OTHER PARTS OF THE CODE, see where we use sem_loss\n","\n","    # print(f\"DEBUG: inside cross_entropy: len(score) = {len(score)}\")\n","    if len(balance_weights) == len(score):\n","        loss = sum([w * compute_ce_loss(x, target) for (w, x) in zip(balance_weights, score)])\n","        return loss\n","    elif len(score) == 1:\n","        loss = sb_weights * compute_ce_loss(score[0], target)\n","        return loss\n","    else:\n","        raise ValueError(\"lengths of prediction and target are not identical!\")\n","\n","sem_loss = cross_entropy\n","bd_loss = boundary_loss\n","bce_loss = torch.nn.BCEWithLogitsLoss()\n","mix_ce_loss = torch.nn.CrossEntropyLoss(weight=None, ignore_index=-1, reduction='none')\n","mix_ce_loss_wtd = lambda pred, target, weight: (mix_ce_loss(pred, target) * weight).mean()"]},{"cell_type":"markdown","metadata":{"id":"OwP4ZvV0rB8I"},"source":["### MixMask Utils"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1737980679968,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"fjBORXPIzU-r"},"outputs":[],"source":["# extract classes from source masks\n","def extract_classes_from_mask(mask):\n","  return torch.unique(mask)\n","\n","# randomly select classes to mix\n","def select_classes_for_mix(classes):\n","    # print(f\"classes are {classes}\")\n","    classes = classes[classes != -1]\n","\n","    nclasses = classes.shape[0]\n","    num_classes_to_select = (nclasses + nclasses % 2) // 2  # Take half of the classes (rounded up if odd)\n","    selected_classes = classes[torch.randint(0, nclasses, (num_classes_to_select,))]\n","    return selected_classes\n","\n","def select_build_and_road_for_mix(soft_hard, classes=None):\n","    if soft_hard == 'Soft' and classes is None:\n","        raise ValueError(\"With 'Soft' mode you should provide the classes through extract_classes_from_mask(mask)\")\n","\n","    if soft_hard == 'Hard':\n","        selected_classes = torch.tensor([1, 2]).to(DEVICE)\n","    elif soft_hard == 'Soft':\n","        # print(f\"classes are {classes}\")\n","        classes = classes[classes != -1]\n","\n","        nclasses = classes.shape[0]\n","        num_classes_to_select = (nclasses + nclasses % 2) // 2 # Take half of the classes (rounded up if odd)\n","        selected_classes = classes[torch.randint(0, nclasses, (num_classes_to_select,))]\n","        selected_classes = torch.tensor([c if c == 0 else c + 2 for c in selected_classes]).to(DEVICE)\n","        if len(selected_classes) > 1:\n","            selected_classes[0] = 1\n","        if len(selected_classes) > 2:\n","            selected_classes[1] = 2\n","    else:\n","        raise ValueError(\"Error with the call of select_build_and_road_for_mix. Param should be either 'Hard' or 'Soft'.\")\n","    return selected_classes\n","\n","# create masks for mixed classes\n","# generate a mask where only pixels from the selected classes are retained\n","def generate_class_mask(labels, selected_classes):\n","    mask = torch.zeros_like(labels)\n","    for class_id in selected_classes:\n","        mask[labels == class_id] = 1 # set to 1 where the class is present\n","    return mask\n","\n","def apply_augmentations(images, augmentation_type):\n","    if augmentation_type == 'strong':\n","        augmentations = T.Compose([\n","            T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n","            T.RandomHorizontalFlip(),\n","            T.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0))\n","        ])\n","    elif augmentation_type == 'weak':\n","        augmentations = T.Compose([\n","            T.RandomHorizontalFlip()\n","        ])\n","    elif augmentation_type == 'None':\n","        augmentations = T.Compose([])\n","\n","    augmented_images = augmentations(images)\n","\n","    return augmented_images\n","\n","# data[0] shape is:     3x512x512       src keeps with mix_mask==1\n","# data[1] shape is:     3x512x512       trg keeps with mix_mask==0\n","# mix_mask shape is:    1x512x512\n","\n","# target[0] has shape:  512x512\n","# target[1] has shape:  512x512\n","# mix_mask shape is:    1x512x512\n","def do_mixing(mix_mask, data=None, target=None, verbose=False):\n","    assert data is not None or target is not None, \"Data or target must be provided.\"\n","\n","    if mix_mask is not None:\n","        # Mix the source and target images/masks\n","        if data is not None:\n","            if verbose: print(f\"mix_mask shape is {mix_mask.shape}. data[0] shape is {data[0].shape}. data[1] shape is {data[1].shape}\")\n","            data = mix_mask * data[0] + (1 - mix_mask) * data[1]\n","            if verbose: print(f\"AFter: mix_mask shape is {mix_mask.shape}. Result data shape is {data.shape}\")\n","        if target is not None:\n","            if verbose: print(f\"mix_mask shape is {mix_mask.shape}. target[0] shape is {target[0].shape}. target[1] shape is {target[1].shape}\")\n","            target = mix_mask * target[0] + (1 - mix_mask) * target[1]\n","            if verbose: print(f\"AFter: mix_mask shape is {mix_mask.shape}. Result target shape is {target.shape}\")\n","\n","    # # For when I upscale to perform on n batches\n","\n","\n","    # # Apply strong augmentations to the mixed data\n","    # if data is not None:\n","    #     data = apply_augmentations(data, augmentation_type='strong')\n","\n","    return data, target\n","\n","\n","# batch_data[0] has shape:  BSx3x512x512\n","# batch_data[1] has shape:  BSx3x512x512\n","# mix_masks has shape:      BSx1x512x512\n","\n","# batch_target[0] has shape:  BSx512x512\n","# batch_target[1] has shape:  BSx512x512\n","# mix_mask shape is:    BSx1x512x512 => I want to bring it down to BSx512x512\n","def do_mixing_fullbatch(mix_masks, batch_data=None, batch_target=None, verbose=False):\n","    assert batch_data is not None or batch_target is not None, \"Data or target must be provided.\"\n","\n","    if mix_masks is not None:\n","        if batch_data is not None:\n","            if verbose: print(f\"mix_masks shape is {mix_masks.shape}. batch_data[0] shape is {batch_data[0].shape}. batch_data[1] shape is {batch_data[1].shape}\")\n","            batch_data = mix_masks * batch_data[0] + (1 - mix_masks) * batch_data[1]\n","            if verbose: print(f\"AFter: mix_masks shape is {mix_masks.shape}. Result batch_data shape is {batch_data.shape}\")\n","        if batch_target is not None:\n","            mix_masks = mix_masks.squeeze(1) # brings down from BSx1x512x512 to BSx512x512\n","            if verbose: print(f\"mix_masks shape is {mix_masks.shape}. batch_target[0] shape is {batch_target[0].shape}. batch_target[1] shape is {batch_target[1].shape}\")\n","            batch_target = mix_masks * batch_target[0] + (1 - mix_masks) * batch_target[1]\n","            if verbose: print(f\"AFter: mix_masks shape is {mix_masks.shape}. Result batch_target shape is {batch_target.shape}\")\n","\n","    return batch_data, batch_target"]},{"cell_type":"markdown","metadata":{"id":"cPFxB02HWn-G"},"source":["### Validation Utils"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1737980679968,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"u09vfXKQWpql"},"outputs":[],"source":["def calculate_intersection_and_union(output, GT_masks, total_intersection, total_union, ignore_unclassified=True, unclassified_label=7):\n","  # output = your output, masks = GT masks\n","\n","  class_indices = torch.argmax(output, dim=1)\n","\n","  for predicted, target in zip(class_indices, GT_masks):\n","    # TODO debug this\n","    if ignore_unclassified:\n","      unclassified_mask = target == unclassified_label\n","      predicted[unclassified_mask] = unclassified_label\n","\n","    for i in range(NUM_CLASSES):\n","        total_intersection[i] += torch.sum(torch.logical_and(predicted == i, target == i))\n","        total_union[i] += torch.sum(torch.logical_or(predicted == i, target == i))\n","\n","  return total_intersection, total_union"]},{"cell_type":"markdown","metadata":{"id":"rOz0Bt-_0uKY"},"source":["### Pixel Weighting Utils"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1737980679968,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"HBb4lKVf1ATr"},"outputs":[],"source":["# Take this from `CalculateClassDistribution.ipynb` in `step4/b` folder\n","class_ratio = {\n","    # 0: 0.0482, unclassified\n","    0: 0.4612,\n","    1: 0.2018,\n","    2: 0.0883,\n","    3: 0.0355,\n","    4: 0.0720,\n","    5: 0.0754,\n","    6: 0.0177,\n","}\n","\n","class_ids = [0, 1, 2, 3, 4, 5, 6]\n","\n","class_ratio_inverse = {}\n","for key, value in class_ratio.items():\n","    class_ratio_inverse[key] = 1 - value\n","\n","def weigh_loss_by_class(pixelweights):\n","    for class_id in class_ids:\n","        pixelweights[pixelweights == class_id] *= class_ratio_inverse[class_id]\n","    pixelweights[pixelweights == -1] = 0 # unclassified is -1\n","\n","    return pixelweights\n","\n","def weigh_unclassified_to_zero(pixelweights):\n","    pixelweights[pixelweights == -1] = 0\n","    return pixelweights\n","\n","def calculate_weight_mask(gt_masks):\n","    for cls in class_ids:\n","        gt_masks[gt_masks == cls] = class_ratio_inverse[cls]\n","    gt_masks[gt_masks == -1] = 0\n","    return gt_masks"]},{"cell_type":"code","source":["print(class_ratio_inverse)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aPCm1sdRsVCX","executionInfo":{"status":"ok","timestamp":1737980679969,"user_tz":-60,"elapsed":13,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"}},"outputId":"232790d5-e386-4d1c-d3e3-7d19bcbe16db"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 0.5388, 1: 0.7982, 2: 0.9117, 3: 0.9645, 4: 0.928, 5: 0.9246, 6: 0.9823}\n"]}]},{"cell_type":"markdown","metadata":{"id":"_rRjwVYaaFG8"},"source":["### Training Loop"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1pedMpl0j8rlU5y8qYKWOoTuWTuQzyu14"},"executionInfo":{"elapsed":798319,"status":"ok","timestamp":1737981478280,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"Y8nlqvfeFpg0","outputId":"89c6f562-dfe8-4efc-c84b-607441688659"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import torch\n","import numpy as np\n","import torch.nn.functional as F\n","\n","# For easier debugging\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","os.environ['TORCH_USE_CUDA_DSA'] = '1'\n","\n","for base_model_name in ['best_model_PIDNET_Jitter+RotateFlip+Resize.pth'] :\n","    model = get_seg_model(cfg, imgnet_pretrained=True)\n","    ## Model is defined some cells above, in LoadPidNetModel\n","    if LOAD_BASE_MODEL:\n","        base_model_path = os.path.join(drive_path, base_model_name)\n","        base_model = torch.load(base_model_path, map_location=DEVICE)\n","        #   base_model = torch.load(f'/content/{base_model_name}', weights_only=True)\n","        model.load_state_dict(base_model)\n","\n","    ## Optimizier and Scheduler\n","    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n","\n","    if USE_EMA_FOR_TARGET:\n","        ema_model = create_ema_model(model)\n","        ema_model.train()\n","        ema_model = ema_model.to(DEVICE)\n","        DETACH_PSEUDO_MASKS = True\n","\n","    if PIXEL_WEIGHTED == False:\n","        pxlwtd_tag = False\n","    elif PIXEL_WEIGHT_SOFT:\n","        pxlwtd_tag = 'Soft'\n","    else:\n","        pxlwtd_tag = 'Hard'\n","    SAVE_MODEL_AS = f'best_DACS_model_{CHOOSE_TRANSFORM}__PRETRN={LOAD_BASE_MODEL}{base_model_name}__LAMBDA_MIXED={LAMBDA_MIXED}__LR={LR}_LRstep={STEP_SIZE}_pxlwtd={pxlwtd_tag}__clswtd={WEIGH_LOSS_BY_CLASS}__DETACH_PSEUDO_MASKS={DETACH_PSEUDO_MASKS}__ema={USE_EMA_FOR_TARGET}_alpha={ALPHA_TEACHER}__BldngsRoads={TAKE_ONLY_BUILDINGS_AND_ROADS}.pth'\n","    MODEL_NAME = SAVE_MODEL_AS\n","\n","    current_step = 0\n","\n","    best_loss = float('inf')\n","    best_model = model.state_dict()\n","\n","    model = model.to(DEVICE)\n","\n","    plot_losses = {\n","        'complete' : [],\n","        'pidnet' : [],\n","        'mixed' : [],\n","    }\n","\n","    #mIoU before\n","    total_union_t = torch.zeros(NUM_CLASSES).to(DEVICE)\n","    total_intersection_t = torch.zeros(NUM_CLASSES).to(DEVICE)\n","    total_union_s = torch.zeros(NUM_CLASSES).to(DEVICE)\n","    total_intersection_s = torch.zeros(NUM_CLASSES).to(DEVICE)\n","    with torch.no_grad():\n","        for batch_i, (batch_s, batch_t) in tqdm(enumerate(zip(source_validation_loader, target_validation_loader))):\n","            images, masks, _, bd_gts = batch_s\n","            images_t, masks_t, _, _ = batch_t\n","            images = images.to(DEVICE)\n","            masks = masks.to(DEVICE)\n","            bd_gts = bd_gts.float().to(DEVICE)\n","            images_t = images_t.to(DEVICE)\n","            masks_t = masks_t.to(DEVICE)\n","            outputs = model(images)\n","            outputs_t = model(images_t)\n","            h, w = masks.size(1), masks.size(2)\n","            ph, pw = outputs[0].size(2), outputs[0].size(3)\n","            if ph != h or pw != w:\n","                for i in range(len(outputs)):\n","                    outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n","                    outputs_t[i] = F.interpolate(outputs_t[i], size=(h, w), mode='bilinear', align_corners=True)\n","\n","\n","            ### ========> Calculate mIoU\n","            ## mIoU on source\n","            class_indices_s = torch.argmax(outputs[1], dim=1)\n","\n","            for predicted, target in zip(class_indices_s, masks):\n","                for i in range(NUM_CLASSES):\n","                    total_intersection_s[i] += torch.sum(torch.logical_and(predicted == i, target == i))\n","                    total_union_s[i] += torch.sum(torch.logical_or(predicted == i, target == i))\n","\n","            ## mIoU on target\n","            class_indices_t = torch.argmax(outputs_t[1], dim=1)\n","\n","            for predicted, target in zip(class_indices_t, masks_t):\n","                for i in range(NUM_CLASSES):\n","                    total_intersection_t[i] += torch.sum(torch.logical_and(predicted == i, target == i))\n","                    total_union_t[i] += torch.sum(torch.logical_or(predicted == i, target == i))\n","\n","        intersection_over_union_s = total_intersection_s / total_union_s\n","        intersection_over_union_t = total_intersection_t / total_union_t\n","        mIoU_s = torch.mean(intersection_over_union_s)\n","        mIoU_t = torch.mean(intersection_over_union_t)\n","        print(f\"mIoU before training. Source: {mIoU_s}. Target = {mIoU_t}\")\n","\n","    best_mIoU_s = 0\n","    best_mIoU_t = 0\n","\n","    print(f\"training model {SAVE_MODEL_AS}\")\n","    for epoch in range(NUM_EPOCHS):\n","        model.train()\n","\n","        # fill with your losses\n","        epoch_losses = {\n","            'loss_complete': 0.0,\n","            'amount': 0, # amt of images I think?\n","            'loss_pidnet': 0.0,\n","            'loss_mixed': 0.0\n","        }\n","\n","        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n","        srcldr_len = len(source_loader)\n","        tgtldr_len = len(target_loader)\n","        print(f\"len of source_loader is {srcldr_len}, len of target_loader is {tgtldr_len}. WARNING: make sure source is shorter than target, or modify code to fix early stopping\")\n","\n","        # ================================== Train Epoch X ==================================\n","        for batch_i, (batch_s, batch_t) in tqdm(enumerate(zip(source_loader, target_loader)), total=len(source_loader)):\n","            optimizer.zero_grad()\n","\n","            ### Extract input\n","            image_list_source, masks_list_source, img_path_source, bd_gts_list_source = batch_s\n","            image_list_target, _, _, _ = batch_t\n","\n","            index = 0\n","            for images, masks, bd_gts, images_t in zip(image_list_source, masks_list_source, bd_gts_list_source, image_list_target):\n","                ### Extract Images, Masks, Boundaries\n","                # Source\n","                images = images.to(DEVICE)\n","                masks = masks.to(DEVICE)\n","                bd_gts = bd_gts.float().to(DEVICE)\n","                # print(f\"DEBUG: source_image batch shape: {images.shape}\")\n","                # print(f\"DEBUG: source_mask batch shape: {masks.shape}\")\n","\n","                # Target\n","                images_t = images_t.to(DEVICE)\n","                # print(f\"DEBUG: target_image batch shape: {images.shape}\")\n","\n","\n","                ### ===> Train Segmentation --- on Source\n","                ## Forward\n","                outputs = model(images) # in model.train() mode batch size must be > 1 I think\n","                                        # NOTE: we have 3 heads (i.e. 3 outputs) but 4 losses: 2nd head is used for both S and BAS\n","\n","                ## Upscale (bilinear interpolation - not learned)\n","                h, w = masks.size(1), masks.size(2)\n","                ph, pw = outputs[0].size(2), outputs[0].size(3)\n","                if ph != h or pw != w:\n","                    for i in range(len(outputs)):\n","                        outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n","\n","                ## Losses\n","                # Semantic Losses (l_0 and l_2)\n","                loss_s = sem_loss(outputs[:-1], masks) # output #1 and #2 are segmentation predictions (i.e. low level (P) and high+low level (PI) respectively)\n","\n","                # Boundary Loss (l_1)\n","                loss_b = bd_loss(outputs[-1], bd_gts) # output #3 is the boundary prediction\n","\n","                # Boundary AwareneSS (BAS) Loss (l_3)\n","                filler = torch.ones_like(masks) * -1\n","                bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:]) > 0.8, masks, filler)\n","                                    # REMEMBER to wrap in list, as the checks in ce use that to know what to do\n","                loss_sb = sem_loss([outputs[-2]], bd_label) # output #2 is the PI segmentation prediction, done here in BAS mode (see `filler` variable)\n","\n","                # Segmentation Loss w/o DA (on source domain, which has labels)\n","                loss_pidnet = loss_s + loss_b + loss_sb  # The coefficients of the sum of the four losses (0.4, 20, 1, 1) are taken into account in the various `sem_loss` and `bd_loss`\n","                if batch_i % LOG_FREQUENCY == 0:\n","                    print(f'Batch {batch_i}) loss_pidnet: {loss_pidnet.item()}', end=\" --- \")\n","                epoch_losses['loss_pidnet'] += loss_pidnet.item()\n","\n","                ## Backprop\n","                epoch_losses['loss_complete'] += loss_pidnet.item()\n","                epoch_losses['loss_pidnet'] += loss_pidnet.item()\n","                loss_pidnet.backward()\n","                ### <=== Train Segmentation --- on Source\n","\n","                ### ===> Train Segmentation --- on Mixed\n","                weak_augmented_target = apply_augmentations(images_t, TARGET_TRANSFORMS)\n","\n","                ## Forward\n","                if USE_EMA_FOR_TARGET:\n","                    outputs_t = ema_model(weak_augmented_target)\n","                else:\n","                    outputs_t = model(weak_augmented_target)\n","\n","                ## Upscale\n","                h, w = images_t.size(2), images_t.size(3)\n","                ph, pw = outputs_t[0].size(2), outputs_t[0].size(3)\n","                output_t = F.interpolate(outputs_t[-2], size=(h, w), mode='bilinear', align_corners=True)\n","\n","                ## Create pseudo masks\n","                if DETACH_PSEUDO_MASKS:\n","                    output_t = output_t.detach()\n","                pseudo_label = torch.softmax(output_t, dim=1)\n","                max_probs, pseudo_masks = torch.max(pseudo_label, dim=1)\n","                # pseudo_masks = torch.argmax(output_t, dim=1)  # Get predicted classes\n","\n","                ## Extract MixMasks\n","                MixMasks = []\n","                for i in range(images.size(0)):\n","                    if TAKE_ONLY_BUILDINGS_AND_ROADS == 'Hard':\n","                        MixMask = generate_class_mask(masks[i], select_build_and_road_for_mix('Hard')).unsqueeze(0)\n","                    elif TAKE_ONLY_BUILDINGS_AND_ROADS == 'Soft':\n","                        MixMask = generate_class_mask(masks[i], select_build_and_road_for_mix('Soft', extract_classes_from_mask(masks[i]))).unsqueeze(0)\n","                    else:\n","                        MixMask = generate_class_mask(masks[i], select_classes_for_mix(extract_classes_from_mask(masks[i]))).unsqueeze(0)\n","\n","                    if REV_TAKE_ONLY_BUILDINGS_AND_ROADS == 'Buildings':\n","                        MixMask[pseudo_masks[i].unsqueeze(0) == 1] = 1\n","                    elif REV_TAKE_ONLY_BUILDINGS_AND_ROADS == 'BuildingsAndRoads':\n","                        MixMask[pseudo_masks[i].unsqueeze(0) == 1] = 1\n","                        MixMask[pseudo_masks[i].unsqueeze(0) == 2] = 1\n","                    MixMasks.append(MixMask)\n","                MixMasks = torch.stack(MixMasks, dim=0)\n","                MixMasks = MixMasks.to(DEVICE)\n","\n","                ## Mix images\n","                source_target_stacked_imgs = torch.stack([images, images_t], dim=0)\n","                source_target_stacked_masks = torch.stack([masks, pseudo_masks], dim=0)\n","                mixed_images, _ = do_mixing_fullbatch(MixMasks, batch_data=source_target_stacked_imgs)\n","                _, mixed_masks = do_mixing_fullbatch(MixMasks, batch_target=source_target_stacked_masks)\n","\n","                ## Upscale\n","                outputs_m = model(mixed_images)\n","                h, w = images_t.size(2), images_t.size(3)\n","                ph, pw = outputs_m[0].size(2), outputs_t[0].size(3)\n","                output_m = F.interpolate(outputs_m[-2], size=(h, w), mode='bilinear', align_corners=True)\n","\n","                ## Weight based on confidence\n","                if PIXEL_WEIGHTED:\n","                    pixelWiseWeight = max_probs.ge(0.968).float().to(DEVICE)\n","\n","                    if PIXEL_WEIGHT_SOFT:\n","                        # by using a big power, really low confidence predictions are set to much lower weights\n","                        softWeights = torch.pow(max_probs, 5, out=max_probs).to(DEVICE)\n","                        pixelWiseWeight[pixelWiseWeight == 0] = softWeights[pixelWiseWeight == 0]\n","                        # del softWeights\n","\n","                    if WEIGH_LOSS_BY_CLASS:\n","                        pixelWiseWeight = weigh_loss_by_class(pixelWiseWeight)\n","\n","                    onesWeights = torch.ones((pixelWiseWeight.shape)).to(DEVICE)\n","                    target_stacked_weights = torch.stack([onesWeights, pixelWiseWeight], dim=0)\n","                    _, pixelWiseWeight = do_mixing_fullbatch(MixMasks, batch_target=target_stacked_weights)\n","                else:\n","                    pixelWiseWeight = 1\n","\n","                                # Careful: sem_loss hardcodes some weights, but the weight for single output loss calculation is 1, thus using it as of now is fine\n","                loss_mixed = LAMBDA_MIXED * mix_ce_loss_wtd(output_m, mixed_masks, pixelWiseWeight)\n","                if batch_i % LOG_FREQUENCY == 0:\n","                    print(f'loss_mixed: {loss_mixed.item()}', end=\" --- \")\n","                epoch_losses['loss_complete'] += loss_mixed.item()\n","                epoch_losses['loss_mixed'] += loss_mixed.item()\n","                loss_mixed.backward()\n","                ### <=== Train Segmentation --- on Mixed\n","\n","                if batch_i % LOG_FREQUENCY == 0:\n","                    print(f'loss_complete: {loss_pidnet.item() + loss_mixed.item()}', end=\" --- \")\n","\n","                optimizer.step()\n","                if USE_EMA_FOR_TARGET:\n","                    ema_model = update_ema_variables(ema_model, model)\n","                epoch_losses['amount'] += images.size(0)\n","\n","        # ================================== Validation Main Model ==================================\n","        # Validate model on the evaluation set and save the parameters if is better than best model\n","        model.eval()\n","\n","        total_union_t = torch.zeros(NUM_CLASSES).to(DEVICE)\n","        total_intersection_t = torch.zeros(NUM_CLASSES).to(DEVICE)\n","        total_union_s = torch.zeros(NUM_CLASSES).to(DEVICE)\n","        total_intersection_s = torch.zeros(NUM_CLASSES).to(DEVICE)\n","\n","        amount = 0\n","\n","        outputs = []\n","        with torch.no_grad():\n","            for batch_i, (batch_s, batch_t) in tqdm(enumerate(zip(source_validation_loader, target_validation_loader))):\n","                ### Extract input\n","                images, masks, _, bd_gts = batch_s\n","                images_t, masks_t, _, _ = batch_t\n","\n","                ### Extract Images, Masks, Boundaries\n","                # Source\n","                images = images.to(DEVICE)\n","                masks = masks.to(DEVICE)\n","                bd_gts = bd_gts.float().to(DEVICE)\n","\n","                # Target\n","                images_t = images_t.to(DEVICE)\n","                masks_t = masks_t.to(DEVICE) # WARNING: idk if taking the labels of Rural for the validation is legal\n","\n","                ### ===> Validate Segmentation --- on Source\n","                ## Forward\n","                outputs = model(images) # in model.train() mode batch size must be > 1 I think\n","                                        # NOTE: we have 3 heads (i.e. 3 outputs) but 4 losses: 2nd head is used for both S and BAS\n","\n","                ## Upscale (bilinear interpolation - not learned)\n","                h, w = masks.size(1), masks.size(2)\n","                ph, pw = outputs[0].size(2), outputs[0].size(3)\n","                if ph != h or pw != w:\n","                    for i in range(len(outputs)):\n","                        outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n","\n","                # ## Losses\n","                # loss_s = sem_loss(outputs[:-1], masks)\n","                # loss_b = bd_loss(outputs[-1], bd_gts)\n","                # filler = torch.ones_like(masks) * -1\n","                # bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:]) > 0.8, masks, filler)\n","                # loss_sb = sem_loss([outputs[-2]], bd_label)\n","                # loss_source = loss_s + loss_b + loss_sb\n","\n","                ### <=== Validate Segmentation --- on Source\n","\n","                ### ===> Validate Segmentation --- on Target\n","                ## Forward\n","                outputs_t = model(images_t)\n","\n","                ## Upscale\n","                h, w = images_t.size(2), images_t.size(3)\n","                ph, pw = outputs_t[0].size(2), outputs_t[0].size(3)\n","                output_t = F.interpolate(outputs_t[-2], size=(h, w), mode='bilinear', align_corners=True)\n","\n","                # ## Loss\n","                # loss_target = sem_loss(output_t, masks_t)\n","                ### ========> Calculate mIoU\n","                ## mIoU on source\n","                class_indices_s = torch.argmax(outputs[1], dim=1)\n","\n","                for predicted, target in zip(class_indices_s, masks):\n","                    for i in range(NUM_CLASSES):\n","                        total_intersection_s[i] += torch.sum(torch.logical_and(predicted == i, target == i))\n","                        total_union_s[i] += torch.sum(torch.logical_or(predicted == i, target == i))\n","\n","\n","                # mIoU on target\n","                class_indices_t = torch.argmax(output_t, dim=1)\n","\n","                for predicted, target in zip(class_indices_t, masks_t):\n","                    for i in range(NUM_CLASSES):\n","                        total_intersection_t[i] += torch.sum(torch.logical_and(predicted == i, target == i))\n","                        total_union_t[i] += torch.sum(torch.logical_or(predicted == i, target == i))\n","                ### <======== Calculate mIoU\n","\n","        intersection_over_union_s = total_intersection_s / total_union_s\n","        intersection_over_union_t = total_intersection_t / total_union_t\n","        mIoU_s = torch.mean(intersection_over_union_s)\n","        mIoU_t = torch.mean(intersection_over_union_t)\n","\n","        print(f'Epoch {epoch+1}, mIoU_s = {mIoU_s}, mIoU_t = {mIoU_t}, difference = {mIoU_s - mIoU_t}')\n","        if (epoch+1) > DONT_SAVE_BEFORE_EPOCH:\n","            ### Saving the best mIoU on Rural (target t)\n","            if mIoU_t > best_mIoU_t:\n","                best_mIoU_t = mIoU_t\n","                best_mIoU_t_epoch = epoch\n","                best_model = model.state_dict()\n","                #Save in Drive and local\n","                SAVE_RURAL_MODEL_AS = SAVE_MODEL_AS[-4] + '__bestmIoURural.pth'\n","                torch.save(best_model, SAVE_RURAL_MODEL_AS)\n","                print(\"===========================================\")\n","                print(\"Best RURAL mIoU model on validation split. Saving\")\n","                if SAVE_ON_DRIVE:\n","                    !cp {SAVE_RURAL_MODEL_AS} /content/drive/MyDrive/LoveDA/{SAVE_RURAL_MODEL_AS}\n","                    print(f\"{SAVE_RURAL_MODEL_AS} model succesfully saved on drive. RURAL mIoU went down to {best_mIoU_t}\")\n","\n","            # ### Saving the best mIoU on Urban (not our focus but let's see) (source s)\n","            # if mIoU_s > best_mIoU_s:\n","            #     best_mIoU_s = mIoU_s\n","            #        best_mIoU_s_epoch = epoch\n","            #     best_model = model.state_dict()\n","            #     #Save in Drive and local\n","            #     SAVE_URBAN_MODEL_AS = SAVE_MODEL_AS[-4] + '__bestmIoUUrban.pth'\n","            #     torch.save(best_model, SAVE_URBAN_MODEL_AS)\n","            #     print(\"===========================================\")\n","            #     print(\"BEST MODEL ON VALIDATION SET, SAVING\")\n","            #     if SAVE_ON_DRIVE:\n","            #         !cp {SAVE_URBAN_MODEL_AS} /content/drive/MyDrive/LoveDA/{SAVE_URBAN_MODEL_AS}\n","            #         print(f\"{SAVE_URBAN_MODEL_AS} model succesfully saved on drive. URBAN mIoU went down to {best_mIoU_s}\")\n","\n","        if (epoch+1) >= SAVE_AFTER_EPOCH:\n","            torch.save(best_model, f'{MODEL_NAME}_{epoch+1}.pth')# Save model\n","            if SAVE_ON_DRIVE:\n","                EPOCH_MODEL_PATH = f'{SAVE_MODEL_AS}_{epoch+1}.pth'\n","                !cp {EPOCH_MODEL_PATH} /content/drive/MyDrive/LoveDA/{EPOCH_MODEL_PATH}\n","                print(f\"{EPOCH_MODEL_PATH} succesfully saved on drive. loss went down to {best_loss}\")\n","\n","        current_step += 1\n","        scheduler.step()\n","\n","        plot_losses['complete'].append(epoch_losses['loss_complete'] / epoch_losses['amount'])\n","        plot_losses['pidnet'].append(epoch_losses['loss_pidnet'] / epoch_losses['amount'])\n","        plot_losses['mixed'].append(epoch_losses['loss_mixed'] / epoch_losses['amount'])\n","\n","        amt = epoch_losses['amount']\n","        print(f'[EPOCH {epoch+1:02}] Avg. Losses:')\n","        print(f'{\"loss_complete\":<20} {epoch_losses[\"loss_complete\"] / amt:.6f}')\n","        print(f'{\"loss_pidnet\":<20} {epoch_losses[\"loss_pidnet\"] / amt:.6f}')\n","        print(f'{\"loss_mixed\":<20} {epoch_losses[\"loss_mixed\"] / amt:.6f}')\n","\n","        # print(\"========================================================================\")\n","        # print(\"Validation\")\n","        # amt = epoch_losses_eval['amount']\n","        # print(f'[EPOCH {epoch+1:02}] Avg. Losses:')\n","        # print(f'{\"loss_complete\":<20} {epoch_losses_eval[\"loss_complete\"] / amt:.6f}')\n","        # print(f'{\"loss_pidnet\":<20} {epoch_losses_eval[\"loss_pidnet\"] / amt:.6f}')\n","        # print(f'{\"loss_target\":<20} {epoch_losses_eval[\"loss_target\"] / amt:.6f}')\n","        # print(\"========================================================================\")\n","\n","        # Create a figure with 1 row and 3 columns\n","        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","        # Plot the original image\n","        axes[0].imshow(images[0].permute(1, 2, 0).cpu().numpy())\n","        axes[0].set_title(\"Original Image\")\n","        axes[0].axis('off')\n","\n","        # Plot the ground truth mask\n","        # axes[1].imshow(masks[0].cpu().numpy())\n","        # axes[1].set_title(\"Ground Truth\")\n","        # axes[1].axis('off')\n","\n","        # # Show mask\n","        adapted_mask = new_colors_mask(masks[0].cpu().numpy())\n","        plt.legend(handles=getLegendHandles(), loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n","        axes[1].imshow(adapted_mask)\n","        axes[1].set_title(\"Ground Truth\")\n","        axes[1].axis('off')\n","\n","        # Plot the predicted mask\n","        # axes[2].imshow(torch.argmax(outputs[0][0], dim=0).cpu().numpy())\n","        # axes[2].set_title(\"Predicted Mask\")\n","        # axes[2].axis('off')\n","\n","        adapted_mask_2 = new_colors_mask(torch.argmax(outputs[0][0], dim=0).cpu().numpy())\n","        plt.legend(handles=getLegendHandles(), loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n","        axes[2].imshow(adapted_mask_2)\n","        axes[2].set_title(\"Predicted Mask\")\n","        axes[2].axis('off')\n","\n","        # Display the figure\n","        plt.tight_layout()\n","        plt.show()\n","\n","    plt.plot(plot_losses['complete'], label='complete')\n","    plt.plot(plot_losses['pidnet'], label='pidnet')\n","    plt.plot(plot_losses['mixed'], label='mixed')\n","    plt.legend()\n","    plt.show()\n","\n","    try:\n","        new_name = SAVE_MODEL_AS[:-4] + f'@epch{best_mIoU_t_epoch}.pth'\n","        os.rename(SAVE_RURAL_MODEL_AS, new_name)\n","        old_drive_name = f'/content/drive/MyDrive/LoveDA/{SAVE_RURAL_MODEL_AS}'\n","        new_drive_name = f'/content/drive/MyDrive/LoveDA/{new_name}'\n","        os.rename(old_drive_name, new_drive_name)\n","    except Exception as e:\n","        print(\"Error while renaming best model to add its epoch: \", e)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1737981478281,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"OBTNg48LufOD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"825c303e-6ae9-4f3f-e241-c7e99e1b65e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Models in local are:\n","best_DACS_model_Resize512__PRETRN=Truebest_model_PIDNET_Jitter+RotateFlip+Resize.pth__LAMBDA_MIXED=18__LR=1e-06_LRstep=21_pxlwtd=Hard__clswtd=True__DETACH_PSEUDO_MASKS=False__ema=False_alpha=0.99__BldngsRoads=No@epch15.pth\n","best_DACS_model_Resize512__PRETRN=Truebest_model_PIDNET_Jitter+RotateFlip+Resize.pth__LAMBDA_MIXED=18__LR=1e-06_LRstep=21_pxlwtd=Hard__clswtd=True__DETACH_PSEUDO_MASKS=False__ema=False_alpha=0.99__BldngsRoads=No.pth_20.pth\n"]}],"source":["print(\"Models in local are:\")\n","files = os.listdir('.')\n","models = [f for f in files if f.endswith('.pth')]\n","for model in models:\n","    print(model)"]},{"cell_type":"markdown","metadata":{"id":"qHQ-M4jeaHOV"},"source":["### MixMasks Debug"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1737981478281,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"h7LSxIi3aJ3V"},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","# from tqdm import tqdm\n","# import torch\n","# import numpy as np\n","# import torch.nn.functional as F\n","\n","\n","# model = model.to(DEVICE)\n","# for batch_i, (batch_s, batch_t) in tqdm(enumerate(zip(source_loader, target_loader)), total=len(source_loader)):\n","#     if batch_i == 0:\n","#         continue\n","#     optimizer.zero_grad()\n","#     image_list_source, masks_list_source, img_path_source, bd_gts_list_source = batch_s\n","#     image_list_target, _, _, _ = batch_t\n","\n","#     index = 0\n","#     for images, masks, bd_gts, images_t in zip(image_list_source, masks_list_source, bd_gts_list_source, image_list_target):\n","#         images = images.to(DEVICE)\n","#         masks = masks.to(DEVICE)\n","#         images_t = images_t.to(DEVICE)\n","#         weak_augmented_target = apply_augmentations(images_t, TARGET_TRANSFORMS)\n","#         outputs_t = model(weak_augmented_target)\n","#         h, w = images_t.size(2), images_t.size(3)\n","#         ph, pw = outputs_t[0].size(2), outputs_t[0].size(3)\n","#         output_t = F.interpolate(outputs_t[-2], size=(h, w), mode='bilinear', align_corners=True)\n","#         pseudo_masks = torch.softmax(output_t.detach(), dim=1)\n","#         max_probs, pseudo_labels = torch.max(pseudo_masks, dim=1)\n","\n","#         # mixed_images, mixed_masks = [], []\n","#         # for i in range(images.size(0)):\n","#         #     MixMask = generate_class_mask(masks[i], select_classes_for_mix(extract_classes_from_mask(masks[i]))).unsqueeze(0)\n","\n","#         #     mixed_img, _ = do_mixing(MixMask, data=torch.cat((images[i].unsqueeze(0), images_t[i].unsqueeze(0)), dim=0))\n","#         #     _, mixed_mask = do_mixing(MixMask, target=torch.cat((masks[i].unsqueeze(0), pseudos_label[i].unsqueeze(0)), dim=0))\n","\n","#         #     mixed_images.append(mixed_img.unsqueeze(0))\n","#         #     mixed_masks.append(mixed_mask)\n","\n","#         # mixed_images = torch.cat(mixed_images)\n","#         # mixed_masks = torch.cat(mixed_masks).long()\n","\n","#         ## Try doing it with batches\n","#         MixMasks = []\n","#         for i in range(images.size(0)):\n","#             # if TAKE_ONLY_BUILDINGS_AND_ROADS == 'Hard':\n","#             selected_classes = select_build_and_road_for_mix('Hard')\n","#             print(f\"Selected classes using 'Hard' are: {selected_classes}\")\n","#             MixMask = generate_class_mask(masks[i], selected_classes).unsqueeze(0)\n","#             # elif TAKE_ONLY_BUILDINGS_AND_ROADS == 'Soft':\n","#             selected_classes = select_build_and_road_for_mix('Soft', extract_classes_from_mask(masks[i]))\n","#             print(f\"Selected classes using 'Soft' are: {selected_classes}\")\n","#                 # MixMask = generate_class_mask(masks[i], selected_classes).unsqueeze(0)\n","#             # else:\n","#             selected_classes = select_classes_for_mix(extract_classes_from_mask(masks[i]))\n","#             print(f\"Selected classes at random are: {selected_classes}\")\n","#                 # MixMask = generate_class_mask(masks[i], selected_classes).unsqueeze(0)\n","\n","#             # if REV_TAKE_ONLY_BUILDINGS_AND_ROADS == 'Buildings':\n","#             # MixMask[pseudo_labels[i].unsqueeze(0) == 1] = 1\n","#             # elif REV_TAKE_ONLY_BUILDINGS_AND_ROADS == 'BuildingsAndRoads':\n","#             MixMask[pseudo_labels[i].unsqueeze(0) == 1] = 1\n","#             MixMask[pseudo_labels[i].unsqueeze(0) == 2] = 1\n","#             MixMasks.append(MixMask)\n","#         MixMasks = torch.stack(MixMasks, dim=0)\n","#         MixMasks = MixMasks.to(DEVICE)\n","\n","#         source_target_stacked_imgs = torch.stack([images, images_t], dim=0)\n","#         source_target_stacked_masks = torch.stack([masks, pseudo_labels], dim=0)\n","\n","#         mixed_images, _ = do_mixing_fullbatch(MixMasks, batch_data=source_target_stacked_imgs)\n","#         _, mixed_masks = do_mixing_fullbatch(MixMasks, batch_target=source_target_stacked_masks)\n","#         break\n","\n","#     break"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1737981478281,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"6hGmcorojhH4"},"outputs":[],"source":["# HOW_MANY_IMAGES = 8\n","\n","# for i in range(HOW_MANY_IMAGES):\n","#     image = images[i]\n","#     image_t = images_t[i]\n","#     mixed_image = mixed_images[i]\n","#     mask = masks[i]\n","#     mixed_mask = mixed_masks[i]\n","#     threshold = 0.6\n","\n","#     # Create a confident mask for all pixels based on the global threshold\n","#     confident_mask = max_probs.ge(threshold)\n","\n","#     #Apply the confident mask to the pseudo_label\n","#     pseudo_labels = pseudo_labels * confident_mask\n","#     pseudo_mask = pseudo_labels[i]\n","\n","#     print(f\"Max prob distribution: {max_probs.min()} - {max_probs.max()} - {max_probs.mean()}\")\n","\n","#     fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n","\n","#     axes = axes.flatten()\n","\n","#     axes[0].imshow(image.permute(1, 2, 0).cpu().numpy())\n","#     axes[0].set_title(\"Original Image\")\n","#     axes[0].axis('off')\n","\n","#     adapted_mask = new_colors_mask(mask.cpu().numpy())\n","#     plt.legend(handles=getLegendHandles(), loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n","#     axes[1].imshow(adapted_mask)\n","#     axes[1].set_title(\"Mask\")\n","#     axes[1].axis('off')\n","\n","#     axes[2].imshow(image_t.permute(1, 2, 0).cpu().numpy())\n","#     axes[2].set_title(\"Target Image\")\n","#     axes[2].axis('off')\n","\n","#     axes[3].imshow(mixed_image.permute(1, 2, 0).cpu().numpy())\n","#     axes[3].set_title(\"Mixed Image\")\n","#     axes[3].axis('off')\n","\n","#     adapted_mask_2 = new_colors_mask(mixed_mask.cpu().numpy())\n","#     plt.legend(handles=getLegendHandles(), loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n","#     axes[4].imshow(adapted_mask_2)\n","#     axes[4].set_title(\"Mixed Mask\")\n","#     axes[4].axis('off')\n","\n","#     adapted_pseudo_mask = new_colors_mask(pseudo_mask.cpu().numpy())\n","#     plt.legend(handles=getLegendHandles(), loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n","#     axes[5].imshow(adapted_pseudo_mask)\n","#     axes[5].set_title(\"Pseudo Mask\")\n","#     axes[5].axis('off')\n","\n","#     plt.tight_layout()\n","#     plt.show()"]},{"cell_type":"markdown","metadata":{"id":"soROYpcYyajp"},"source":["# TEST"]},{"cell_type":"markdown","metadata":{"id":"XzpLZ4IWIOlJ"},"source":["### Load other models from drive"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1737981478281,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"xkHefAtcIQ5m"},"outputs":[],"source":["# # Run the first cell pls\n","# import os\n","\n","# BASE_PATH = \"/content/drive/MyDrive/LoveDA/\"\n","# INTEREST = 'best_DACS_model_Resize512__PRETRN=Truebest_model_Resize__bestmIoURural.pth__LAMBDA_MIXED=25__LR=1e-05_LRstep=14_pxlwtd=Hard__clswtd=False__DETACH_PSEUDO_MASKS=True__ema=True_alpha=0.99__bestmIoURural.pth'\n","\n","# all_mydrive = os.listdir(BASE_PATH)\n","# all_models = [f for f in all_mydrive if f.endswith('.pth')]\n","# models_of_interest = [f for f in all_models if INTEREST in f]\n","# for model_name in models_of_interest:\n","#     if not os.path.exists(model_name):\n","#         print(f\"Copying {model_name} locally from MyDrive\")\n","#         !cp {BASE_PATH+model_name} ."]},{"cell_type":"markdown","source":["### Test Loop"],"metadata":{"id":"dzelOZ8A9qdX"}},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":2615,"status":"ok","timestamp":1737981480892,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"7DKgjGPKydLH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d34f6a77-7502-46f0-d6bd-2952f8c89cbc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchmetrics\n","  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n","Collecting ptflops\n","  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu121)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->torchmetrics) (12.6.85)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n","Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ptflops-0.7.4-py3-none-any.whl (19 kB)\n","Downloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n","Installing collected packages: lightning-utilities, torchmetrics, ptflops\n","Successfully installed lightning-utilities-0.11.9 ptflops-0.7.4 torchmetrics-1.6.1\n"]}],"source":["!pip install torchmetrics ptflops"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":110997,"status":"ok","timestamp":1737981591884,"user":{"displayName":"Gianni Trattore","userId":"13760690802088993741"},"user_tz":-60},"id":"M3c7DFDRymnI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c89143ad-4cea-4ec4-e30e-a7a85f20aae2"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-c45f6e9ccf7a>:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  pretrained_dict = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')\n"]},{"output_type":"stream","name":"stdout","text":["['best_DACS_model_Resize512__PRETRN=Truebest_model_PIDNET_Jitter+RotateFlip+Resize.pth__LAMBDA_MIXED=18__LR=1e-06_LRstep=21_pxlwtd=Hard__clswtd=True__DETACH_PSEUDO_MASKS=False__ema=False_alpha=0.99__BldngsRoads=No@epch15.pth', 'best_DACS_model_Resize512__PRETRN=Truebest_model_PIDNET_Jitter+RotateFlip+Resize.pth__LAMBDA_MIXED=18__LR=1e-06_LRstep=21_pxlwtd=Hard__clswtd=True__DETACH_PSEUDO_MASKS=False__ema=False_alpha=0.99__BldngsRoads=No.pth_20.pth']\n","Dataset size: 992\n","Testing model=best_DACS_model_Resize512__PRETRN=Truebest_model_PIDNET_Jitter+RotateFlip+Resize.pth__LAMBDA_MIXED=18__LR=1e-06_LRstep=21_pxlwtd=Hard__clswtd=True__DETACH_PSEUDO_MASKS=False__ema=False_alpha=0.99__BldngsRoads=No@epch15.pth on domain=Rural on a ActualTest split, with ignoring unclassified class to False\n","Latency: 0.0252 seconds\n","FLOPs: 12682936320\n","Total number of parameters: 7717839\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 31/31 [00:26<00:00,  1.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy on the target domain: 49.32%\n","Background IoU: 0.4573\n","Building IoU: 0.2320\n","Road IoU: 0.2203\n","Water IoU: 0.3131\n","Barren IoU: 0.0459\n","Forest IoU: 0.1271\n","Agricultural IoU: 0.1287\n","\n","mIoU on the Rural domain: 0.21774038672447205\n","========================================================================\n","Dataset size: 677\n","Testing model=best_DACS_model_Resize512__PRETRN=Truebest_model_PIDNET_Jitter+RotateFlip+Resize.pth__LAMBDA_MIXED=18__LR=1e-06_LRstep=21_pxlwtd=Hard__clswtd=True__DETACH_PSEUDO_MASKS=False__ema=False_alpha=0.99__BldngsRoads=No@epch15.pth on domain=Urban on a ActualTest split, with ignoring unclassified class to False\n","Latency: 0.0228 seconds\n","FLOPs: 12682936320\n","Total number of parameters: 7717839\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 21/21 [00:20<00:00,  1.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy on the target domain: 44.81%\n","Background IoU: 0.2852\n","Building IoU: 0.3611\n","Road IoU: 0.3715\n","Water IoU: 0.4665\n","Barren IoU: 0.0641\n","Forest IoU: 0.3609\n","Agricultural IoU: 0.0617\n","\n","mIoU on the Urban domain: 0.2815602421760559\n","========================================================================\n","Dataset size: 992\n","Testing model=best_DACS_model_Resize512__PRETRN=Truebest_model_PIDNET_Jitter+RotateFlip+Resize.pth__LAMBDA_MIXED=18__LR=1e-06_LRstep=21_pxlwtd=Hard__clswtd=True__DETACH_PSEUDO_MASKS=False__ema=False_alpha=0.99__BldngsRoads=No.pth_20.pth on domain=Rural on a ActualTest split, with ignoring unclassified class to False\n","Latency: 0.0237 seconds\n","FLOPs: 12682936320\n","Total number of parameters: 7717839\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 31/31 [00:27<00:00,  1.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy on the target domain: 49.90%\n","Background IoU: 0.4666\n","Building IoU: 0.2666\n","Road IoU: 0.2040\n","Water IoU: 0.2969\n","Barren IoU: 0.0453\n","Forest IoU: 0.1126\n","Agricultural IoU: 0.1133\n","\n","mIoU on the Rural domain: 0.21504715085029602\n","========================================================================\n","Dataset size: 677\n","Testing model=best_DACS_model_Resize512__PRETRN=Truebest_model_PIDNET_Jitter+RotateFlip+Resize.pth__LAMBDA_MIXED=18__LR=1e-06_LRstep=21_pxlwtd=Hard__clswtd=True__DETACH_PSEUDO_MASKS=False__ema=False_alpha=0.99__BldngsRoads=No.pth_20.pth on domain=Urban on a ActualTest split, with ignoring unclassified class to False\n","Latency: 0.0226 seconds\n","FLOPs: 12682936320\n","Total number of parameters: 7717839\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 21/21 [00:20<00:00,  1.03it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy on the target domain: 45.07%\n","Background IoU: 0.2891\n","Building IoU: 0.3699\n","Road IoU: 0.3707\n","Water IoU: 0.4545\n","Barren IoU: 0.0625\n","Forest IoU: 0.3616\n","Agricultural IoU: 0.0578\n","\n","mIoU on the Urban domain: 0.28086960315704346\n","========================================================================\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["from torchmetrics import Accuracy\n","from tqdm import tqdm\n","import time\n","import ptflops\n","import os\n","\n","IGNORE_UNCLASSIFIED = False\n","BATCH_SIZE = 32\n","\n","test_augmentation = AUGMENTATIONS['Resize512']\n","# target_type = 'ActualTest'\n","NUM_CLASSES = len(LABEL_MAP)\n","\n","\n","# Create unweighted models\n","model = get_seg_model(cfg, imgnet_pretrained=False)\n","\n","model_files_paths = [f for f in os.listdir('.') if f.endswith('.pth')]\n","print(model_files_paths)\n","\n","for model_file_path in model_files_paths:\n","    best_model = torch.load(model_file_path, weights_only=True)\n","\n","    model.load_state_dict(best_model)\n","    model = model.to(DEVICE)\n","\n","    accuracy, mIoU = True, True\n","\n","\n","    TARGETs = ['Rural', 'Urban']\n","    for TARGET in TARGETs:\n","        if TARGET == 'Urban':\n","            target_type = 'ActualTest'\n","            #target_type = 'Validation'\n","        elif TARGET == 'Rural':\n","            target_type = 'ActualTest'\n","            #target_type = 'Total'\n","\n","        test_dataset = LoveDADataset(baseTransform=test_augmentation, augTransforms=None, split=TARGET, type=target_type, useBoundaries=False)\n","        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True, pin_memory=True)\n","\n","        #### TEST LOOP\n","        model.eval()\n","        print(f\"Testing model={model_file_path} on domain={TARGET} on a {target_type} split, with ignoring unclassified class to {IGNORE_UNCLASSIFIED}\")\n","\n","        # Latency\n","        with torch.no_grad():\n","            start_time = time.time()\n","            for _ in range(100):\n","                _ = model(torch.randn(1, 3, RESIZE, RESIZE).to(DEVICE))\n","            end_time = time.time()\n","        latency = (end_time - start_time) / 100\n","        print(f\"Latency: {latency:.4f} seconds\")\n","\n","        # FLOPs\n","        macs, _ = ptflops.get_model_complexity_info(model,\n","            (3, RESIZE, RESIZE), as_strings=False,\n","            print_per_layer_stat=False, verbose=False)\n","        flops = macs * 2  # MACs perform two FLOPs\n","        print(\"FLOPs:\", flops)\n","\n","        # Number of parameters\n","        total_params = sum(p.numel() for p in model.parameters())\n","        print(f\"Total number of parameters: {total_params}\")\n","\n","        with torch.no_grad():\n","            total_union = torch.zeros(NUM_CLASSES).to(DEVICE)\n","            total_intersection = torch.zeros(NUM_CLASSES).to(DEVICE)\n","            meter = Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(DEVICE)\n","            for (batch) in tqdm(test_loader):\n","                ### Extract input\n","                images, masks, img_path, bd_gts = batch\n","                images = images.float().to(DEVICE)\n","                masks = masks.to(DEVICE)\n","\n","                ### ===> Forward, Upscale, Compute Losses\n","                ## Forward\n","                outputs = model(images)\n","\n","                ## Upscale (bilinear interpolation - not learned)\n","                h, w = masks.size(1), masks.size(2)\n","                ph, pw = outputs[0].size(2), outputs[0].size(3)\n","                if ph != h or pw != w:\n","                    for i in range(len(outputs)):\n","                        outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n","\n","                # Output 1 is the prediction\n","\n","                # Shape: NBATCHES x classes x h x w\n","                class_indices = torch.argmax(outputs[1], dim=1)  # Shape: NBATCHES x h x w\n","\n","                if accuracy:\n","                # Create a mask for valid targets (where target is not -1)\n","                    valid_mask = (masks != -1)  # Mask of shape: NBATCHES x h x w\n","                    # Apply the mask to ignore -1 targets when updating the accuracy metric\n","                    meter.update(class_indices[valid_mask], masks[valid_mask])\n","\n","                if mIoU:\n","                    for predicted, target in zip(class_indices, masks):\n","                        if IGNORE_UNCLASSIFIED:\n","                          unclassified_mask = target == -1\n","                          predicted[unclassified_mask] = -1\n","\n","                        for i in range(NUM_CLASSES):\n","                            total_intersection[i] += torch.sum(torch.logical_and(predicted == i, target == i))\n","                            total_union[i] += torch.sum(torch.logical_or(predicted == i, target == i))\n","\n","        if accuracy:\n","            accuracy = meter.compute()\n","            print(f'\\nAccuracy on the target domain: {100 * accuracy:.2f}%')\n","\n","        if mIoU:\n","            intersection_over_union = total_intersection / total_union\n","\n","            # Per class IoU\n","            for i, iou in enumerate(intersection_over_union):\n","                class_name = list(LABEL_MAP.keys())[list(LABEL_MAP.values()).index(i)]  # Get the class name from LABEL_MAP\n","                print(f'{class_name} IoU: {iou:.4f}')\n","\n","            mIoU = torch.mean(intersection_over_union)\n","            print(f'\\nmIoU on the {TARGET} domain: {mIoU}')\n","\n","        print(\"========================================================================\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["YwDm553BzPlB","7zeYu_g-bpip","OftJbvlhH725","68BKhuGh2vJH","xSaOfED0-zoc","IrsSERdV_xZc","uZwN55AlCRZc","V1PldejM1ZCB","uiBwR_Yg1dVO","isRcFkZ3CxpI","XlPJ2YY9Esas","o_JWq-xGE4iq","l18jlCwmiTZp","OwP4ZvV0rB8I","cPFxB02HWn-G","rOz0Bt-_0uKY","qHQ-M4jeaHOV"],"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}