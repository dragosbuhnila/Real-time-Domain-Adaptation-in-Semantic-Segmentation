{"cells":[{"cell_type":"markdown","metadata":{"id":"cbIwgK7-uZ0l"},"source":[]},{"cell_type":"code","source":["SAVE_ON_DRIVE = False\n","TYPE = 'Train'"],"metadata":{"id":"qeY1VfJ5T8kV","executionInfo":{"status":"ok","timestamp":1735840329146,"user_tz":-60,"elapsed":568,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YwDm553BzPlB"},"source":["# Dataset initialization\n"]},{"cell_type":"markdown","source":["### Download Data"],"metadata":{"id":"7zeYu_g-bpip"}},{"cell_type":"code","execution_count":38,"metadata":{"collapsed":true,"id":"cjuW8Yf0zO7K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735840331105,"user_tz":-60,"elapsed":1620,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}},"outputId":"afcc30cd-afad-4506-931d-ad5b6d09af62"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Dataset already in local\n"]}],"source":["from google.colab import drive\n","import os\n","import shutil\n","\n","drive.mount('/content/drive')\n","dataset_path = '/content/drive/MyDrive/LoveDA/Train'\n","if (os.path.exists(\"./Train\") == False):\n","    if (os.path.exists(\"/content/drive/MyDrive/LoveDA/Train.zip\")):\n","        print(\"Dataset available on own drive, unzipping...\")\n","        !unzip -q /content/drive/MyDrive/LoveDA/Train.zip -d ./\n","    else:\n","        print(\"Downloading dataset...\")\n","        !wget -O Train.zip \"https://zenodo.org/records/5706578/files/Train.zip?download=1\"\n","        if(SAVE_ON_DRIVE):\n","            print(\"Saving dataset on drive...\")\n","            !cp Train.zip /content/drive/MyDrive/LoveDA/\n","        !unzip -q Train.zip -d ./\n","\n","else:\n","    print(\"Dataset already in local\")\n"]},{"cell_type":"markdown","source":["### Import Boundaries"],"metadata":{"id":"OftJbvlhH725"}},{"cell_type":"code","source":["# Paths\n","rural_boundaries_path = \"./Train/Rural/boundaries_png\"\n","rural_masks_path = './Train/Rural/masks_png'\n","\n","urban_boundaries_path = \"./Train/Urban/boundaries_png\"\n","urban_masks_path = './Train/Urban/masks_png'\n","drive_rural_boundaries_path = '/content/drive/MyDrive/LoveDA/boundaries/Rural/boundaries_png'\n","drive_urban_boundaries_path = '/content/drive/MyDrive/LoveDA/boundaries/Urban/boundaries_png'\n","\n","boundaries_paths = [rural_boundaries_path, urban_boundaries_path]\n","\n","# Make dir inside ./Train/...\n","for boundaries_path in boundaries_paths:\n","    if (os.path.exists(boundaries_path) == False):\n","        print(f\"Creating {boundaries_path}...\")\n","        os.makedirs(boundaries_path)\n","    else:\n","        print(f\"{boundaries_path} exists...\")\n","\n","\n","# Check if files are already present\n","rural_file_count = len([name for name in os.listdir(rural_boundaries_path) if os.path.isfile(os.path.join(rural_boundaries_path, name))])\n","rural_mask_file_count = len([name for name in os.listdir(rural_masks_path) if os.path.isfile(os.path.join(rural_masks_path, name))])\n","urban_file_count = len([name for name in os.listdir(urban_boundaries_path) if os.path.isfile(os.path.join(urban_boundaries_path, name))])\n","urban_mask_file_count = len([name for name in os.listdir(urban_masks_path) if os.path.isfile(os.path.join(urban_masks_path, name))])\n","\n","# if (rural_file_count != rural_mask_file_count):\n","#     print(f\"Importing boundaries, as we have {rural_file_count} rural boundaries as of now...\")\n","#     shutil.copytree(drive_rural_boundaries_path, rural_boundaries_path, dirs_exist_ok=True)\n","# else:\n","#     print(f\"Rural boundaries already present, {rural_file_count} files...\")\n","\n","if (urban_file_count != urban_mask_file_count):\n","    print(f\"Importing boundaries, as we have {urban_file_count} urban boundaries as of now...\")\n","    shutil.copytree(drive_urban_boundaries_path, urban_boundaries_path, dirs_exist_ok=True)\n","else:\n","    print(f\"Urban boundaries already present, {urban_file_count} files...\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XObla4TbH9Y7","executionInfo":{"status":"ok","timestamp":1735840331105,"user_tz":-60,"elapsed":6,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}},"outputId":"aca21813-2581-4eb2-b7cc-4d6afdb0e429"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["./Train/Rural/boundaries_png exists...\n","./Train/Urban/boundaries_png exists...\n","Urban boundaries already present, 1156 files...\n"]}]},{"cell_type":"markdown","metadata":{"id":"68BKhuGh2vJH"},"source":["### Dataset Definition"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"iUsQczlZ3Wx0","executionInfo":{"status":"ok","timestamp":1735841169815,"user_tz":-60,"elapsed":372,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}}},"outputs":[],"source":["import os\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import torchvision.transforms as T\n","import random\n","import cv2\n","\n","\n","def pil_loader(path, color_type):\n","    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n","    with open(path, 'rb') as f:\n","        img = Image.open(f)\n","        return img.convert(color_type)\n","\n","class LoveDADataset(Dataset):\n","    def __init__(self, transforms, split = 'Urban', type = 'Train', useBoundaries=True, validation_ratio=0.2, seed=265637):\n","        # Validate type input\n","        if type not in ['Train', 'Validation', 'Total']:\n","            raise ValueError(\"Invalid type. Expected 'Train' or 'Validation'.\")\n","        self.directory = []\n","        directory_path = os.path.join('./Train', split, 'images_png')\n","        # Check if the directory exists\n","        if not os.path.exists(directory_path):\n","            raise FileNotFoundError(f\"Directory not found: {directory_path}\")\n","        # Get all image paths\n","        all_images = [os.path.join(directory_path, entry) for entry in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, entry))]\n","        # Shuffle images for random splitting\n","        random.seed(seed)\n","        random.shuffle(all_images)\n","        # Split into training and validation sets\n","        split_idx = int(len(all_images) * (1 - validation_ratio))\n","        if type == 'Train':\n","            self.directory = all_images[:split_idx]\n","        elif type == 'Validation':\n","            self.directory = all_images[split_idx:]\n","        elif type == 'Total':\n","            self.directory = all_images\n","        else:\n","            raise ValueError(\"Invalid type. Expected 'Train' or 'Validation' or 'Total'.\")\n","        self.transforms = transforms\n","        self.useBoundaries = useBoundaries\n","        # Print dataset size\n","        print(f\"Dataset size: {len(self.directory)}\")\n","\n","      # Define albumentations transforms\n","        \"\"\"\n","        # WITH AUGMENTATIONS\n","          self.transform = A.Compose([\n","            A.RandomCrop(256, 256),\n","            A.OneOf([\n","                A.HorizontalFlip(p=1),\n","                A.VerticalFlip(p=1),\n","                A.RandomRotate90(p=1)\n","            ], p=0.75),\n","            A.Normalize(mean=(123.675, 116.28, 103.53),\n","                        std=(58.395, 57.12, 57.375),\n","                        max_pixel_value=1.0, always_apply=True),\n","            ToTensorV2()\n","        ])\n","      \"\"\"\n","\n","    def __len__(self):\n","        return len(self.directory)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.directory[idx]\n","        image = pil_loader(image_path, 'RGB')\n","        mask_path = image_path.replace('images_png', 'masks_png')\n","        boundaries_path = image_path.replace('images_png', 'boundaries_png')\n","\n","        mask = pil_loader(mask_path, 'L')\n","        if self.useBoundaries:\n","          boundaries = pil_loader(boundaries_path, 'L')\n","          # Apply transformations\n","          augmented = self.transforms(image=np.array(image), mask=np.array(mask), boundaries=np.array(boundaries))\n","        else:\n","          augmented = self.transforms(image=np.array(image), mask=np.array(mask), boundaries=np.array(mask))\n","\n","        image = T.Compose([T.ToTensor()])(augmented['image'])\n","\n","        mask = augmented['mask']\n","        mask = torch.from_numpy(mask).long()\n","        mask = mask-1\n","\n","\n","        boundaries = augmented['boundaries']\n","        boundaries = torch.from_numpy(boundaries)\n","\n","\n","        return image, mask, image_path, boundaries"]},{"cell_type":"markdown","source":["### Dataset Utils"],"metadata":{"id":"xSaOfED0-zoc"}},{"cell_type":"code","source":["from collections import OrderedDict\n","COLOR_MAP = OrderedDict(\n","    Background=(255, 255, 255),\n","    Building=(255, 0, 0),\n","    Road=(255, 255, 0),\n","    Water=(0, 0, 255),\n","    Barren=(159, 129, 183),\n","    Forest=(34, 139, 34),\n","    Agricultural=(255, 195, 128),\n",")\n","\n","LABEL_MAP = OrderedDict(\n","    Background=0,\n","    Building=1,\n","    Road=2,\n","    Water=3,\n","    Barren=4,\n","    Forest=5,\n","    Agricultural=6,\n",")\n","inverted_label_map = OrderedDict((v, k) for k, v in LABEL_MAP.items())\n","\n","\n","def getLabelColor(label):\n","    # Default color for unclassified labels\n","    default_color = np.array([128, 128, 128])  # Gray\n","\n","    # Check if label exists in inverted_label_map\n","    label_name = inverted_label_map.get(label, None)\n","    if label_name is None or label_name not in COLOR_MAP:\n","        return default_color  # Return default color for unclassified\n","\n","    # Return the mapped color\n","    label_color = np.array(COLOR_MAP[label_name])\n","    return label_color\n","\n","\n","def getLegendHandles():\n","  handles = [mpatches.Patch(color=getLabelColor(i)/255, label=inverted_label_map[i]) for i in range(0, len(LABEL_MAP))]\n","  handles.append(mpatches.Patch(color=getLabelColor(-1)/255, label='Unclassified'))\n","  return handles\n","\n","def new_colors_mask(mask):\n","  new_image = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n","  for i, row in enumerate(mask):\n","    for j, cell in enumerate(row):\n","      new_image[i][j] = getLabelColor(cell.item())\n","  return new_image\n","\n"],"metadata":{"id":"tfi1pG3P-2H1","executionInfo":{"status":"ok","timestamp":1735840331105,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Debug"],"metadata":{"id":"IrsSERdV_xZc"}},{"cell_type":"code","execution_count":42,"metadata":{"id":"l5iBVMMn6-3-","executionInfo":{"status":"ok","timestamp":1735840331105,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}}},"outputs":[],"source":["# # Comment this cell to save GPU time\n","\n","# import matplotlib.pyplot as plt\n","# import torch\n","# from torch.utils.data import DataLoader\n","# import matplotlib.patches as mpatches\n","\n","# train_dataset = LoveDADataset(type='Train', seed=222)\n","# print(train_dataset.__len__())\n","\n","# # Get item\n","# image, mask, path, bd = train_dataset.__getitem__(88)\n","\n","# # Show path\n","# print(f\"Image is at {path}\")\n","\n","# # Show image\n","# image = image.permute(1, 2, 0)\n","# image = image.numpy()\n","# plt.imshow(image)\n","\n","# # Show mask\n","# new_image = new_colors_mask(mask)\n","# plt.imshow(image)\n","# plt.show()\n","# plt.legend(handles=getLegendHandles(), loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n","# plt.imshow(new_image)\n","# plt.show()\n","\n","# # Show boundaries\n","# # for row in bd:\n","# #     for col in row:\n","# #         if col != 0 and col != 1:\n","# #             print(col)\n","# bd = bd.numpy()\n","# plt.imshow(bd)\n"]},{"cell_type":"markdown","metadata":{"id":"uZwN55AlCRZc"},"source":["# Initialize model"]},{"cell_type":"markdown","source":["### PIDNet Util Modules"],"metadata":{"id":"V1PldejM1ZCB"}},{"cell_type":"code","source":["# ------------------------------------------------------------------------------\n","# Written by Jiacong Xu (jiacong.xu@tamu.edu)\n","# ------------------------------------------------------------------------------\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","BatchNorm2d = nn.BatchNorm2d\n","bn_mom = 0.1\n","algc = False\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=False):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.no_relu = no_relu\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","\n","        if self.no_relu:\n","            return out\n","        else:\n","            return self.relu(out)\n","\n","class Bottleneck(nn.Module):\n","    expansion = 2\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=True):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n","                               bias=False)\n","        self.bn3 = BatchNorm2d(planes * self.expansion, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.no_relu = no_relu\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        if self.no_relu:\n","            return out\n","        else:\n","            return self.relu(out)\n","\n","class segmenthead(nn.Module):\n","\n","    def __init__(self, inplanes, interplanes, outplanes, scale_factor=None):\n","        super(segmenthead, self).__init__()\n","        self.bn1 = BatchNorm2d(inplanes, momentum=bn_mom)\n","        self.conv1 = nn.Conv2d(inplanes, interplanes, kernel_size=3, padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(interplanes, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(interplanes, outplanes, kernel_size=1, padding=0, bias=True)\n","        self.scale_factor = scale_factor\n","\n","    def forward(self, x):\n","\n","        x = self.conv1(self.relu(self.bn1(x)))\n","        out = self.conv2(self.relu(self.bn2(x)))\n","\n","        if self.scale_factor is not None:\n","            height = x.shape[-2] * self.scale_factor\n","            width = x.shape[-1] * self.scale_factor\n","            out = F.interpolate(out,\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)\n","\n","        return out\n","\n","class DAPPM(nn.Module):\n","    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n","        super(DAPPM, self).__init__()\n","        bn_mom = 0.1\n","        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale0 = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.process1 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.process2 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.process3 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.process4 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.compression = nn.Sequential(\n","                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n","                                    )\n","        self.shortcut = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n","                                    )\n","\n","    def forward(self, x):\n","        width = x.shape[-1]\n","        height = x.shape[-2]\n","        x_list = []\n","\n","        x_list.append(self.scale0(x))\n","        x_list.append(self.process1((F.interpolate(self.scale1(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[0])))\n","        x_list.append((self.process2((F.interpolate(self.scale2(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[1]))))\n","        x_list.append(self.process3((F.interpolate(self.scale3(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[2])))\n","        x_list.append(self.process4((F.interpolate(self.scale4(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[3])))\n","\n","        out = self.compression(torch.cat(x_list, 1)) + self.shortcut(x)\n","        return out\n","\n","class PAPPM(nn.Module):\n","    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n","        super(PAPPM, self).__init__()\n","        bn_mom = 0.1\n","        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","\n","        self.scale0 = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","\n","        self.scale_process = nn.Sequential(\n","                                    BatchNorm(branch_planes*4, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes*4, branch_planes*4, kernel_size=3, padding=1, groups=4, bias=False),\n","                                    )\n","\n","\n","        self.compression = nn.Sequential(\n","                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n","                                    )\n","\n","        self.shortcut = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n","                                    )\n","\n","\n","    def forward(self, x):\n","        width = x.shape[-1]\n","        height = x.shape[-2]\n","        scale_list = []\n","\n","        x_ = self.scale0(x)\n","        scale_list.append(F.interpolate(self.scale1(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","        scale_list.append(F.interpolate(self.scale2(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","        scale_list.append(F.interpolate(self.scale3(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","        scale_list.append(F.interpolate(self.scale4(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","\n","        scale_out = self.scale_process(torch.cat(scale_list, 1))\n","\n","        out = self.compression(torch.cat([x_,scale_out], 1)) + self.shortcut(x)\n","        return out\n","\n","\n","class PagFM(nn.Module):\n","    def __init__(self, in_channels, mid_channels, after_relu=False, with_channel=False, BatchNorm=nn.BatchNorm2d):\n","        super(PagFM, self).__init__()\n","        self.with_channel = with_channel\n","        self.after_relu = after_relu\n","        self.f_x = nn.Sequential(\n","                                nn.Conv2d(in_channels, mid_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(mid_channels)\n","                                )\n","        self.f_y = nn.Sequential(\n","                                nn.Conv2d(in_channels, mid_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(mid_channels)\n","                                )\n","        if with_channel:\n","            self.up = nn.Sequential(\n","                                    nn.Conv2d(mid_channels, in_channels,\n","                                              kernel_size=1, bias=False),\n","                                    BatchNorm(in_channels)\n","                                   )\n","        if after_relu:\n","            self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x, y):\n","        input_size = x.size()\n","        if self.after_relu:\n","            y = self.relu(y)\n","            x = self.relu(x)\n","\n","        y_q = self.f_y(y)\n","        y_q = F.interpolate(y_q, size=[input_size[2], input_size[3]],\n","                            mode='bilinear', align_corners=False)\n","        x_k = self.f_x(x)\n","\n","        if self.with_channel:\n","            sim_map = torch.sigmoid(self.up(x_k * y_q))\n","        else:\n","            sim_map = torch.sigmoid(torch.sum(x_k * y_q, dim=1).unsqueeze(1))\n","\n","        y = F.interpolate(y, size=[input_size[2], input_size[3]],\n","                            mode='bilinear', align_corners=False)\n","        x = (1-sim_map)*x + sim_map*y\n","\n","        return x\n","\n","class Light_Bag(nn.Module):\n","    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n","        super(Light_Bag, self).__init__()\n","        self.conv_p = nn.Sequential(\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","        self.conv_i = nn.Sequential(\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","\n","    def forward(self, p, i, d):\n","        edge_att = torch.sigmoid(d)\n","\n","        p_add = self.conv_p((1-edge_att)*i + p)\n","        i_add = self.conv_i(i + edge_att*p)\n","\n","        return p_add + i_add\n","\n","\n","class DDFMv2(nn.Module):\n","    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n","        super(DDFMv2, self).__init__()\n","        self.conv_p = nn.Sequential(\n","                                BatchNorm(in_channels),\n","                                nn.ReLU(inplace=True),\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","        self.conv_i = nn.Sequential(\n","                                BatchNorm(in_channels),\n","                                nn.ReLU(inplace=True),\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","\n","    def forward(self, p, i, d):\n","        edge_att = torch.sigmoid(d)\n","\n","        p_add = self.conv_p((1-edge_att)*i + p)\n","        i_add = self.conv_i(i + edge_att*p)\n","\n","        return p_add + i_add\n","\n","class Bag(nn.Module):\n","    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n","        super(Bag, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","                                BatchNorm(in_channels),\n","                                nn.ReLU(inplace=True),\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=3, padding=1, bias=False)\n","                                )\n","\n","\n","    def forward(self, p, i, d):\n","        edge_att = torch.sigmoid(d)\n","        return self.conv(edge_att*p + (1-edge_att)*i)\n","\n","\n","\n","if __name__ == '__main__':\n","\n","\n","    x = torch.rand(4, 64, 32, 64).cuda()\n","    y = torch.rand(4, 64, 32, 64).cuda()\n","    z = torch.rand(4, 64, 32, 64).cuda()\n","    net = PagFM(64, 16, with_channel=True).cuda()\n","\n","    out = net(x,y)"],"metadata":{"id":"8lOLZdcA1W04","executionInfo":{"status":"ok","timestamp":1735840331990,"user_tz":-60,"elapsed":888,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["### PIDNet Definition"],"metadata":{"id":"uiBwR_Yg1dVO"}},{"cell_type":"code","execution_count":44,"metadata":{"id":"dcHkfgpmCUys","executionInfo":{"status":"ok","timestamp":1735840331990,"user_tz":-60,"elapsed":6,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}}},"outputs":[],"source":["# ------------------------------------------------------------------------------\n","# Written by Jiacong Xu (jiacong.xu@tamu.edu)\n","# ------------------------------------------------------------------------------\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","import logging\n","\n","BatchNorm2d = nn.BatchNorm2d\n","bn_mom = 0.1\n","algc = False\n","\n","\n","\n","class PIDNet(nn.Module):\n","\n","    def __init__(self, m=2, n=3, num_classes=19, planes=64, ppm_planes=96, head_planes=128, augment=True):\n","        super(PIDNet, self).__init__()\n","        self.augment = augment\n","\n","        # I Branch\n","        self.conv1 =  nn.Sequential(\n","                          nn.Conv2d(3,planes,kernel_size=3, stride=2, padding=1),\n","                          BatchNorm2d(planes, momentum=bn_mom),\n","                          nn.ReLU(inplace=True),\n","                          nn.Conv2d(planes,planes,kernel_size=3, stride=2, padding=1),\n","                          BatchNorm2d(planes, momentum=bn_mom),\n","                          nn.ReLU(inplace=True),\n","                      )\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.layer1 = self._make_layer(BasicBlock, planes, planes, m)\n","        self.layer2 = self._make_layer(BasicBlock, planes, planes * 2, m, stride=2)\n","        self.layer3 = self._make_layer(BasicBlock, planes * 2, planes * 4, n, stride=2)\n","        self.layer4 = self._make_layer(BasicBlock, planes * 4, planes * 8, n, stride=2)\n","        self.layer5 =  self._make_layer(Bottleneck, planes * 8, planes * 8, 2, stride=2)\n","\n","        # P Branch\n","        self.compression3 = nn.Sequential(\n","                                          nn.Conv2d(planes * 4, planes * 2, kernel_size=1, bias=False),\n","                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                          )\n","\n","        self.compression4 = nn.Sequential(\n","                                          nn.Conv2d(planes * 8, planes * 2, kernel_size=1, bias=False),\n","                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                          )\n","        self.pag3 = PagFM(planes * 2, planes)\n","        self.pag4 = PagFM(planes * 2, planes)\n","\n","        self.layer3_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n","        self.layer4_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n","        self.layer5_ = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n","\n","        # D Branch\n","        if m == 2:\n","            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes)\n","            self.layer4_d = self._make_layer(Bottleneck, planes, planes, 1)\n","            self.diff3 = nn.Sequential(\n","                                        nn.Conv2d(planes * 4, planes, kernel_size=3, padding=1, bias=False),\n","                                        BatchNorm2d(planes, momentum=bn_mom),\n","                                        )\n","            self.diff4 = nn.Sequential(\n","                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n","                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                     )\n","            self.spp = PAPPM(planes * 16, ppm_planes, planes * 4)\n","            self.dfm = Light_Bag(planes * 4, planes * 4)\n","        else:\n","            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n","            self.layer4_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n","            self.diff3 = nn.Sequential(\n","                                        nn.Conv2d(planes * 4, planes * 2, kernel_size=3, padding=1, bias=False),\n","                                        BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                        )\n","            self.diff4 = nn.Sequential(\n","                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n","                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                     )\n","            self.spp = DAPPM(planes * 16, ppm_planes, planes * 4)\n","            self.dfm = Bag(planes * 4, planes * 4)\n","\n","        self.layer5_d = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n","\n","        # Prediction Head\n","        if self.augment:\n","            self.seghead_p = segmenthead(planes * 2, head_planes, num_classes)\n","            self.seghead_d = segmenthead(planes * 2, planes, 1)\n","\n","        self.final_layer = segmenthead(planes * 4, head_planes, num_classes)\n","\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","\n","    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n","            )\n","\n","        layers = []\n","        layers.append(block(inplanes, planes, stride, downsample))\n","        inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            if i == (blocks-1):\n","                layers.append(block(inplanes, planes, stride=1, no_relu=True))\n","            else:\n","                layers.append(block(inplanes, planes, stride=1, no_relu=False))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _make_single_layer(self, block, inplanes, planes, stride=1):\n","        downsample = None\n","        if stride != 1 or inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n","            )\n","\n","        layer = block(inplanes, planes, stride, downsample, no_relu=True)\n","\n","        return layer\n","\n","    def forward(self, x):\n","\n","        width_output = x.shape[-1] // 8\n","        height_output = x.shape[-2] // 8\n","\n","        x = self.conv1(x)\n","        x = self.layer1(x)\n","        x = self.relu(self.layer2(self.relu(x)))\n","        x_ = self.layer3_(x)\n","        x_d = self.layer3_d(x)\n","\n","        x = self.relu(self.layer3(x))\n","        x_ = self.pag3(x_, self.compression3(x))\n","        x_d = x_d + F.interpolate(\n","                        self.diff3(x),\n","                        size=[height_output, width_output],\n","                        mode='bilinear', align_corners=algc)\n","        if self.augment:\n","            temp_p = x_\n","\n","        x = self.relu(self.layer4(x))\n","        x_ = self.layer4_(self.relu(x_))\n","        x_d = self.layer4_d(self.relu(x_d))\n","\n","        x_ = self.pag4(x_, self.compression4(x))\n","        x_d = x_d + F.interpolate(\n","                        self.diff4(x),\n","                        size=[height_output, width_output],\n","                        mode='bilinear', align_corners=algc)\n","        if self.augment:\n","            temp_d = x_d\n","\n","        x_ = self.layer5_(self.relu(x_))\n","        x_d = self.layer5_d(self.relu(x_d))\n","        x = F.interpolate(\n","                        self.spp(self.layer5(x)),\n","                        size=[height_output, width_output],\n","                        mode='bilinear', align_corners=algc)\n","\n","        x_ = self.final_layer(self.dfm(x_, x, x_d))\n","\n","        if self.augment:\n","            x_extra_p = self.seghead_p(temp_p)\n","            x_extra_d = self.seghead_d(temp_d)\n","            return [x_extra_p, x_, x_extra_d]\n","        else:\n","            return x_\n","\n","def get_seg_model(cfg, imgnet_pretrained):\n","\n","    if 's' in cfg.MODEL.NAME:\n","        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=32, ppm_planes=96, head_planes=128, augment=True)\n","    elif 'm' in cfg.MODEL.NAME:\n","        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=96, head_planes=128, augment=True)\n","    else:\n","        model = PIDNet(m=3, n=4, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=112, head_planes=256, augment=True)\n","\n","    if imgnet_pretrained:\n","        pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n","        model_dict = model.state_dict()\n","        pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n","        model_dict.update(pretrained_state)\n","        msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n","        logging.info('Attention!!!')\n","        logging.info(msg)\n","        logging.info('Over!!!')\n","        model.load_state_dict(model_dict, strict = False)\n","    else:\n","        pretrained_dict = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')\n","        if 'state_dict' in pretrained_dict:\n","            pretrained_dict = pretrained_dict['state_dict']\n","        model_dict = model.state_dict()\n","        pretrained_dict = {k[6:]: v for k, v in pretrained_dict.items() if (k[6:] in model_dict and v.shape == model_dict[k[6:]].shape)}\n","        msg = 'Loaded {} parameters!'.format(len(pretrained_dict))\n","        logging.info('Attention!!!')\n","        logging.info(msg)\n","        logging.info('Over!!!')\n","        model_dict.update(pretrained_dict)\n","        model.load_state_dict(model_dict, strict = False)\n","\n","    return model\n","\n","def get_pred_model(name, num_classes):\n","\n","    if 's' in name:\n","        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=False)\n","    elif 'm' in name:\n","        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=False)\n","    else:\n","        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=False)\n","\n","    return model"]},{"cell_type":"markdown","source":["### Load PIDNet Model"],"metadata":{"id":"isRcFkZ3CxpI"}},{"cell_type":"code","source":["import gdown\n","import tarfile\n","\n","if (os.path.exists(\"./PIDNet_S_ImageNet.pth.tar\") == False):\n","  url = \"https://drive.google.com/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-\"\n","  output = \"./\"\n","  gdown.download(url, output, quiet=False)\n","# Then keep as tar, as it's already the correct format to feed the model\n","\n","# Create a config object with required parameters\n","class Config:\n","    class MODEL:\n","        NAME = 'pidnet_s'  # or 'pidnet_m' or 'pidnet_l'\n","        PRETRAINED = 'PIDNet_S_ImageNet.pth.tar'\n","    class DATASET:\n","        NUM_CLASSES = len(LABEL_MAP)\n","\n","cfg = Config()\n","\n","model = get_seg_model(cfg, imgnet_pretrained=True)\n","# model = get_pred_model('s', len(LABEL_MAP))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ujP_2PffskRk","executionInfo":{"status":"ok","timestamp":1735840332570,"user_tz":-60,"elapsed":585,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}},"outputId":"34c99d16-92ea-4a82-a439-66e8868f4b08"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-44-d799b7f58ec7>:193: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n"]}]},{"cell_type":"markdown","source":["### Model Debugging"],"metadata":{"id":"RtcmrSGDcA4w"}},{"cell_type":"code","source":["# import torch.nn.functional as F\n","# from torch.utils.data import DataLoader\n","# import matplotlib.pyplot as plt\n","\n","# train_dataset = LoveDADataset(type='Train')\n","# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n","\n","# model = model.train()\n","# model = model.to('cuda')\n","\n","# for img, mask, _ in train_loader:\n","#     print(f\"iamge shape: {img.shape}\")\n","#     print(f\"mask shape: {mask.shape}\")\n","\n","#     img = img.to('cuda')\n","#     outputs = model(img)\n","\n","#     # bilinear interpolation\n","#     h, w = mask.size(1), mask.size(2)\n","#     ph, pw = outputs[0].size(2), outputs[0].size(3)\n","#     if ph != h or pw != w:\n","#         for i in range(len(outputs)):\n","#             outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear',\n","#                                        align_corners=True)\n","\n","#     for output in outputs:\n","#       print(output.shape)\n","#     break\n","\n","# print(\"===================== Original Image =====================\")\n","# plt.imshow(img[0].permute(1, 2, 0).cpu().numpy())\n","# plt.show()\n","\n","# print(\"===================== Ground Truth =====================\")\n","# plt.imshow(mask[0].cpu().numpy())\n","# plt.show()\n","\n","# print(\"===================== Predicted Mask =====================\")\n","# plt.imshow(torch.argmax(outputs[0][0], dim=0).cpu().numpy())\n","# plt.show()"],"metadata":{"id":"xkTB43MxcDRA","executionInfo":{"status":"ok","timestamp":1735840332570,"user_tz":-60,"elapsed":5,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3DwQcmOWpUxn"},"source":["# Training & Dataset creation"]},{"cell_type":"markdown","source":["### Ablations and Macros"],"metadata":{"id":"iToC6F28cnl5"}},{"cell_type":"code","source":["DEVICE = 'cuda' # 'cuda' or 'cpu'\n","\n","LR = 2*1e-3            # The initial Learning Rate -- I increased it using quadratic rule in relation with batch size\n","MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n","WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n","\n","NUM_EPOCHS = 20      # Total number of training epochs (iterations over dataset)\n","STEP_SIZE = 14      # How many epochs before decreasing learning rate (if using a step-down policy) -- Trying to keep a 2:3 ratio with NUM_EPOCHS\n","GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n","\n","LOG_FREQUENCY = 5\n","NUM_CLASSES = len(LABEL_MAP)\n","BATCH_SIZE = 64"],"metadata":{"id":"moWCCVykx1ac","executionInfo":{"status":"ok","timestamp":1735840332570,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["### Setup, Create Datasets and DataLoaders. With annexed transforms."],"metadata":{"id":"IwDwW9Hac6SS"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","from albumentations import Compose, HorizontalFlip, RandomRotate90, RandomScale, RandomCrop, GaussNoise, ElasticTransform, Rotate, Resize, OneOf, Normalize, HistogramMatching\n","from albumentations.pytorch import ToTensorV2\n","import cv2\n","\n","#How big should be the image that we feed to the model?\n","RESIZE = 512\n","SMALL_RESIZE = 256\n","# DEFINE TRANSFORMATIONS HERE\n","# To Tensor is not needed since its performed inside the getitem\n","\n","AUGMENTATIONS = {\n","    'None': Compose([\n","            Resize(RESIZE, RESIZE),\n","    ], additional_targets={\"boundaries\": \"mask\"}),\n","    'Normalize': Compose([\n","            Resize(RESIZE, RESIZE),\n","            Normalize(mean=(123.675, 116.28, 103.53), std=(58.395, 57.12, 57.375), max_pixel_value=1.0, always_apply=True)\n","                      ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomCrop+Normalization' : Compose([\n","        RandomCrop(RESIZE, RESIZE),\n","        Normalize(mean=(123.675, 116.28, 103.53), std=(58.395, 57.12, 57.375), max_pixel_value=1.0, always_apply=True)\n","    ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomCrop_Resize': Compose([\n","                  OneOf([\n","                      RandomCrop(RESIZE, RESIZE, p=0.5),  # Random crop to resize\n","                      Resize(RESIZE, RESIZE, p=0.5)\n","                  ], p=1)\n","                  ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomCrop': Compose([\n","                RandomCrop(RESIZE, RESIZE)\n","              ], additional_targets={\"boundaries\": \"mask\"}),\n","}\n","\n","CHOOSE_TRANSFORM = 'HistogramMatching'\n","transforms = AUGMENTATIONS[CHOOSE_TRANSFORM]\n","\n","# Dataset and Loader\n","train_dataset = LoveDADataset(transforms=transforms,split='Urban', type='Train', validation_ratio=0.2)\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True, pin_memory=True)\n","\n","validation_dataset = LoveDADataset(transforms=transforms, split='Urban', type='Validation', validation_ratio=0.2)\n","validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True, pin_memory=True)\n","\n","\n","# Model is defined some cells above, in LoadPidNetModel\n","\n","# Optimizier and Scheduler\n","optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"],"metadata":{"id":"J8DhNU7Lc7aD","executionInfo":{"status":"ok","timestamp":1735841700334,"user_tz":-60,"elapsed":582,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3a5bfe81-132f-46c5-ca82-2b92a5fb9468"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset size: 924\n","Dataset size: 232\n"]}]},{"cell_type":"markdown","source":["### Losses"],"metadata":{"id":"l18jlCwmiTZp"}},{"cell_type":"code","source":["def weighted_bce(bd_pre, target):\n","    n, c, h, w = bd_pre.size()\n","    log_p = bd_pre.permute(0,2,3,1).contiguous().view(1, -1)\n","    target_t = target.view(1, -1)\n","\n","    pos_index = (target_t == 1)\n","    neg_index = (target_t == 0)\n","\n","    weight = torch.zeros_like(log_p)\n","    pos_num = pos_index.sum()\n","    neg_num = neg_index.sum()\n","    sum_num = pos_num + neg_num\n","    weight[pos_index] = neg_num * 1.0 / sum_num\n","    weight[neg_index] = pos_num * 1.0 / sum_num\n","\n","    loss = F.binary_cross_entropy_with_logits(log_p, target_t, weight, reduction='mean')\n","\n","    return loss\n","\n","def boundary_loss(bd_pre, bd_gt):\n","    loss = 20.0 * weighted_bce(bd_pre, bd_gt)\n","    return loss\n","\n","# TODO EXTRA add weights=class_weights to nn.CrossEntropyLoss()\n","# TODO EXTRA use OHCE instead of basic one\n","def cross_entropy(score, target):\n","    compute_ce_loss = nn.CrossEntropyLoss(ignore_index=-1)\n","\n","    # See paper for weights. In order of loss index: (0.4, 20, 1, 1) # But on cfg they set everything to 0.5\n","    balance_weights = [0.4, 1]\n","    sb_weights = 1\n","\n","    # print(f\"DEBUG: inside cross_entropy: len(score) = {len(score)}\")\n","    if len(balance_weights) == len(score):\n","        return sum([w * compute_ce_loss(x, target) for (w, x) in zip(balance_weights, score)])\n","    elif len(score) == 1:\n","        return sb_weights * compute_ce_loss(score[0], target)\n","    else:\n","        raise ValueError(\"lengths of prediction and target are not identical!\")\n","\n","sem_loss = cross_entropy\n","bd_loss = boundary_loss"],"metadata":{"id":"tU1zr5WKiU0y","executionInfo":{"status":"ok","timestamp":1735840336041,"user_tz":-60,"elapsed":6,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":["### Training Loop"],"metadata":{"id":"_rRjwVYaaFG8"}},{"cell_type":"code","execution_count":77,"metadata":{"id":"Y8nlqvfeFpg0","colab":{"base_uri":"https://localhost:8080/","height":755},"executionInfo":{"status":"error","timestamp":1735841706781,"user_tz":-60,"elapsed":878,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}},"outputId":"26c0a8a2-ad9d-445b-dfb6-6460ef06faf0","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["DEVICE is cuda\n","Starting epoch 1/20, LR = [0.002]\n"]},{"output_type":"error","ename":"error","evalue":"Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-55-80dbc10bd76f>\", line 78, in __getitem__\n    augmented = self.transforms(image=np.array(image), mask=np.array(mask), boundaries=np.array(boundaries))\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/composition.py\", line 386, in __call__\n    data = t(**data)\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/transforms_interface.py\", line 106, in __call__\n    params = self.get_params()\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/domain_adaptation/transforms.py\", line 121, in get_params\n    \"reference_image\": self.read_fn(random.choice(self.reference_images)),\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/utils.py\", line 41, in read_rgb_image\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\ncv2.error: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-77-bad028fea5c8>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting epoch {}/{}, LR = {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-55-80dbc10bd76f>\", line 78, in __getitem__\n    augmented = self.transforms(image=np.array(image), mask=np.array(mask), boundaries=np.array(boundaries))\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/composition.py\", line 386, in __call__\n    data = t(**data)\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/transforms_interface.py\", line 106, in __call__\n    params = self.get_params()\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/domain_adaptation/transforms.py\", line 121, in get_params\n    \"reference_image\": self.read_fn(random.choice(self.reference_images)),\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/utils.py\", line 41, in read_rgb_image\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\ncv2.error: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n\n"]}],"source":["import matplotlib.pyplot as plt\n","\n","# For easier debugging\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","os.environ['TORCH_USE_CUDA_DSA'] = '1'\n","\n","SAVE_MODEL_AS = f'best_model_{CHOOSE_TRANSFORM}.pth'\n","\n","current_step = 0\n","\n","best_loss = float('inf')\n","best_model = model.state_dict()\n","\n","model = model.to(DEVICE)\n","print(f\"DEVICE is {DEVICE}\")\n","if TYPE == 'Train':\n","  for epoch in range(NUM_EPOCHS):\n","    model.train()\n","    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n","    epoch_loss = [0.0, 0]\n","    for (batch_i, batch) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","\n","        ### Extract input\n","        images, masks, img_path, bd_gts = batch\n","        images = images.to(DEVICE)\n","        masks = masks.to(DEVICE)\n","        bd_gts = bd_gts.float().to(DEVICE)\n","        # print(f\"DEBUG: image path: {img_path}\")\n","        # print(f\"DEBUG: image batch shape: {images.shape}\")\n","        # print(f\"DEBUG: mask batch shape: {masks.shape}\")\n","\n","        ### ===> Forward, Upscale, Compute Losses\n","        ## Forward\n","        outputs = model(images) # in model.train() mode batch size must be > 1 I think\n","                                # NOTE: we have 3 heads (i.e. 3 outputs) but 4 losses: 2nd head is used for both S and BAS\n","\n","        ## Upscale (bilinear interpolation - not learned)\n","        h, w = masks.size(1), masks.size(2)\n","        ph, pw = outputs[0].size(2), outputs[0].size(3)\n","        if ph != h or pw != w:\n","            for i in range(len(outputs)):\n","                outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n","\n","        # print(f\"DEBUG: outputs shapes below: ({len(outputs)} of them)\")\n","        # for output in outputs:\n","        #     print(output.shape)\n","        # print(f\"DEBUG: in particular, shape of outputs[-2] is {outputs[-2].shape}\")\n","\n","        ## Losses\n","        # Semantic Losses (l_0 and l_2)\n","        loss_s = sem_loss(outputs[:-1], masks) # output #1 and #2 are segmentation predictions (i.e. low level (P) and high+low level (PI) respectively)\n","\n","        # Boundary Loss (l_1)\n","        loss_b = bd_loss(outputs[-1], bd_gts) # output #3 is the boundary prediction\n","\n","        # Boundary AwareneSS (BAS) Loss (l_3)\n","        filler = torch.ones_like(masks) * -1\n","        bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:]) > 0.8, masks, filler)\n","                            # REMEMBER to wrap in list, as the checks in ce use that to know what to do\n","        loss_sb = sem_loss([outputs[-2]], bd_label) # output #2 is the PI segmentation prediction, done here in BAS mode (see `filler` variable)\n","\n","        # Complete Loss\n","        loss = loss_s + loss_b + loss_sb # The coefficients of the sum of the four losses (0.4, 20, 1, 1) are taken into account in the various `sem_loss` and `bd_loss`\n","        ### <=== Forward, Upscale, Compute Losses\n","\n","        ### Backprop\n","        if batch_i % LOG_FREQUENCY == 0:\n","            print(f'Loss at batch {batch_i}: {loss.item()}')\n","        loss.backward()\n","\n","        optimizer.step()\n","        epoch_loss[0] += loss.item()\n","        epoch_loss[1] += images.size(0)\n","\n","    # Evaluate model on the evaluation set and save the parameters if is better than best model\n","    model.eval()\n","    total_loss = 0.0\n","    outputs = []\n","    with torch.no_grad():\n","      for (batch_i, batch) in enumerate(validation_loader):\n","        ### Extract input\n","        images, masks, img_path, bd_gts = batch\n","        images = images.float().to(DEVICE)\n","        masks = masks.to(DEVICE)\n","        bd_gts = bd_gts.float().to(DEVICE)\n","\n","        ### ===> Forward, Upscale, Compute Losses\n","        ## Forward\n","        outputs = model(images)\n","\n","        ## Upscale (bilinear interpolation - not learned)\n","        h, w = masks.size(1), masks.size(2)\n","        ph, pw = outputs[0].size(2), outputs[0].size(3)\n","        if ph != h or pw != w:\n","            for i in range(len(outputs)):\n","                outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n","\n","        ## Losses\n","        # Semantic Losses (l_0 and l_2)\n","        loss_s = sem_loss(outputs[:-1], masks)\n","\n","        # Boundary Loss (l_1)\n","        loss_b = bd_loss(outputs[-1], bd_gts)\n","\n","        # Boundary AwareneSS (BAS) Loss (l_3)\n","        filler = torch.ones_like(masks) * -1\n","        bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:]) > 0.8, masks, filler)\n","        loss_sb = sem_loss([outputs[-2]], bd_label)\n","\n","        # Complete Loss\n","        loss = loss_s + loss_b + loss_sb\n","        ### <=== Forward, Upscale, Compute Losses\n","        total_loss += loss.item()\n","\n","    print('Epoch {}, Loss {}'.format(epoch+1, total_loss))\n","    if total_loss < best_loss:\n","      best_loss = total_loss\n","      best_model = model.state_dict()\n","      #Save in Drive and local\n","      torch.save(best_model, SAVE_MODEL_AS)\n","      if SAVE_ON_DRIVE:\n","        !cp {SAVE_MODEL_AS} /content/drive/MyDrive/LoveDA/{SAVE_MODEL_AS}\n","        print(f\"model succesfully saved on drive. loss went down to {best_loss}\")\n","\n","    current_step += 1\n","    scheduler.step()\n","    print(f'[EPOCH {epoch+1}] Avg. Loss: {epoch_loss[0] / epoch_loss[1]}')\n","\n","    # Create a figure with 1 row and 3 columns\n","    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","    # Plot the original image\n","    axes[0].imshow(images[0].permute(1, 2, 0).cpu().numpy())\n","    axes[0].set_title(\"Original Image\")\n","    axes[0].axis('off')\n","\n","    # Plot the ground truth mask\n","    axes[1].imshow(masks[0].cpu().numpy())\n","    axes[1].set_title(\"Ground Truth\")\n","    axes[1].axis('off')\n","\n","    # Plot the predicted mask\n","    axes[2].imshow(torch.argmax(outputs[0][0], dim=0).cpu().numpy())\n","    axes[2].set_title(\"Predicted Mask\")\n","    axes[2].axis('off')\n","\n","    # Display the figure\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","source":["# TEST"],"metadata":{"id":"soROYpcYyajp"}},{"cell_type":"code","source":["!pip install torchmetrics ptflops"],"metadata":{"id":"7DKgjGPKydLH","executionInfo":{"status":"aborted","timestamp":1735840336570,"user_tz":-60,"elapsed":5,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchmetrics import Accuracy\n","from tqdm import tqdm\n","import time\n","import ptflops\n","\n","TYPE = 'Test'\n","TARGET = 'Urban'\n","#Load best_model\n","#!cp /content/drive/MyDrive/LoveDA/best_model_step2b.pth /content/best_model_step2b.pth\n","\n","model = get_seg_model(cfg, imgnet_pretrained=False)\n","best_model = torch.load(f'/content/best_model_{CHOOSE_TRANSFORM}.pth', weights_only=True)\n","model.load_state_dict(best_model)\n","model = model.to(DEVICE)\n","\n","accuracy, mIoU = True, True\n","\n","#using type = train i have more images (0.8)\n","test_dataset = LoveDADataset(transforms=AUGMENTATIONS['None'], split=TARGET, type='Validation', useBoundaries=False)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True, pin_memory=True)\n","\n","#### TEST LOOP\n","model.eval()\n","print('Validation on Target', TARGET)\n","print('Model: ', CHOOSE_TRANSFORM)\n","print('Test images transformed with ', 'RandomCrop')\n","# Latency\n","with torch.no_grad():\n","    start_time = time.time()\n","    for _ in range(100):\n","        _ = model(torch.randn(1, 3, RESIZE, RESIZE).to(DEVICE))\n","    end_time = time.time()\n","latency = (end_time - start_time) / 100\n","print(f\"Latency: {latency:.4f} seconds\")\n","\n","# FLOPs\n","macs, _ = ptflops.get_model_complexity_info(model,\n","  (3, RESIZE, RESIZE), as_strings=False,\n","  print_per_layer_stat=False, verbose=False)\n","flops = macs * 2  # MACs perform two FLOPs\n","print(\"FLOPs:\", flops)\n","\n","# Number of parameters\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"Total number of parameters: {total_params}\")\n","\n","if TYPE == 'Test':\n","  with torch.no_grad():\n","      total_union = torch.zeros(NUM_CLASSES).to(DEVICE)\n","      total_intersection = torch.zeros(NUM_CLASSES).to(DEVICE)\n","      meter = Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(DEVICE)\n","      for (batch) in tqdm(test_loader):\n","          ### Extract input\n","          images, masks, img_path, bd_gts = batch\n","          images = images.float().to(DEVICE)\n","          masks = masks.to(DEVICE)\n","\n","          ### ===> Forward, Upscale, Compute Losses\n","          ## Forward\n","          outputs = model(images)\n","\n","          ## Upscale (bilinear interpolation - not learned)\n","          h, w = masks.size(1), masks.size(2)\n","          ph, pw = outputs[0].size(2), outputs[0].size(3)\n","          if ph != h or pw != w:\n","              for i in range(len(outputs)):\n","                  outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n","\n","          # Output 1 is the prediction\n","\n","          # Shape: NBATCHES x classes x h x w\n","          class_indices = torch.argmax(outputs[1], dim=1)  # Shape: NBATCHES x h x w\n","\n","          if accuracy:\n","          # Create a mask for valid targets (where target is not -1)\n","            valid_mask = (masks != -1)  # Mask of shape: NBATCHES x h x w\n","            # Apply the mask to ignore -1 targets when updating the accuracy metric\n","            meter.update(class_indices[valid_mask], masks[valid_mask])\n","\n","          if mIoU:\n","            for predicted, target in zip(class_indices, masks):\n","              for i in range(NUM_CLASSES):\n","                total_intersection[i] += torch.sum(torch.logical_and(predicted == i, target == i))\n","                total_union[i] += torch.sum(torch.logical_or(predicted == i, target == i))\n","\n","  if accuracy:\n","    accuracy = meter.compute()\n","    print(f'\\nAccuracy on the target domain: {100 * accuracy:.2f}%')\n","\n","  if mIoU:\n","    intersection_over_union = total_intersection / total_union\n","\n","    # Per class IoU\n","    for i, iou in enumerate(intersection_over_union):\n","        class_name = list(LABEL_MAP.keys())[list(LABEL_MAP.values()).index(i)]  # Get the class name from LABEL_MAP\n","        print(f'{class_name} IoU: {iou:.4f}')\n","\n","    mIoU = torch.mean(intersection_over_union)\n","    print(f'\\nmIoU on the target domain: {mIoU}')\n"],"metadata":{"id":"M3c7DFDRymnI","executionInfo":{"status":"aborted","timestamp":1735840336571,"user_tz":-60,"elapsed":6,"user":{"displayName":"Fabio Gigante","userId":"01122745869049385485"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["xSaOfED0-zoc","IrsSERdV_xZc","V1PldejM1ZCB","uiBwR_Yg1dVO","isRcFkZ3CxpI","RtcmrSGDcA4w","l18jlCwmiTZp"],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}