{"cells":[{"cell_type":"markdown","metadata":{"id":"cbIwgK7-uZ0l"},"source":[]},{"cell_type":"markdown","metadata":{"id":"OiovToEGs9mz"},"source":["## Classic semantic segmentation network."]},{"cell_type":"code","execution_count":50,"metadata":{"id":"qeY1VfJ5T8kV","executionInfo":{"status":"ok","timestamp":1737817619222,"user_tz":-60,"elapsed":399,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[],"source":["SAVE_ON_DRIVE = True\n","TYPE = 'Test'"]},{"cell_type":"markdown","metadata":{"id":"YwDm553BzPlB"},"source":["## Dataset initialization\n"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1821,"status":"ok","timestamp":1737817621414,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"},"user_tz":-60},"id":"cjuW8Yf0zO7K","outputId":"17c55b75-ccf9-4560-e29c-215fd29b993b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Validation dataset already in local\n"]}],"source":["from google.colab import drive\n","import os\n","import shutil\n","\n","drive.mount('/content/drive')\n","\n","# Set paths for Validation and Test datasets\n","val_dataset_path = '/content/drive/MyDrive/LoveDA/Val'\n","test_dataset_path = '/content/drive/MyDrive/LoveDA/Test'\n","\n","\n","# Function to handle dataset download and extraction\n","def handle_dataset(dataset_name, zip_url, local_path, drive_path):\n","    if not os.path.exists(local_path):\n","        if os.path.exists(f\"{drive_path}.zip\"):\n","            print(f\"{dataset_name} dataset available on own drive, unzipping...\")\n","            !unzip -q {drive_path}.zip -d ./\n","        else:\n","            print(f\"Downloading {dataset_name} dataset...\")\n","            !wget -O {dataset_name}.zip \"{zip_url}\"\n","            if SAVE_ON_DRIVE:\n","                print(f\"Saving {dataset_name} dataset on drive...\")\n","                !cp {dataset_name}.zip {drive_path}.zip\n","                print(f\"{dataset_name} dataset saved on drive\")\n","            print(f\"Unzipping {dataset_name} dataset...\")\n","            !unzip -q {dataset_name}.zip -d ./\n","    else:\n","        print(f\"{dataset_name} dataset already in local\")\n","\n","# Handle Validation dataset\n","handle_dataset(\"Validation\", \"https://zenodo.org/records/5706578/files/Val.zip?download=1\", \"./Val\", \"/content/drive/MyDrive/LoveDA/Val\")\n","\n","# Handle Test dataset\n","#handle_dataset(\"Test\", \"https://zenodo.org/records/5706578/files/Test.zip?download=1\", \"./Test\", \"/content/drive/MyDrive/LoveDA/Test\")\n","\n","# Handle Train dataset\n","#handle_dataset(\"Train\", \"https://zenodo.org/records/5706578/files/Train.zip?download=1\", \"./Train\", \"/content/drive/MyDrive/LoveDA/Train\")"]},{"cell_type":"markdown","metadata":{"id":"68BKhuGh2vJH"},"source":["## DataSet initialization"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"iUsQczlZ3Wx0","executionInfo":{"status":"ok","timestamp":1737817621414,"user_tz":-60,"elapsed":13,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[],"source":["import os\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import torchvision.transforms as T\n","import random\n","\n","RESIZE = 256\n","\n","def pil_loader(path, color_type):\n","    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n","    with open(path, 'rb') as f:\n","        img = Image.open(f)\n","        return img.convert(color_type)\n","\n","class LoveDADataset(Dataset):\n","    def __init__(self, split = 'Urban', type = 'Train', folder = './Train', validation_ratio=0.2, seed=265637):\n","        # Validate type input\n","        if type not in ['Train', 'Validation']:\n","            raise ValueError(\"Invalid type. Expected 'Train' or 'Validation'.\")\n","\n","        self.directory = []\n","\n","\n","        directory_path = os.path.join(folder, split, 'images_png')\n","\n","        # Check if the directory exists\n","        if not os.path.exists(directory_path):\n","            raise FileNotFoundError(f\"Directory not found: {directory_path}\")\n","\n","        # Get all image paths\n","        all_images = [os.path.join(directory_path, entry) for entry in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, entry))]\n","\n","        # Shuffle images for random splitting\n","        random.seed(seed)\n","        random.shuffle(all_images)\n","\n","        # Split into training and validation sets\n","        split_idx = int(len(all_images) * (1 - validation_ratio))\n","        if type == 'Train':\n","            self.directory = all_images[:split_idx]\n","        elif type == 'Validation':\n","            self.directory = all_images[split_idx:]\n","        else:\n","            raise ValueError(\"Invalid type. Expected 'Train' or 'Validation'.\")\n","\n","\n","      # Define albumentations transforms\n","        \"\"\"\n","        # WITH AUGMENTATIONS\n","          self.transform = A.Compose([\n","            A.RandomCrop(256, 256),\n","            A.OneOf([\n","                A.HorizontalFlip(p=1),\n","                A.VerticalFlip(p=1),\n","                A.RandomRotate90(p=1)\n","            ], p=0.75),\n","            A.Normalize(mean=(123.675, 116.28, 103.53),\n","                        std=(58.395, 57.12, 57.375),\n","                        max_pixel_value=1.0, always_apply=True),\n","            ToTensorV2()\n","        ])\n","      \"\"\"\n","\n","    def __len__(self):\n","        return len(self.directory)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.directory[idx]\n","        image = pil_loader(image_path, 'RGB')\n","        mask_path = image_path.replace('images_png', 'masks_png')\n","\n","        mask = pil_loader(mask_path, 'L')\n","\n","        img_transform = T.Compose([\n","            T.ToTensor(),\n","        ])\n","        # mask_transform = T.Compose([\n","        #       T.Resize((RESIZE, RESIZE)),\n","        #   ])\n","\n","        # # Apply transformations\n","        # image = img_transform(image)\n","        # mask = mask_transform(mask)\n","\n","        image = img_transform(image)\n","\n","        mask = np.array(mask)\n","        mask = torch.from_numpy(mask).long()\n","        mask = mask-1\n","\n","        return image, mask, image_path"]},{"cell_type":"markdown","metadata":{"id":"xSaOfED0-zoc"},"source":["## Useful functions"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"tfi1pG3P-2H1","executionInfo":{"status":"ok","timestamp":1737817621414,"user_tz":-60,"elapsed":12,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[],"source":["from collections import OrderedDict\n","COLOR_MAP = OrderedDict(\n","    Background=(255, 255, 255),\n","    Building=(255, 0, 0),\n","    Road=(255, 255, 0),\n","    Water=(0, 0, 255),\n","    Barren=(159, 129, 183),\n","    Forest=(34, 139, 34),\n","    Agricultural=(255, 195, 128),\n",")\n","\n","LABEL_MAP = OrderedDict(\n","    Background=0,\n","    Building=1,\n","    Road=2,\n","    Water=3,\n","    Barren=4,\n","    Forest=5,\n","    Agricultural=6,\n",")\n","inverted_label_map = OrderedDict((v, k) for k, v in LABEL_MAP.items())\n","\n","\n","def getLabelColor(label):\n","    # Default color for unclassified labels\n","    default_color = np.array([128, 128, 128])  # Gray\n","\n","    # Check if label exists in inverted_label_map\n","    label_name = inverted_label_map.get(label, None)\n","    if label_name is None or label_name not in COLOR_MAP:\n","        return default_color  # Return default color for unclassified\n","\n","    # Return the mapped color\n","    label_color = np.array(COLOR_MAP[label_name])\n","    return label_color\n","\n","\n","def getLegendHandles():\n","  handles = [mpatches.Patch(color=getLabelColor(i)/255, label=inverted_label_map[i]) for i in range(0, len(LABEL_MAP))]\n","  handles.append(mpatches.Patch(color=getLabelColor(-1)/255, label='Unclassified'))\n","  return handles\n","\n","def new_colors_mask(mask):\n","  new_image = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n","  for i, row in enumerate(mask):\n","    for j, cell in enumerate(row):\n","      new_image[i][j] = getLabelColor(cell.item())\n","  return new_image\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IrsSERdV_xZc"},"source":["## Try dataset"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1737817621414,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"},"user_tz":-60},"id":"l5iBVMMn6-3-"},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","# import torch\n","# from torch.utils.data import DataLoader\n","# import matplotlib.patches as mpatches\n","\n","# train_dataset = LoveDADataset(type='Validation', seed=222)\n","# print(train_dataset.__len__())\n","\n","# image, mask, _ = train_dataset.__getitem__(22)\n","# image = image.permute(1, 2, 0)\n","# image = image.numpy()\n","# plt.imshow(image)\n","\n","# new_image = new_colors_mask(mask)\n","# plt.imshow(image)\n","# plt.show()\n","# plt.legend(handles=getLegendHandles(), loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n","# plt.imshow(new_image)\n","# plt.show()\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uZwN55AlCRZc"},"source":["## Initialize model"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"dcHkfgpmCUys","executionInfo":{"status":"ok","timestamp":1737817621414,"user_tz":-60,"elapsed":12,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","affine_par = True\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes, affine=affine_par)\n","        for i in self.bn1.parameters():\n","            i.requires_grad = False\n","        padding = dilation\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n","                               padding=padding, bias=False, dilation=dilation)\n","        self.bn2 = nn.BatchNorm2d(planes, affine=affine_par)\n","        for i in self.bn2.parameters():\n","            i.requires_grad = False\n","        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * 4, affine=affine_par)\n","        for i in self.bn3.parameters():\n","            i.requires_grad = False\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class ClassifierModule(nn.Module):\n","    def __init__(self, inplanes, dilation_series, padding_series, num_classes):\n","        super(ClassifierModule, self).__init__()\n","        self.conv2d_list = nn.ModuleList()\n","        for dilation, padding in zip(dilation_series, padding_series):\n","            self.conv2d_list.append(\n","                nn.Conv2d(inplanes, num_classes, kernel_size=3, stride=1, padding=padding,\n","                          dilation=dilation, bias=True))\n","\n","        for m in self.conv2d_list:\n","            m.weight.data.normal_(0, 0.01)\n","\n","    def forward(self, x):\n","        out = self.conv2d_list[0](x)\n","        for i in range(len(self.conv2d_list) - 1):\n","            out += self.conv2d_list[i + 1](x)\n","        return out\n","\n","\n","class ResNetMulti(nn.Module):\n","    def __init__(self, block, layers, num_classes):\n","        self.inplanes = 64\n","        super(ResNetMulti, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = nn.BatchNorm2d(64, affine=affine_par)\n","        for i in self.bn1.parameters():\n","            i.requires_grad = False\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)  # change\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n","        self.layer6 = ClassifierModule(2048, [6, 12, 18, 24], [6, 12, 18, 24], num_classes)\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                m.weight.data.normal_(0, 0.01)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n","        downsample = None\n","        if (stride != 1\n","                or self.inplanes != planes * block.expansion\n","                or dilation == 2\n","                or dilation == 4):\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion, affine=affine_par))\n","        for i in downsample._modules['1'].parameters():\n","            i.requires_grad = False\n","        layers = []\n","        layers.append(\n","            block(self.inplanes, planes, stride, dilation=dilation, downsample=downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, dilation=dilation))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        _, _, H, W = x.size()\n","\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.layer6(x)\n","\n","        x = torch.nn.functional.interpolate(x, size=(H, W), mode='bilinear')\n","\n","        if self.training == True:\n","            return x, None, None\n","\n","        return x\n","\n","    def get_1x_lr_params_no_scale(self):\n","        \"\"\"\n","        This generator returns all the parameters of the net except for\n","        the last classification layer. Note that for each batchnorm layer,\n","        requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n","        any batchnorm parameter\n","        \"\"\"\n","        b = []\n","\n","        b.append(self.conv1)\n","        b.append(self.bn1)\n","        b.append(self.layer1)\n","        b.append(self.layer2)\n","        b.append(self.layer3)\n","        b.append(self.layer4)\n","\n","        for i in range(len(b)):\n","            for j in b[i].modules():\n","                jj = 0\n","                for k in j.parameters():\n","                    jj += 1\n","                    if k.requires_grad:\n","                        yield k\n","\n","    def get_10x_lr_params(self):\n","        \"\"\"\n","        This generator returns all the parameters for the last layer of the net,\n","        which does the classification of pixel into classes\n","        \"\"\"\n","        b = []\n","        if self.multi_level:\n","            b.append(self.layer5.parameters())\n","        b.append(self.layer6.parameters())\n","\n","        for j in range(len(b)):\n","            for i in b[j]:\n","                yield i\n","\n","    def optim_parameters(self, lr):\n","        return [{'params': self.get_1x_lr_params_no_scale(), 'lr': lr},\n","                {'params': self.get_10x_lr_params(), 'lr': 10 * lr}]\n","\n","\n","def get_deeplab_v2(num_classes=19, pretrain=True, pretrain_model_path='DeepLab_resnet_pretrained_imagenet.pth'):\n","    model = ResNetMulti(Bottleneck, [3, 4, 23, 3], num_classes)\n","\n","    # Pretraining loading\n","    if pretrain:\n","        print('Deeplab pretraining loading...')\n","        saved_state_dict = torch.load(pretrain_model_path)\n","\n","        new_params = model.state_dict().copy()\n","        for i in saved_state_dict:\n","            i_parts = i.split('.')\n","            new_params['.'.join(i_parts[1:])] = saved_state_dict[i]\n","        model.load_state_dict(new_params, strict=False)\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"kv4QKgqpD6Dr"},"source":["## Get Pre Trained weights"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"UKhjuuhxD9NP","executionInfo":{"status":"ok","timestamp":1737817621415,"user_tz":-60,"elapsed":12,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[],"source":["#!pip install gdown"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1737817621415,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"},"user_tz":-60},"id":"y-R5fJBKEhQF","outputId":"b1ba1918-fa3f-4a97-cbff-c482dec571c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pre Trained model available on Drive\n","Pre Trained model already in local\n"]}],"source":["import gdown\n","\n","# File ID and destination path\n","file_id = \"1ZX0UCXvJwqd2uBGCX7LI2n-DfMg3t74v\"\n","url = f\"https://drive.google.com/uc?id={file_id}\"\n","output = \"deeplab_resnet_pretrained_imagenet.pth\"\n","\n","\n","if (os.path.exists(\"/content/drive/MyDrive/LoveDA/\" + output)):\n","    print(\"Pre Trained model available on Drive\")\n","    if (os.path.exists(\"./deeplab_resnet_pretrained_imagenet.pth\") == False):\n","      !cp /content/drive/MyDrive/LoveDA/deeplab_resnet_pretrained_imagenet.pth ./deeplab_resnet_pretrained_imagenet.pth\n","    else:\n","      print(\"Pre Trained model already in local\")\n","else:\n","    print(\"Downloading deeplab_resnet_pretrained_imagenet.pth...\")\n","    gdown.download(url, output, quiet=False)\n","    if SAVE_ON_DRIVE:\n","      print(\"Saving pre-trained model on drive\")\n","      !cp /content/deeplab_resnet_pretrained_imagenet.pth /content/drive/MyDrive/LoveDA/deeplab_resnet_pretrained_imagenet.pth"]},{"cell_type":"markdown","metadata":{"id":"3DwQcmOWpUxn"},"source":["## Training & Dataset creation"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"moWCCVykx1ac","executionInfo":{"status":"ok","timestamp":1737817621415,"user_tz":-60,"elapsed":8,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[],"source":["DEVICE = 'cuda' # 'cuda' or 'cpu'\n","\n","LR = 1e-3            # The initial Learning Rate\n","MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n","WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n","\n","NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)\n","STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)\n","GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n","\n","LOG_FREQUENCY = 10\n","NUM_CLASSES = len(LABEL_MAP)\n","BATCH_SIZE = 24"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MYlsgAtdPT9I","executionInfo":{"status":"ok","timestamp":1737817621415,"user_tz":-60,"elapsed":7,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"outputId":"87a778aa-07cf-46bb-c91f-13dfdbbf6fb0"},"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat '/content/drive/MyDrive/LoveDA/best_model.pth': No such file or directory\n"]}],"source":["if SAVE_ON_DRIVE:\n","  if TYPE != 'Train':\n","    !cp /content/drive/MyDrive/LoveDA/best_model.pth ./best_model.pth"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1078,"status":"ok","timestamp":1737817622488,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"},"user_tz":-60},"id":"Y8nlqvfeFpg0","outputId":"def06415-cb0b-4567-99af-9cdede021394"},"outputs":[{"output_type":"stream","name":"stdout","text":["Deeplab pretraining loading...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-55-bae244a7c21c>:179: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  saved_state_dict = torch.load(pretrain_model_path)\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","\n","pretrain_model_path = 'deeplab_resnet_pretrained_imagenet.pth'\n","path = './best_model_deeplab.pth'\n","drive_path = '/content/drive/MyDrive/LoveDA'\n","model_path = os.path.join(drive_path, path)\n","if TYPE == 'Train':\n","  path = pretrain_model_path\n","else:\n","  path = model_path\n","num_classes = len(LABEL_MAP)\n","model = get_deeplab_v2(num_classes=num_classes, pretrain=True, pretrain_model_path=path)\n","\n","#loss = SegmentationLoss(loss_config={'ce': True})\n","\n","# Example optimizer\n","optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n","\n","# train_dataset = LoveDADataset(split='Urban', type='Train', validation_ratio=0.2)\n","# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n","\n","# validation_dataset = LoveDADataset(split='Urban', type='Validation', validation_ratio=0.2)\n","# validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gT7bVXshNCk4","executionInfo":{"status":"ok","timestamp":1737817622488,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"outputId":"d04846d0-56fa-48bb-f243-292219fe1f48"},"outputs":[{"output_type":"stream","name":"stdout","text":["DEVICE is cuda\n"]}],"source":["\n","# Enable CUDA launch blocking for better debugging\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","# Enable CUDA device-side assertions\n","os.environ['TORCH_USE_CUDA_DSA'] = '1'\n","\n","# cudnn.benchmark # Calling this optimizes runtime\n","model = model.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n","\n","current_step = 0\n","# Start iterating over the epochs\n","RESIZE = 256\n","best_loss = float('inf')\n","best_model = model.state_dict()\n","\n","print(f\"DEVICE is {DEVICE}\")\n","if TYPE == 'Train':\n","  for epoch in range(NUM_EPOCHS):\n","      model.train()\n","      print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n","      epoch_loss = [0.0, 0]\n","      for (i, batch) in enumerate(train_loader):\n","          optimizer.zero_grad()\n","\n","          # Extract input\n","          images, masks, image_path = batch\n","          images = images.float().to(DEVICE)\n","          masks = masks.to(DEVICE)\n","\n","          # Forward to get output and loss\n","          outputs, _, _ = model(images)\n","          outputs = outputs.to(DEVICE)\n","\n","          loss = F.cross_entropy(outputs, masks, ignore_index=-1)\n","          print(f'Loss at batch {i}: {loss.item()}')\n","          loss.backward()\n","          optimizer.step()\n","          epoch_loss[0] += loss.item()\n","          epoch_loss[1] += images.size(0)\n","\n","\n","      # Evaluate model on the evaluation set and save the parameters if is better than best model\n","      model.eval()\n","      total_loss = 0.0\n","      outputs = []\n","      with torch.no_grad():\n","        for batch in validation_loader:\n","          images, masks, _ = batch\n","          images = images.float().to(DEVICE)\n","          masks = masks.to(DEVICE)\n","          # Forward Pass\n","          outputs = model(images)\n","          outputs = outputs.to(DEVICE)\n","          loss = F.cross_entropy(outputs, masks, ignore_index=-1)\n","          total_loss += loss.item()\n","\n","      print('Epoch {}, Loss {}'.format(epoch+1, total_loss))\n","      if total_loss < best_loss:\n","        best_loss = total_loss\n","        best_model = model.state_dict()\n","        #Save in Drive and local\n","        torch.save(best_model, 'best_model.pth')\n","        if SAVE_ON_DRIVE:\n","          !cp best_model.pth /content/drive/MyDrive/LoveDA/best_model.pth\n","\n","\n","      current_step += 1\n","      scheduler.step()\n","      print(f'[EPOCH {epoch+1}] Avg. Loss: {epoch_loss[0] / epoch_loss[1]}')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9njWK9xfNZgu"},"source":["## Debugging fossils"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"jX6mI5vOC8HE","executionInfo":{"status":"ok","timestamp":1737817622488,"user_tz":-60,"elapsed":3,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[],"source":["# # # Good paths\n","# # './Train/Urban/images_png/1458.png'\n","# # './Train/Urban/images_png/1549.png'\n","\n","# # # Bad paths\n","# # './Train/Urban/images_png/1788.png'\n","# # './Train/Urban/images_png/2300.png'\n","\n","# # ============ Load =============\n","# image_path = './Train/Urban/images_png/2300.png'\n","# image = pil_loader(image_path, 'RGB')\n","# mask_path = image_path.replace('images_png', 'masks_png')\n","# mask = pil_loader(mask_path, 'L')\n","\n","# img_transform = T.Compose([\n","#       T.Resize((256, 256)),\n","#       T.ToTensor(),\n","#   ])\n","# mask_transform = T.Compose([\n","#       T.Resize((256, 256)),\n","#   ])\n","\n","\n","# image = img_transform(image)\n","# mask = mask_transform(mask)\n","# mask = np.array(mask)\n","# mask = mask-1\n","# mask = torch.from_numpy(mask).long()\n","\n","# # ========== show ===============\n","# img_to_show = image.permute(1, 2, 0)\n","# img_to_show = img_to_show.numpy()\n","# print(f\"image shape is {img_to_show.shape}\")\n","# plt.imshow(img_to_show)\n","# plt.show()\n","\n","# mask_to_show = np.array(mask)\n","# toShow = new_colors_mask(mask)\n","# plt.legend(handles=getLegendHandles(), loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.5)\n","# plt.imshow(toShow)\n","# plt.show()\n","# print(f\"mask shape is {mask_to_show.shape}\")\n","\n","\n","# =============== loss ==============\n","# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","# os.environ['TORCH_USE_CUDA_DSA'] = '1'\n","\n","# image = image.to('cuda')\n","# mask = mask.to('cuda')\n","# model = model.to('cuda')\n","\n","# image = image.reshape([1, 3, 256, 256])\n","# mask = mask.reshape([1, 256, 256])\n","\n","# output, _, _ = model(image)\n","# output = output.to('cuda')\n","\n","# print(f\"output shape is {output.shape}\")\n","# print(f\"mask shape is {mask.shape}\")\n","# # for batch in mask:\n","# #   for row in batch:\n","# #     for cell in row:\n","# #       if cell.cpu().numpy() > 6 :\n","# #         print(cell.cpu().numpy())\n","# #         break\n","# ce_loss = F.cross_entropy(output, mask, ignore_index=7)\n","# print(f'Cross Entropy Loss: {ce_loss.item()}')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cQzNalCxc9Rf"},"source":["## TEST"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2458,"status":"ok","timestamp":1737817624944,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"},"user_tz":-60},"id":"IBtTssRUdmeN","outputId":"1e0b97f3-136f-41c3-c76a-51da5ef322bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: ptflops in /usr/local/lib/python3.11/dist-packages (0.7.4)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu121)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.11.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->torchmetrics) (12.6.85)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"]}],"source":["!pip install torchmetrics ptflops"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WiCaSHN1c6GA","outputId":"48fe236a-35d9-4b2a-bf20-764e3dbff64a"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-74-984487af1b92>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  best_model = torch.load(model_path, map_location=DEVICE)\n"]},{"output_type":"stream","name":"stdout","text":["Latency: 0.0377 seconds\n","FLOPs: 95570910776\n","Total number of parameters: 43016284\n"]},{"output_type":"stream","name":"stderr","text":[" 98%|█████████▊| 40/41 [01:23<00:01,  1.98s/it]"]}],"source":["from torchmetrics import Accuracy\n","from tqdm import tqdm\n","import time\n","import ptflops\n","\n","#Load best_model\n","\n","drive_path = '/content/drive/MyDrive/LoveDA'\n","model_name = 'best_model_deeplab.pth'\n","model_path = os.path.join(drive_path, model_name)\n","best_model = torch.load(model_path, map_location=DEVICE)\n","model.load_state_dict(best_model)\n","model = model.to(DEVICE)\n","\n","accuracy, mIoU = False, True\n","\n","validation_dataset = LoveDADataset(split='Rural', type='Train', folder = './Val',validation_ratio=0.0)\n","validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True)\n","\n","#### TEST LOOP\n","model.eval()\n","\n","# Latency\n","with torch.no_grad():\n","    start_time = time.time()\n","    for _ in range(1000):\n","        _ = model(torch.randn(1, 3, RESIZE, RESIZE).to(DEVICE))\n","    end_time = time.time()\n","latency = (end_time - start_time) / 1000\n","print(f\"Latency: {latency:.4f} seconds\")\n","\n","# FLOPs\n","macs, _ = ptflops.get_model_complexity_info(model,\n","  (3, RESIZE, RESIZE), as_strings=False,\n","  print_per_layer_stat=False, verbose=False)\n","flops = macs * 2  # MACs perform two FLOPs\n","print(\"FLOPs:\", flops)\n","\n","# Number of parameters\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"Total number of parameters: {total_params}\")\n","\n","if TYPE == 'Test':\n","  with torch.no_grad():\n","      total_union = torch.zeros(NUM_CLASSES).to(DEVICE)\n","      total_intersection = torch.zeros(NUM_CLASSES).to(DEVICE)\n","      meter = Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(DEVICE)\n","      for batch_img, batch_mask, _ in tqdm(validation_loader):\n","          batch_img, batch_mask = batch_img.to(DEVICE), batch_mask.to(DEVICE)\n","\n","          # Downscale images batch\n","          batch_img = F.interpolate(batch_img, size=(RESIZE, RESIZE), mode='bilinear')\n","\n","          # Forward pass\n","          cls_o = model(batch_img)\n","          cls_o = cls_o.to(DEVICE)\n","\n","          ## Upscale (bilinear interpolation to original size)\n","          h, w = batch_mask.size(1), batch_mask.size(2)\n","          cls_o = F.interpolate(cls_o, size=(h, w), mode='bilinear', align_corners=True)\n","\n","          # Shape: NBATCHES x classes x h x w\n","          class_indices = torch.argmax(cls_o, dim=1)  # Shape: NBATCHES x h x w\n","\n","          if accuracy:\n","          # Create a mask for valid targets (where target is not -1)\n","            valid_mask = (batch_mask != -1)  # Mask of shape: NBATCHES x h x w\n","            # Apply the mask to ignore -1 targets when updating the accuracy metric\n","            meter.update(class_indices[valid_mask], batch_mask[valid_mask])\n","\n","          if mIoU:\n","              for predicted, target in zip(class_indices, batch_mask): # Iterating image for image\n","                  for i in range(NUM_CLASSES):\n","                      total_intersection[i] += torch.sum(torch.logical_and(predicted == i, target == i))\n","                      total_union[i] += torch.sum(torch.logical_or(torch.logical_and(predicted == i, target != -1 ), target == i))\n","\n","  if accuracy:\n","    accuracy = meter.compute()\n","    print(f'\\nAccuracy on the target domain: {100 * accuracy:.2f}%')\n","\n","  if mIoU:\n","    intersection_over_union = total_intersection / total_union\n","    mIoU = torch.mean(intersection_over_union)\n","    print(f'\\nmIoU on the target domain: {mIoU}')\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["xSaOfED0-zoc","IrsSERdV_xZc","uZwN55AlCRZc","9njWK9xfNZgu"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}