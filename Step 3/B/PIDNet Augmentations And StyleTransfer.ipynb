{"cells":[{"cell_type":"markdown","metadata":{"id":"cbIwgK7-uZ0l"},"source":["**WARNING**: Remember to run the `ExtractBoundaries` notebook present in this same folder before running all (or specifically the `Import Boundaries` cell)"]},{"cell_type":"code","source":["SAVE_ON_DRIVE = True\n","TYPE = 'Train'\n","STYLE_TRANSFER = False"],"metadata":{"id":"qeY1VfJ5T8kV","executionInfo":{"status":"ok","timestamp":1737994694311,"user_tz":-60,"elapsed":381,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YwDm553BzPlB"},"source":["# Dataset initialization\n"]},{"cell_type":"markdown","source":["### Download Data"],"metadata":{"id":"7zeYu_g-bpip"}},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":true,"id":"cjuW8Yf0zO7K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737994696837,"user_tz":-60,"elapsed":2088,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"outputId":"adda217e-6db3-430e-b207-02a84b4b8e2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Dataset already in local\n"]}],"source":["from google.colab import drive\n","import os\n","import shutil\n","\n","drive.mount('/content/drive')\n","if (os.path.exists(\"./Train\") == False):\n","    if (os.path.exists(\"/content/drive/MyDrive/LoveDA/Train.zip\")):\n","        print(\"Dataset available on own drive, unzipping...\")\n","        !unzip -q /content/drive/MyDrive/LoveDA/Train.zip -d ./\n","    else:\n","        print(\"Downloading dataset...\")\n","        !wget -O Train.zip \"https://zenodo.org/records/5706578/files/Train.zip?download=1\"\n","        if(SAVE_ON_DRIVE):\n","            print(\"Saving dataset on drive...\")\n","            !cp Train.zip /content/drive/MyDrive/LoveDA/\n","        !unzip -q Train.zip -d ./\n","\n","else:\n","    print(\"Dataset already in local\")\n","\n","\n","if STYLE_TRANSFER:\n","  if (os.path.exists(\"./StyleTransfer\") == False):\n","    if (os.path.exists(\"/content/drive/MyDrive/LoveDA/StyleTransfer.zip\")):\n","      print(\"StyleTransfer available on own drive, unzipping...\")\n","      !mkdir ./StyleTransfer\n","      !unzip -q /content/drive/MyDrive/LoveDA/StyleTransfer.zip -d ./StyleTransfer\n","    else:\n","      print(\"Cant download StyleTransfer\")\n","  else:\n","    print(\"StyleTransfer already in local\")\n"]},{"cell_type":"markdown","source":["### Import Boundaries"],"metadata":{"id":"OftJbvlhH725"}},{"cell_type":"code","source":["# Paths\n","rural_boundaries_path = \"./Train/Rural/boundaries_png\"\n","rural_masks_path = './Train/Rural/masks_png'\n","\n","urban_boundaries_path = \"./Train/Urban/boundaries_png\"\n","urban_masks_path = './Train/Urban/masks_png'\n","drive_rural_boundaries_path = '/content/drive/MyDrive/LoveDA/boundaries/Rural/boundaries_png'\n","drive_urban_boundaries_path = '/content/drive/MyDrive/LoveDA/boundaries/Urban/boundaries_png'\n","\n","boundaries_paths = [rural_boundaries_path, urban_boundaries_path]\n","\n","# Make dir inside ./Train/...\n","for boundaries_path in boundaries_paths:\n","    if (os.path.exists(boundaries_path) == False):\n","        print(f\"Creating {boundaries_path}...\")\n","        os.makedirs(boundaries_path)\n","    else:\n","        print(f\"{boundaries_path} exists...\")\n","\n","\n","# Check if files are already present\n","rural_file_count = len([name for name in os.listdir(rural_boundaries_path) if os.path.isfile(os.path.join(rural_boundaries_path, name))])\n","rural_mask_file_count = len([name for name in os.listdir(rural_masks_path) if os.path.isfile(os.path.join(rural_masks_path, name))])\n","urban_file_count = len([name for name in os.listdir(urban_boundaries_path) if os.path.isfile(os.path.join(urban_boundaries_path, name))])\n","urban_mask_file_count = len([name for name in os.listdir(urban_masks_path) if os.path.isfile(os.path.join(urban_masks_path, name))])\n","\n","if (rural_file_count != rural_mask_file_count):\n","    print(f\"Importing boundaries, as we have {rural_file_count} rural boundaries as of now...\")\n","    shutil.copytree(drive_rural_boundaries_path, rural_boundaries_path, dirs_exist_ok=True)\n","else:\n","    print(f\"Rural boundaries already present, {rural_file_count} files...\")\n","\n","if (urban_file_count != urban_mask_file_count):\n","    print(f\"Importing boundaries, as we have {urban_file_count} urban boundaries as of now...\")\n","    shutil.copytree(drive_urban_boundaries_path, urban_boundaries_path, dirs_exist_ok=True)\n","else:\n","    print(f\"Urban boundaries already present, {urban_file_count} files...\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XObla4TbH9Y7","executionInfo":{"status":"ok","timestamp":1737994696837,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"outputId":"74db66d8-5333-481d-f25a-5a19ce75c6a7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["./Train/Rural/boundaries_png exists...\n","./Train/Urban/boundaries_png exists...\n","Rural boundaries already present, 1366 files...\n","Urban boundaries already present, 1156 files...\n"]}]},{"cell_type":"markdown","metadata":{"id":"68BKhuGh2vJH"},"source":["### Dataset Definition"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"iUsQczlZ3Wx0","executionInfo":{"status":"ok","timestamp":1737994699830,"user_tz":-60,"elapsed":2995,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[],"source":["import os\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import torchvision.transforms as T\n","import random\n","import cv2\n","\n","\n","def pil_loader(path, color_type):\n","    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n","    with open(path, 'rb') as f:\n","        img = Image.open(f)\n","        return img.convert(color_type)\n","\n","class LoveDADataset(Dataset):\n","    def __init__(self, baseTransform, augTransforms, split = 'Urban', typeDataset = 'Train', useBoundaries=True, styleTransfer = False, probStyle=0.0, validation_ratio=0.2, seed=265637):\n","        # Validate typeDataset input\n","        if typeDataset not in ['Train', 'Validation', 'Total']:\n","            raise ValueError(\"Invalid typeDataset. Expected 'Train' or 'Validation'.\")\n","        self.directory = []\n","        directory_path = os.path.join('./Train', split, 'images_png')\n","\n","        # Check if the directory exists\n","        if not os.path.exists(directory_path):\n","            raise FileNotFoundError(f\"Directory not found: {directory_path}\")\n","        # Get all image paths\n","        all_images = [os.path.join(directory_path, entry) for entry in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, entry))]\n","        # Shuffle images for random splitting\n","        random.seed(seed)\n","        random.shuffle(all_images)\n","        # Split into training and validation sets\n","        split_idx = int(len(all_images) * (1 - validation_ratio))\n","        if typeDataset == 'Train':\n","            self.directory = all_images[:split_idx]\n","            if styleTransfer:\n","              style_directory = []\n","              listFromStyleToOriginal = []\n","              # Remove ones that not exist in directory, we have StyleTransfer Only in train\n","              for el in self.directory:\n","                name = el.replace('Train/Urban/images_png', 'StyleTransfer/Urban')\n","                style_directory.append(name)\n","                listFromStyleToOriginal.append(el)\n","              self.style_directory = style_directory\n","              self.probStyle = probStyle\n","              self.listFromStyleToOriginal = listFromStyleToOriginal\n","\n","        elif typeDataset == 'Validation':\n","            self.directory = all_images[split_idx:]\n","        elif typeDataset == 'Total':\n","            self.directory = all_images\n","        else:\n","            raise ValueError(\"Invalid typeDataset. Expected 'Train' or 'Validation' or 'Total'.\")\n","        self.baseTransforms = baseTransform\n","        self.augTransforms = augTransforms\n","        self.useBoundaries = useBoundaries\n","        self.typeDataset = typeDataset\n","        self.styleTransfer = styleTransfer\n","        self.probStyle = probStyle\n","        # Print dataset size\n","        print(f\"Dataset size: {len(self.directory)}\")\n","\n","    def __len__(self):\n","        return len(self.directory)\n","\n","    def __getitem__(self, idx):\n","\n","        flagStyle = False\n","        if self.styleTransfer and random.random() < self.probStyle:\n","          image_path = self.style_directory[idx]\n","          original_path = self.listFromStyleToOriginal[idx]\n","          flagStyle = True\n","        else:\n","          image_path = self.directory[idx]\n","          original_path = image_path\n","\n","        image = pil_loader(image_path, 'RGB')\n","        mask_path = original_path.replace('images_png', 'masks_png')\n","        if not os.path.exists(mask_path):\n","          raise FileNotFoundError(f\"Mask not found: {mask_path}\")\n","        boundaries_path = original_path.replace('images_png', 'boundaries_png')\n","        if not os.path.exists(boundaries_path):\n","          raise FileNotFoundError(f\"Boundaries not found: {boundaries_path}\")\n","\n","        mask = pil_loader(mask_path, 'L')\n","        if flagStyle: # This is only because StyleTransfered images are 512x512\n","          # Resize mask to 512x512\n","          mask = T.Resize((512,512))(mask)\n","\n","        if self.useBoundaries:\n","          boundaries = pil_loader(boundaries_path, 'L')\n","          if flagStyle:\n","            boundaries = T.Resize((512,512))(boundaries)\n","        else:\n","          boundaries = mask # Just a placeholder to mantain current dimensionality. May cause errors, so use `useBoundaries` only in testing scenario\n","\n","        base_transformed = self.baseTransforms(image=np.array(image), mask=np.array(mask), boundaries=np.array(boundaries))\n","        base_image = base_transformed['image']\n","        base_mask = base_transformed['mask']\n","        base_boundaries = base_transformed['boundaries']\n","\n","        base_image = T.Compose([T.ToTensor()])(base_image)\n","        base_mask = torch.from_numpy(base_mask).long()\n","        base_mask -= 1\n","        base_boundaries = torch.from_numpy(base_boundaries)\n","\n","        if self.typeDataset == 'Train':\n","          return [base_image], [base_mask], image_path, [base_boundaries]\n","        else:\n","          return base_image, base_mask, image_path, base_boundaries"]},{"cell_type":"markdown","source":["### Dataset Utils"],"metadata":{"id":"xSaOfED0-zoc"}},{"cell_type":"code","source":["from collections import OrderedDict\n","COLOR_MAP = OrderedDict(\n","    Background=(255, 255, 255),\n","    Building=(255, 0, 0),\n","    Road=(255, 255, 0),\n","    Water=(0, 0, 255),\n","    Barren=(159, 129, 183),\n","    Forest=(34, 139, 34),\n","    Agricultural=(255, 195, 128),\n",")\n","\n","LABEL_MAP = OrderedDict(\n","    Background=0,\n","    Building=1,\n","    Road=2,\n","    Water=3,\n","    Barren=4,\n","    Forest=5,\n","    Agricultural=6,\n",")\n","inverted_label_map = OrderedDict((v, k) for k, v in LABEL_MAP.items())\n","\n","\n","def getLabelColor(label):\n","    # Default color for unclassified labels\n","    default_color = np.array([128, 128, 128])  # Gray\n","\n","    # Check if label exists in inverted_label_map\n","    label_name = inverted_label_map.get(label, None)\n","    if label_name is None or label_name not in COLOR_MAP:\n","        return default_color  # Return default color for unclassified\n","\n","    # Return the mapped color\n","    label_color = np.array(COLOR_MAP[label_name])\n","    return label_color\n","\n","\n","def getLegendHandles():\n","  handles = [mpatches.Patch(color=getLabelColor(i)/255, label=inverted_label_map[i]) for i in range(0, len(LABEL_MAP))]\n","  handles.append(mpatches.Patch(color=getLabelColor(-1)/255, label='Unclassified'))\n","  return handles\n","\n","def new_colors_mask(mask):\n","  new_image = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n","  for i, row in enumerate(mask):\n","    for j, cell in enumerate(row):\n","      new_image[i][j] = getLabelColor(cell.item())\n","  return new_image\n","\n"],"metadata":{"id":"tfi1pG3P-2H1","executionInfo":{"status":"ok","timestamp":1737994699830,"user_tz":-60,"elapsed":5,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Debug"],"metadata":{"id":"IrsSERdV_xZc"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"l5iBVMMn6-3-","executionInfo":{"status":"ok","timestamp":1737994699830,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[],"source":["# # Comment this cell to save GPU time\n","\n","# import matplotlib.pyplot as plt\n","# import torch\n","# from torch.utils.data import DataLoader\n","# import matplotlib.patches as mpatches\n","\n","# train_dataset = LoveDADataset(type='Train', seed=222)\n","# print(train_dataset.__len__())\n","\n","# # Get item\n","# image, mask, path, bd = train_dataset.__getitem__(88)\n","\n","# # Show path\n","# print(f\"Image is at {path}\")\n","\n","# # Show image\n","# image = image.permute(1, 2, 0)\n","# image = image.numpy()\n","# plt.imshow(image)\n","\n","# # Show mask\n","# new_image = new_colors_mask(mask)\n","# plt.imshow(image)\n","# plt.show()\n","# plt.legend(handles=getLegendHandles(), loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n","# plt.imshow(new_image)\n","# plt.show()\n","\n","# # Show boundaries\n","# # for row in bd:\n","# #     for col in row:\n","# #         if col != 0 and col != 1:\n","# #             print(col)\n","# bd = bd.numpy()\n","# plt.imshow(bd)\n"]},{"cell_type":"markdown","metadata":{"id":"uZwN55AlCRZc"},"source":["# Initialize model"]},{"cell_type":"markdown","source":["### PIDNet Util Modules"],"metadata":{"id":"V1PldejM1ZCB"}},{"cell_type":"code","source":["# ------------------------------------------------------------------------------\n","# Written by Jiacong Xu (jiacong.xu@tamu.edu)\n","# ------------------------------------------------------------------------------\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","BatchNorm2d = nn.BatchNorm2d\n","bn_mom = 0.1\n","algc = False\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=False):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.no_relu = no_relu\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","\n","        if self.no_relu:\n","            return out\n","        else:\n","            return self.relu(out)\n","\n","class Bottleneck(nn.Module):\n","    expansion = 2\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=True):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n","                               bias=False)\n","        self.bn3 = BatchNorm2d(planes * self.expansion, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.no_relu = no_relu\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        if self.no_relu:\n","            return out\n","        else:\n","            return self.relu(out)\n","\n","class segmenthead(nn.Module):\n","\n","    def __init__(self, inplanes, interplanes, outplanes, scale_factor=None):\n","        super(segmenthead, self).__init__()\n","        self.bn1 = BatchNorm2d(inplanes, momentum=bn_mom)\n","        self.conv1 = nn.Conv2d(inplanes, interplanes, kernel_size=3, padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(interplanes, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(interplanes, outplanes, kernel_size=1, padding=0, bias=True)\n","        self.scale_factor = scale_factor\n","\n","    def forward(self, x):\n","\n","        x = self.conv1(self.relu(self.bn1(x)))\n","        out = self.conv2(self.relu(self.bn2(x)))\n","\n","        if self.scale_factor is not None:\n","            height = x.shape[-2] * self.scale_factor\n","            width = x.shape[-1] * self.scale_factor\n","            out = F.interpolate(out,\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)\n","\n","        return out\n","\n","class DAPPM(nn.Module):\n","    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n","        super(DAPPM, self).__init__()\n","        bn_mom = 0.1\n","        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale0 = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.process1 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.process2 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.process3 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.process4 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.compression = nn.Sequential(\n","                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n","                                    )\n","        self.shortcut = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n","                                    )\n","\n","    def forward(self, x):\n","        width = x.shape[-1]\n","        height = x.shape[-2]\n","        x_list = []\n","\n","        x_list.append(self.scale0(x))\n","        x_list.append(self.process1((F.interpolate(self.scale1(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[0])))\n","        x_list.append((self.process2((F.interpolate(self.scale2(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[1]))))\n","        x_list.append(self.process3((F.interpolate(self.scale3(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[2])))\n","        x_list.append(self.process4((F.interpolate(self.scale4(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[3])))\n","\n","        out = self.compression(torch.cat(x_list, 1)) + self.shortcut(x)\n","        return out\n","\n","class PAPPM(nn.Module):\n","    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n","        super(PAPPM, self).__init__()\n","        bn_mom = 0.1\n","        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","\n","        self.scale0 = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","\n","        self.scale_process = nn.Sequential(\n","                                    BatchNorm(branch_planes*4, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes*4, branch_planes*4, kernel_size=3, padding=1, groups=4, bias=False),\n","                                    )\n","\n","\n","        self.compression = nn.Sequential(\n","                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n","                                    )\n","\n","        self.shortcut = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n","                                    )\n","\n","\n","    def forward(self, x):\n","        width = x.shape[-1]\n","        height = x.shape[-2]\n","        scale_list = []\n","\n","        x_ = self.scale0(x)\n","        scale_list.append(F.interpolate(self.scale1(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","        scale_list.append(F.interpolate(self.scale2(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","        scale_list.append(F.interpolate(self.scale3(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","        scale_list.append(F.interpolate(self.scale4(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","\n","        scale_out = self.scale_process(torch.cat(scale_list, 1))\n","\n","        out = self.compression(torch.cat([x_,scale_out], 1)) + self.shortcut(x)\n","        return out\n","\n","\n","class PagFM(nn.Module):\n","    def __init__(self, in_channels, mid_channels, after_relu=False, with_channel=False, BatchNorm=nn.BatchNorm2d):\n","        super(PagFM, self).__init__()\n","        self.with_channel = with_channel\n","        self.after_relu = after_relu\n","        self.f_x = nn.Sequential(\n","                                nn.Conv2d(in_channels, mid_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(mid_channels)\n","                                )\n","        self.f_y = nn.Sequential(\n","                                nn.Conv2d(in_channels, mid_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(mid_channels)\n","                                )\n","        if with_channel:\n","            self.up = nn.Sequential(\n","                                    nn.Conv2d(mid_channels, in_channels,\n","                                              kernel_size=1, bias=False),\n","                                    BatchNorm(in_channels)\n","                                   )\n","        if after_relu:\n","            self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x, y):\n","        input_size = x.size()\n","        if self.after_relu:\n","            y = self.relu(y)\n","            x = self.relu(x)\n","\n","        y_q = self.f_y(y)\n","        y_q = F.interpolate(y_q, size=[input_size[2], input_size[3]],\n","                            mode='bilinear', align_corners=False)\n","        x_k = self.f_x(x)\n","\n","        if self.with_channel:\n","            sim_map = torch.sigmoid(self.up(x_k * y_q))\n","        else:\n","            sim_map = torch.sigmoid(torch.sum(x_k * y_q, dim=1).unsqueeze(1))\n","\n","        y = F.interpolate(y, size=[input_size[2], input_size[3]],\n","                            mode='bilinear', align_corners=False)\n","        x = (1-sim_map)*x + sim_map*y\n","\n","        return x\n","\n","class Light_Bag(nn.Module):\n","    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n","        super(Light_Bag, self).__init__()\n","        self.conv_p = nn.Sequential(\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","        self.conv_i = nn.Sequential(\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","\n","    def forward(self, p, i, d):\n","        edge_att = torch.sigmoid(d)\n","\n","        p_add = self.conv_p((1-edge_att)*i + p)\n","        i_add = self.conv_i(i + edge_att*p)\n","\n","        return p_add + i_add\n","\n","\n","class DDFMv2(nn.Module):\n","    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n","        super(DDFMv2, self).__init__()\n","        self.conv_p = nn.Sequential(\n","                                BatchNorm(in_channels),\n","                                nn.ReLU(inplace=True),\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","        self.conv_i = nn.Sequential(\n","                                BatchNorm(in_channels),\n","                                nn.ReLU(inplace=True),\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","\n","    def forward(self, p, i, d):\n","        edge_att = torch.sigmoid(d)\n","\n","        p_add = self.conv_p((1-edge_att)*i + p)\n","        i_add = self.conv_i(i + edge_att*p)\n","\n","        return p_add + i_add\n","\n","class Bag(nn.Module):\n","    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n","        super(Bag, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","                                BatchNorm(in_channels),\n","                                nn.ReLU(inplace=True),\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=3, padding=1, bias=False)\n","                                )\n","\n","\n","    def forward(self, p, i, d):\n","        edge_att = torch.sigmoid(d)\n","        return self.conv(edge_att*p + (1-edge_att)*i)\n","\n","\n"],"metadata":{"id":"8lOLZdcA1W04","executionInfo":{"status":"ok","timestamp":1737994699830,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### PIDNet Definition"],"metadata":{"id":"uiBwR_Yg1dVO"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"dcHkfgpmCUys","executionInfo":{"status":"ok","timestamp":1737994699830,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[],"source":["# ------------------------------------------------------------------------------\n","# Written by Jiacong Xu (jiacong.xu@tamu.edu)\n","# ------------------------------------------------------------------------------\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","import logging\n","\n","BatchNorm2d = nn.BatchNorm2d\n","bn_mom = 0.1\n","algc = False\n","\n","\n","\n","class PIDNet(nn.Module):\n","\n","    def __init__(self, m=2, n=3, num_classes=19, planes=64, ppm_planes=96, head_planes=128, augment=True):\n","        super(PIDNet, self).__init__()\n","        self.augment = augment\n","\n","        # I Branch\n","        self.conv1 =  nn.Sequential(\n","                          nn.Conv2d(3,planes,kernel_size=3, stride=2, padding=1),\n","                          BatchNorm2d(planes, momentum=bn_mom),\n","                          nn.ReLU(inplace=True),\n","                          nn.Conv2d(planes,planes,kernel_size=3, stride=2, padding=1),\n","                          BatchNorm2d(planes, momentum=bn_mom),\n","                          nn.ReLU(inplace=True),\n","                      )\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.layer1 = self._make_layer(BasicBlock, planes, planes, m)\n","        self.layer2 = self._make_layer(BasicBlock, planes, planes * 2, m, stride=2)\n","        self.layer3 = self._make_layer(BasicBlock, planes * 2, planes * 4, n, stride=2)\n","        self.layer4 = self._make_layer(BasicBlock, planes * 4, planes * 8, n, stride=2)\n","        self.layer5 =  self._make_layer(Bottleneck, planes * 8, planes * 8, 2, stride=2)\n","\n","        # P Branch\n","        self.compression3 = nn.Sequential(\n","                                          nn.Conv2d(planes * 4, planes * 2, kernel_size=1, bias=False),\n","                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                          )\n","\n","        self.compression4 = nn.Sequential(\n","                                          nn.Conv2d(planes * 8, planes * 2, kernel_size=1, bias=False),\n","                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                          )\n","        self.pag3 = PagFM(planes * 2, planes)\n","        self.pag4 = PagFM(planes * 2, planes)\n","\n","        self.layer3_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n","        self.layer4_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n","        self.layer5_ = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n","\n","        # D Branch\n","        if m == 2:\n","            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes)\n","            self.layer4_d = self._make_layer(Bottleneck, planes, planes, 1)\n","            self.diff3 = nn.Sequential(\n","                                        nn.Conv2d(planes * 4, planes, kernel_size=3, padding=1, bias=False),\n","                                        BatchNorm2d(planes, momentum=bn_mom),\n","                                        )\n","            self.diff4 = nn.Sequential(\n","                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n","                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                     )\n","            self.spp = PAPPM(planes * 16, ppm_planes, planes * 4)\n","            self.dfm = Light_Bag(planes * 4, planes * 4)\n","        else:\n","            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n","            self.layer4_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n","            self.diff3 = nn.Sequential(\n","                                        nn.Conv2d(planes * 4, planes * 2, kernel_size=3, padding=1, bias=False),\n","                                        BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                        )\n","            self.diff4 = nn.Sequential(\n","                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n","                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                     )\n","            self.spp = DAPPM(planes * 16, ppm_planes, planes * 4)\n","            self.dfm = Bag(planes * 4, planes * 4)\n","\n","        self.layer5_d = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n","\n","        # Prediction Head\n","        if self.augment:\n","            self.seghead_p = segmenthead(planes * 2, head_planes, num_classes)\n","            self.seghead_d = segmenthead(planes * 2, planes, 1)\n","\n","        self.final_layer = segmenthead(planes * 4, head_planes, num_classes)\n","\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","\n","    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n","            )\n","\n","        layers = []\n","        layers.append(block(inplanes, planes, stride, downsample))\n","        inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            if i == (blocks-1):\n","                layers.append(block(inplanes, planes, stride=1, no_relu=True))\n","            else:\n","                layers.append(block(inplanes, planes, stride=1, no_relu=False))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _make_single_layer(self, block, inplanes, planes, stride=1):\n","        downsample = None\n","        if stride != 1 or inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n","            )\n","\n","        layer = block(inplanes, planes, stride, downsample, no_relu=True)\n","\n","        return layer\n","\n","    def forward(self, x):\n","\n","        width_output = x.shape[-1] // 8\n","        height_output = x.shape[-2] // 8\n","\n","        x = self.conv1(x)\n","        x = self.layer1(x)\n","        x = self.relu(self.layer2(self.relu(x)))\n","        x_ = self.layer3_(x)\n","        x_d = self.layer3_d(x)\n","\n","        x = self.relu(self.layer3(x))\n","        x_ = self.pag3(x_, self.compression3(x))\n","        x_d = x_d + F.interpolate(\n","                        self.diff3(x),\n","                        size=[height_output, width_output],\n","                        mode='bilinear', align_corners=algc)\n","        if self.augment:\n","            temp_p = x_\n","\n","        x = self.relu(self.layer4(x))\n","        x_ = self.layer4_(self.relu(x_))\n","        x_d = self.layer4_d(self.relu(x_d))\n","\n","        x_ = self.pag4(x_, self.compression4(x))\n","        x_d = x_d + F.interpolate(\n","                        self.diff4(x),\n","                        size=[height_output, width_output],\n","                        mode='bilinear', align_corners=algc)\n","        if self.augment:\n","            temp_d = x_d\n","\n","        x_ = self.layer5_(self.relu(x_))\n","        x_d = self.layer5_d(self.relu(x_d))\n","        x = F.interpolate(\n","                        self.spp(self.layer5(x)),\n","                        size=[height_output, width_output],\n","                        mode='bilinear', align_corners=algc)\n","\n","        x_ = self.final_layer(self.dfm(x_, x, x_d))\n","\n","        if self.augment:\n","            x_extra_p = self.seghead_p(temp_p)\n","            x_extra_d = self.seghead_d(temp_d)\n","            return [x_extra_p, x_, x_extra_d]\n","        else:\n","            return x_\n","\n","def get_seg_model(cfg, imgnet_pretrained):\n","\n","    if 's' in cfg.MODEL.NAME:\n","        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=32, ppm_planes=96, head_planes=128, augment=True)\n","    elif 'm' in cfg.MODEL.NAME:\n","        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=96, head_planes=128, augment=True)\n","    else:\n","        model = PIDNet(m=3, n=4, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=112, head_planes=256, augment=True)\n","\n","    if imgnet_pretrained:\n","        pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n","        model_dict = model.state_dict()\n","        pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n","        model_dict.update(pretrained_state)\n","        msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n","        logging.info('Attention!!!')\n","        logging.info(msg)\n","        logging.info('Over!!!')\n","        model.load_state_dict(model_dict, strict = False)\n","    else:\n","        pretrained_dict = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')\n","        if 'state_dict' in pretrained_dict:\n","            pretrained_dict = pretrained_dict['state_dict']\n","        model_dict = model.state_dict()\n","        pretrained_dict = {k[6:]: v for k, v in pretrained_dict.items() if (k[6:] in model_dict and v.shape == model_dict[k[6:]].shape)}\n","        msg = 'Loaded {} parameters!'.format(len(pretrained_dict))\n","        logging.info('Attention!!!')\n","        logging.info(msg)\n","        logging.info('Over!!!')\n","        model_dict.update(pretrained_dict)\n","        model.load_state_dict(model_dict, strict = False)\n","\n","    return model\n","\n","def get_pred_model(name, num_classes):\n","\n","    if 's' in name:\n","        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=False)\n","    elif 'm' in name:\n","        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=False)\n","    else:\n","        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=False)\n","\n","    return model"]},{"cell_type":"markdown","source":["### Load PIDNet Model"],"metadata":{"id":"isRcFkZ3CxpI"}},{"cell_type":"code","source":["import gdown\n","import tarfile\n","\n","if (os.path.exists(\"./PIDNet_S_ImageNet.pth.tar\") == False):\n","  url = \"https://drive.google.com/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-\"\n","  output = \"./\"\n","  gdown.download(url, output, quiet=False)\n","# Then keep as tar, as it's already the correct format to feed the model\n","\n","# Create a config object with required parameters\n","class Config:\n","    class MODEL:\n","        NAME = 'pidnet_s'  # or 'pidnet_m' or 'pidnet_l'\n","        PRETRAINED = 'PIDNet_S_ImageNet.pth.tar'\n","    class DATASET:\n","        NUM_CLASSES = len(LABEL_MAP)\n","\n","cfg = Config()\n","\n","model = get_seg_model(cfg, imgnet_pretrained=True)\n","# model = get_pred_model('s', len(LABEL_MAP))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ujP_2PffskRk","executionInfo":{"status":"ok","timestamp":1737994700607,"user_tz":-60,"elapsed":781,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"outputId":"e510320e-1f40-47e1-decc-9b07a7f658f2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-d799b7f58ec7>:193: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n"]}]},{"cell_type":"markdown","source":["### Model Debugging"],"metadata":{"id":"RtcmrSGDcA4w"}},{"cell_type":"code","source":["# import torch.nn.functional as F\n","# from torch.utils.data import DataLoader\n","# import matplotlib.pyplot as plt\n","\n","# train_dataset = LoveDADataset(type='Train')\n","# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n","\n","# model = model.train()\n","# model = model.to('cuda')\n","\n","# for img, mask, _ in train_loader:\n","#     print(f\"iamge shape: {img.shape}\")\n","#     print(f\"mask shape: {mask.shape}\")\n","\n","#     img = img.to('cuda')\n","#     outputs = model(img)\n","\n","#     # bilinear interpolation\n","#     h, w = mask.size(1), mask.size(2)\n","#     ph, pw = outputs[0].size(2), outputs[0].size(3)\n","#     if ph != h or pw != w:\n","#         for i in range(len(outputs)):\n","#             outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear',\n","#                                        align_corners=True)\n","\n","#     for output in outputs:\n","#       print(output.shape)\n","#     break\n","\n","# print(\"===================== Original Image =====================\")\n","# plt.imshow(img[0].permute(1, 2, 0).cpu().numpy())\n","# plt.show()\n","\n","# print(\"===================== Ground Truth =====================\")\n","# plt.imshow(mask[0].cpu().numpy())\n","# plt.show()\n","\n","# print(\"===================== Predicted Mask =====================\")\n","# plt.imshow(torch.argmax(outputs[0][0], dim=0).cpu().numpy())\n","# plt.show()"],"metadata":{"id":"xkTB43MxcDRA","executionInfo":{"status":"ok","timestamp":1737994700607,"user_tz":-60,"elapsed":5,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3DwQcmOWpUxn"},"source":["# Training & Dataset creation"]},{"cell_type":"markdown","source":["### Ablations and Macros"],"metadata":{"id":"iToC6F28cnl5"}},{"cell_type":"code","source":["DEVICE = 'cuda' # 'cuda' or 'cpu'\n","\n","LR = 2e-3            # The initial Learning Rate -- I increased it using quadratic rule in relation with batch size\n","MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n","WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n","\n","NUM_EPOCHS = 20      # Total number of training epochs (iterations over dataset)\n","STEP_SIZE = 21      # How many epochs before decreasing learning rate (if using a step-down policy) -- Trying to keep a 2:3 ratio with NUM_EPOCHS\n","GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n","\n","LOG_FREQUENCY = 5\n","NUM_CLASSES = len(LABEL_MAP)\n","BATCH_SIZE = 64"],"metadata":{"id":"moWCCVykx1ac","executionInfo":{"status":"ok","timestamp":1737994700608,"user_tz":-60,"elapsed":5,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["#!pip install -U albumentations"],"metadata":{"id":"8Sd25sfowP1F","executionInfo":{"status":"ok","timestamp":1737994700608,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### Setup, Create Datasets and DataLoaders. With annexed transforms."],"metadata":{"id":"IwDwW9Hac6SS"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","from albumentations import Compose, HorizontalFlip, RandomRotate90, RandomScale, RandomCrop, GaussNoise, Rotate, Resize, OneOf, Normalize, ColorJitter, GaussianBlur\n","from albumentations.pytorch import ToTensorV2\n","\n","#How big should be the image that we feed to the model?\n","RESIZE = 512\n","# DEFINE TRANSFORMATIONS HERE\n","# To Tensor is not needed since its performed inside the getitem\n","\n","\n","AUGMENTATIONS = {\n","    'Resize': Compose([\n","            Resize(RESIZE, RESIZE),\n","    ], additional_targets={\"boundaries\": \"mask\"}),\n","    'Normalize': Compose([\n","            Normalize(mean=(123.675, 116.28, 103.53), std=(58.395, 57.12, 57.375), max_pixel_value=1.0, always_apply=True),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomCropOrResize': Compose([\n","            OneOf([\n","                RandomCrop(RESIZE, RESIZE, p=0.5),  # Random crop to resize\n","                Resize(RESIZE, RESIZE, p=0.5)\n","            ], p=1)\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'Jitter+Resize': Compose([\n","            ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'NormalizeOnRural+Resize': Compose([\n","            Normalize(mean=(73.532, 80.017, 74.593),\n","                      std=(41.493, 35.653, 33.747), max_pixel_value=1.0, always_apply=True),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'GaussianBlur+Resize': Compose([\n","            GaussianBlur(p=0.5),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomFlipOrRotate+Resize': Compose([\n","            OneOf([\n","                HorizontalFlip(p=0.5),\n","                RandomRotate90(p=0.5),\n","            ], p=0.5),\n","            Resize(RESIZE, RESIZE)\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'RandomCrop600-900+Resize': Compose([\n","            RandomCrop(900, 900, p=0.3),\n","            RandomCrop(800, 800, p=0.3),\n","            RandomCrop(700, 700, p=0.2),\n","            RandomCrop(600, 600, p=0.2),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'Jitter+RotateFlip+Resize': Compose([\n","            ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n","            OneOf([\n","                HorizontalFlip(p=0.5),\n","                RandomRotate90(p=0.5),\n","            ], p=0.5),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'Jitter+RandomCrop600-900+Resize': Compose([\n","            ColorJitter(brightness=0.3, contrast=0, saturation=0, hue=0, p=0.5),\n","            RandomCrop(900, 900, p=0.3),\n","            RandomCrop(800, 800, p=0.3),\n","            RandomCrop(700, 700, p=0.2),\n","            RandomCrop(600, 600, p=0.2),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'Brightess+Resize': Compose([\n","            ColorJitter(brightness=0.3, contrast=0, saturation=0, hue=0, p=0.5),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"}),\n","    'Jitter+RotateFlip+RandomCrop600-900+Resize_LESSPROB': Compose([\n","            ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.3),\n","            OneOf([\n","                HorizontalFlip(p=0.5),\n","                RandomRotate90(p=0.5),\n","              ], p=0.3),\n","            OneOf([\n","                RandomCrop(900, 900, p=0.3),\n","                RandomCrop(800, 800, p=0.3),\n","                RandomCrop(700, 700, p=0.2),\n","                RandomCrop(600, 600, p=0.2),\n","                ], p= 0.3),\n","            Resize(RESIZE, RESIZE),\n","            ], additional_targets={\"boundaries\": \"mask\"})\n","\n","\n","\n","}\n","\n","#CHOOSE_TRANSFORM = ['Jitter+Resize', 'GaussianBlur+Resize', 'NormalizeOnRural+Resize',\n","CHOOSE_TRANSFORM =  ['Jitter+RandomCrop600-900+Resize']\n","transforms = [AUGMENTATIONS[transform] for transform in CHOOSE_TRANSFORM]"],"metadata":{"id":"J8DhNU7Lc7aD","executionInfo":{"status":"ok","timestamp":1737994701284,"user_tz":-60,"elapsed":680,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"eadaa031-3de0-41e5-8cbd-bceae402bd9a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.1 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","  check_for_updates()\n"]}]},{"cell_type":"markdown","source":["### Losses"],"metadata":{"id":"l18jlCwmiTZp"}},{"cell_type":"code","source":["def weighted_bce(bd_pre, target):\n","    n, c, h, w = bd_pre.size()\n","    log_p = bd_pre.permute(0,2,3,1).contiguous().view(1, -1)\n","    target_t = target.view(1, -1)\n","\n","    pos_index = (target_t == 1)\n","    neg_index = (target_t == 0)\n","\n","    weight = torch.zeros_like(log_p)\n","    pos_num = pos_index.sum()\n","    neg_num = neg_index.sum()\n","    sum_num = pos_num + neg_num\n","    weight[pos_index] = neg_num * 1.0 / sum_num\n","    weight[neg_index] = pos_num * 1.0 / sum_num\n","\n","    loss = F.binary_cross_entropy_with_logits(log_p, target_t, weight, reduction='mean')\n","\n","    return loss\n","\n","def boundary_loss(bd_pre, bd_gt):\n","    loss = 20.0 * weighted_bce(bd_pre, bd_gt)\n","    return loss\n","\n","# TODO EXTRA add weights=class_weights to nn.CrossEntropyLoss()\n","# TODO EXTRA use OHCE instead of basic one\n","def cross_entropy(score, target):\n","    compute_ce_loss = nn.CrossEntropyLoss(ignore_index=-1)\n","\n","    # See paper for weights. In order of loss index: (0.4, 20, 1, 1) # But on cfg they set everything to 0.5\n","    balance_weights = [0.4, 1]\n","    sb_weights = 1\n","\n","    # print(f\"DEBUG: inside cross_entropy: len(score) = {len(score)}\")\n","    if len(balance_weights) == len(score):\n","        return sum([w * compute_ce_loss(x, target) for (w, x) in zip(balance_weights, score)])\n","    elif len(score) == 1:\n","        return sb_weights * compute_ce_loss(score[0], target)\n","    else:\n","        raise ValueError(\"lengths of prediction and target are not identical!\")\n","\n","sem_loss = cross_entropy\n","bd_loss = boundary_loss"],"metadata":{"id":"tU1zr5WKiU0y","executionInfo":{"status":"ok","timestamp":1737994701284,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["### Training Loop"],"metadata":{"id":"_rRjwVYaaFG8"}},{"cell_type":"code","execution_count":15,"metadata":{"id":"Y8nlqvfeFpg0","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1L4Bj8Dj2_Uo7KGFnjP9TJTUKmch3S76u"},"outputId":"a86a7cf5-d897-454e-c288-4599f4f5bc2c","collapsed":true,"executionInfo":{"status":"ok","timestamp":1737995479575,"user_tz":-60,"elapsed":778295,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","\n","for transform, name in zip(transforms, CHOOSE_TRANSFORM):\n","  cfg = Config()\n","  model = get_seg_model(cfg, imgnet_pretrained=True)\n","\n","  style_transf = ''\n","  if STYLE_TRANSFER:\n","    style_transf = '_styleTransfer'\n","\n","  SAVE_MODEL_AS = f'best_model_PIDNET{style_transf}_{name}.pth'\n","\n","  # Dataset and Loader\n","  train_dataset = LoveDADataset(baseTransform=transform, augTransforms=None ,split='Urban', typeDataset='Train', styleTransfer=STYLE_TRANSFER, probStyle=0.5, validation_ratio=0.2)\n","\n","  train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n","                            num_workers=2, drop_last=True, pin_memory=True)\n","\n","  validation_dataset = LoveDADataset(baseTransform=AUGMENTATIONS['Resize'], augTransforms=None,\n","                        split='Urban', typeDataset='Validation', validation_ratio=0.2)\n","\n","  validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True, pin_memory=True)\n","\n","  # Optimizier and Scheduler\n","  optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n","\n","  best_loss = float('inf')\n","  best_model = model.state_dict()\n","  model = model.to(DEVICE)\n","  print(f\"DEVICE is {DEVICE}\")\n","  if TYPE == 'Train':\n","    for epoch in range(NUM_EPOCHS):\n","      model.train()\n","      print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n","      epoch_loss = [0.0, 0]\n","      for (batch_i, batch) in enumerate(train_loader):\n","          optimizer.zero_grad()\n","\n","          ### Extract input\n","          image_list, masks_list, img_path, bd_gts_list = batch\n","\n","          index = 0\n","          for images, masks, bd_gts in zip(image_list, masks_list, bd_gts_list):\n","            images = images.to(DEVICE)\n","            masks = masks.to(DEVICE)\n","            bd_gts = bd_gts.float().to(DEVICE)\n","\n","            ### ===> Forward, Upscale, Compute Losses\n","            ## Forward\n","            outputs = model(images) # in model.train() mode batch size must be > 1 I think\n","                                    # NOTE: we have 3 heads (i.e. 3 outputs) but 4 losses: 2nd head is used for both S and BAS\n","\n","            ## Upscale (bilinear interpolation - not learned)\n","            h, w = masks.size(1), masks.size(2)\n","            ph, pw = outputs[0].size(2), outputs[0].size(3)\n","            if ph != h or pw != w:\n","                for i in range(len(outputs)):\n","                    outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n","\n","            ## Losses\n","            # Semantic Losses (l_0 and l_2)\n","            loss_s = sem_loss(outputs[:-1], masks) # output #1 and #2 are segmentation predictions (i.e. low level (P) and high+low level (PI) respectively)\n","            # Boundary Loss (l_1)\n","            loss_b = bd_loss(outputs[-1], bd_gts) # output #3 is the boundary prediction\n","\n","            # Boundary AwareneSS (BAS) Loss (l_3)\n","            filler = torch.ones_like(masks) * -1\n","            bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:]) > 0.8, masks, filler)\n","                                # REMEMBER to wrap in list, as the checks in ce use that to know what to do\n","            loss_sb = sem_loss([outputs[-2]], bd_label) # output #2 is the PI segmentation prediction, done here in BAS mode (see `filler` variable)\n","\n","            # Complete Loss\n","            loss = loss_s + loss_b + loss_sb # The coefficients of the sum of the four losses (0.4, 20, 1, 1) are taken into account in the various `sem_loss` and `bd_loss`\n","            ### <=== Forward, Upscale, Compute Losses\n","\n","            ### Backprop\n","            if batch_i % LOG_FREQUENCY == 0:\n","                if index > 0:\n","                  print(f'Augmented images loss: {loss.item()}')\n","                else:\n","                  print(f'Loss at batch {batch_i}: {loss.item()}')\n","            loss.backward()\n","\n","            optimizer.step()\n","            epoch_loss[0] += loss.item()\n","            epoch_loss[1] += images.size(0)\n","            index+=1\n","\n","      # Evaluate model on the evaluation set and save the parameters if is better than best model\n","      model.eval()\n","      total_loss = 0.0\n","      outputs = []\n","      with torch.no_grad():\n","        for (batch_i, batch) in enumerate(validation_loader):\n","          ### Extract input\n","          images, masks, img_path, bd_gts = batch\n","          images = images.float().to(DEVICE)\n","          masks = masks.to(DEVICE)\n","          bd_gts = bd_gts.float().to(DEVICE)\n","\n","          ### ===> Forward, Upscale, Compute Losses\n","          ## Forward\n","          outputs = model(images)\n","\n","          ## Upscale (bilinear interpolation - not learned)\n","          h, w = masks.size(1), masks.size(2)\n","          ph, pw = outputs[0].size(2), outputs[0].size(3)\n","          if ph != h or pw != w:\n","              for i in range(len(outputs)):\n","                  outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n","\n","          ## Losses\n","          # Semantic Losses (l_0 and l_2)\n","          loss_s = sem_loss(outputs[:-1], masks)\n","\n","          # Boundary Loss (l_1)\n","          loss_b = bd_loss(outputs[-1], bd_gts)\n","\n","          # Boundary AwareneSS (BAS) Loss (l_3)\n","          filler = torch.ones_like(masks) * -1\n","          bd_label = torch.where(F.sigmoid(outputs[-1][:,0,:,:]) > 0.8, masks, filler)\n","          loss_sb = sem_loss([outputs[-2]], bd_label)\n","\n","          # Complete Loss\n","          loss = loss_s + loss_b + loss_sb\n","          ### <=== Forward, Upscale, Compute Losses\n","          total_loss += loss.item()\n","\n","      print('Epoch {}, Loss {}'.format(epoch+1, total_loss))\n","      if total_loss < best_loss:\n","        best_loss = total_loss\n","        best_model = model.state_dict()\n","        #Save in Drive and local\n","        torch.save(best_model, SAVE_MODEL_AS)\n","        if SAVE_ON_DRIVE:\n","          !cp {SAVE_MODEL_AS} /content/drive/MyDrive/LoveDA/{SAVE_MODEL_AS}\n","          print(f\"model succesfully saved on drive. loss went down to {best_loss}\")\n","\n","      scheduler.step()\n","      print(f'[EPOCH {epoch+1}] Avg. Loss: {epoch_loss[0] / epoch_loss[1]}')\n","\n","      # Create a figure with 1 row and 3 columns\n","      fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","      # Plot the original image\n","      axes[0].imshow(images[0].permute(1, 2, 0).cpu().numpy())\n","      axes[0].set_title(\"Original Image\")\n","      axes[0].axis('off')\n","\n","      # Plot the ground truth mask\n","      axes[1].imshow(masks[0].cpu().numpy())\n","      axes[1].set_title(\"Ground Truth\")\n","      axes[1].axis('off')\n","\n","      # Plot the predicted mask\n","      axes[2].imshow(torch.argmax(outputs[0][0], dim=0).cpu().numpy())\n","      axes[2].set_title(\"Predicted Mask\")\n","      axes[2].axis('off')\n","\n","      # Display the figure\n","      plt.tight_layout()\n","      plt.show()"]},{"cell_type":"code","source":["# from google.colab import runtime\n","# runtime.unassign()"],"metadata":{"id":"D2JY8zlZgo40","executionInfo":{"status":"ok","timestamp":1737995479575,"user_tz":-60,"elapsed":5,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## Try styleTransfer dataset\n"],"metadata":{"id":"55vVIUJRV_Vj"}},{"cell_type":"code","source":["# import matplotlib.pyplot as plt\n","# for i in range(10):\n","#   image, mask, path, boundaries = train_dataset.__getitem__(i)\n","#   image = image[0]\n","#   mask = mask[0]\n","#   boundaries = boundaries[0]\n","#   print(path)\n","#   # Draw image\n","#   plt.imshow(image.permute(1, 2, 0).cpu().numpy())\n","#   plt.show()\n","#   # Draw mask\n","#   plt.imshow(mask.cpu().numpy())\n","#   plt.show()\n","#   # Draw boundaries in greyscale\n","#   plt.imshow(boundaries.cpu().numpy(), cmap='gray')\n","#   plt.show()"],"metadata":{"id":"to4-qfn3SBgj","executionInfo":{"status":"ok","timestamp":1737995479575,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# TEST"],"metadata":{"id":"soROYpcYyajp"}},{"cell_type":"code","source":["!pip install torchmetrics ptflops"],"metadata":{"id":"7DKgjGPKydLH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737995480746,"user_tz":-60,"elapsed":1175,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"outputId":"a6fce588-4c61-4b5d-f9dc-23bae30065f9"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: ptflops in /usr/local/lib/python3.11/dist-packages (0.7.4)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu121)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.11.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->torchmetrics) (12.6.85)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"]}]},{"cell_type":"code","source":["from torchmetrics import Accuracy\n","from tqdm import tqdm\n","import time\n","import ptflops\n","\n","TYPE = 'Test'\n","# TARGET = 'Rural' # ALREADY RUNNING ON BOTH BELOW\n","#Load best_model\n","#!cp /content/drive/MyDrive/LoveDA/best_model_step2b.pth /content/best_model_step2b.pth\n","\n","\n","model = get_seg_model(cfg, imgnet_pretrained=False)\n","# original model path: /content/drive/MyDrive/LoveDA/best_model_step2b.pth\n","best_model = torch.load(f'/content/best_model_{CHOOSE_TRANSFORM}.pth', weights_only=True)\n","model.load_state_dict(best_model)\n","model = model.to(DEVICE)\n","\n","accuracy, mIoU = True, True\n","\n","TARGETs = ['Urban', 'Rural']\n","\n","for TARGET in TARGETs:\n","  if TARGET == 'Urban': # Here we just validate on less images (20%) if URBAN, as it's not the focus of step 3b.\n","      target_type = 'Validation'\n","  elif TARGET == 'Rural': # While we take the entirety of the Rural folder in case of Rural\n","      target_type = 'Total'\n","  else:\n","      raise ValueError(\"TARGET must be 'Urban' or 'Rural'\")\n","\n","  if CHOOSE_TRANSFORM == 'RandomCropOrResize':\n","    test_augmentation = AUGMENTATIONS['RandomCrop512']\n","  elif CHOOSE_TRANSFORM == 'RandomCropXXX':\n","    test_augmentation = AUGMENTATIONS['None']\n","  elif CHOOSE_TRANSFORM == 'Jitter':\n","    test_augmentation = AUGMENTATIONS['None']\n","  elif CHOOSE_TRANSFORM == 'GaussianBlur':\n","    test_augmentation = AUGMENTATIONS['None']\n","  else:\n","    test_augmentation = AUGMENTATIONS[CHOOSE_TRANSFORM]\n","\n","  test_dataset = LoveDADataset(baseTranform=test_augmentation, augTransforms=None, split=TARGET, type=target_type)\n","  test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True, pin_memory=True)\n","\n","  #### TEST LOOP\n","  model.eval()\n","  print(f\"Testing on domain={TARGET} using augmentations={CHOOSE_TRANSFORM} on a {target_type} split\")\n","\n","  # Latency\n","  with torch.no_grad():\n","      start_time = time.time()\n","      for _ in range(100):\n","          _ = model(torch.randn(1, 3, RESIZE, RESIZE).to(DEVICE))\n","      end_time = time.time()\n","  latency = (end_time - start_time) / 100\n","  print(f\"Latency: {latency:.4f} seconds\")\n","\n","  # FLOPs\n","  macs, _ = ptflops.get_model_complexity_info(model,\n","    (3, RESIZE, RESIZE), as_strings=False,\n","    print_per_layer_stat=False, verbose=False)\n","  flops = macs * 2  # MACs perform two FLOPs\n","  print(\"FLOPs:\", flops)\n","\n","  # Number of parameters\n","  total_params = sum(p.numel() for p in model.parameters())\n","  print(f\"Total number of parameters: {total_params}\")\n","\n","  if TYPE == 'Test':\n","    with torch.no_grad():\n","        total_union = torch.zeros(NUM_CLASSES).to(DEVICE)\n","        total_intersection = torch.zeros(NUM_CLASSES).to(DEVICE)\n","        meter = Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(DEVICE)\n","        for (batch) in tqdm(test_loader):\n","            ### Extract input\n","            images, masks, img_path, bd_gts = batch\n","            images = images.float().to(DEVICE)\n","            masks = masks.to(DEVICE)\n","\n","            ### ===> Forward, Upscale, Compute Losses\n","            ## Forward\n","            outputs = model(images)\n","\n","            ## Upscale (bilinear interpolation - not learned)\n","            h, w = masks.size(1), masks.size(2)\n","            ph, pw = outputs[0].size(2), outputs[0].size(3)\n","            if ph != h or pw != w:\n","                for i in range(len(outputs)):\n","                    outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n","\n","            # Output 1 is the prediction\n","\n","            # Shape: NBATCHES x classes x h x w\n","            class_indices = torch.argmax(outputs[1], dim=1)  # Shape: NBATCHES x h x w\n","\n","            if accuracy:\n","            # Create a mask for valid targets (where target is not -1)\n","              valid_mask = (masks != -1)  # Mask of shape: NBATCHES x h x w\n","              # Apply the mask to ignore -1 targets when updating the accuracy metric\n","              meter.update(class_indices[valid_mask], masks[valid_mask])\n","\n","            if mIoU:\n","              for predicted, target in zip(class_indices, masks):\n","                for i in range(NUM_CLASSES):\n","                  total_intersection[i] += torch.sum(torch.logical_and(predicted == i, target == i))\n","                  total_union[i] += torch.sum(torch.logical_or(predicted == i, target == i))\n","\n","    if accuracy:\n","      accuracy = meter.compute()\n","      print(f'\\nAccuracy on the target domain: {100 * accuracy:.2f}%')\n","\n","    if mIoU:\n","      intersection_over_union = total_intersection / total_union\n","\n","      # Per class IoU\n","      for i, iou in enumerate(intersection_over_union):\n","          class_name = list(LABEL_MAP.keys())[list(LABEL_MAP.values()).index(i)]  # Get the class name from LABEL_MAP\n","          print(f'{class_name} IoU: {iou:.4f}')\n","\n","      mIoU = torch.mean(intersection_over_union)\n","      print(f'\\nmIoU on the target domain: {mIoU}')\n","\n","  print(\"========================================================================\")\n"],"metadata":{"id":"M3c7DFDRymnI","colab":{"base_uri":"https://localhost:8080/","height":428},"executionInfo":{"status":"error","timestamp":1737995483253,"user_tz":-60,"elapsed":2511,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"outputId":"8ffd3461-a083-4774-cf24-bd577b0c659b"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-d799b7f58ec7>:203: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  pretrained_dict = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: \"/content/best_model_['Jitter+RandomCrop600-900+Resize'].pth\"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-8b45c389d4d7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_seg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgnet_pretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# original model path: /content/drive/MyDrive/LoveDA/best_model_step2b.pth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/content/best_model_{CHOOSE_TRANSFORM}.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"/content/best_model_['Jitter+RandomCrop600-900+Resize'].pth\""]}]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["7zeYu_g-bpip","OftJbvlhH725","xSaOfED0-zoc","IrsSERdV_xZc","V1PldejM1ZCB","uiBwR_Yg1dVO","isRcFkZ3CxpI","RtcmrSGDcA4w","l18jlCwmiTZp"],"gpuType":"L4","machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}