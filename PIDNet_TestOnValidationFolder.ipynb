{"cells":[{"cell_type":"markdown","metadata":{"id":"cbIwgK7-uZ0l"},"source":["**WARNING**: Remember to run the `ExtractBoundaries` notebook present in this same folder before running all (or specifically the `Import Boundaries` cell)"]},{"cell_type":"markdown","metadata":{"id":"YwDm553BzPlB"},"source":["# Dataset initialization\n"]},{"cell_type":"code","source":["SAVE_VAL_ON_DRIVE = True # Set to `False` if you already have the drive full with ./Train and .pths\n","SAVE_TRAIN_ON_DRIVE = True\n","DEVICE = 'cuda'"],"metadata":{"id":"KYJFWwKpYNK8","executionInfo":{"status":"ok","timestamp":1738260945254,"user_tz":-60,"elapsed":215,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### Download Data"],"metadata":{"id":"7zeYu_g-bpip"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"cjuW8Yf0zO7K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738260947774,"user_tz":-60,"elapsed":2149,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"outputId":"873443f7-2460-4bd6-f754-7e61582e21fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Validation dataset already in local\n"]}],"source":["from google.colab import drive\n","import os\n","import shutil\n","\n","drive.mount('/content/drive')\n","\n","# Set paths for Validation and Test datasets\n","val_dataset_path = '/content/drive/MyDrive/LoveDA/Val'\n","test_dataset_path = '/content/drive/MyDrive/LoveDA/Test'\n","\n","\n","# Function to handle dataset download and extraction\n","def handle_dataset(dataset_name, zip_url, local_path, drive_path, save_on_drive):\n","    if not os.path.exists(local_path):\n","        if os.path.exists(f\"{drive_path}.zip\"):\n","            print(f\"{dataset_name} dataset available on own drive, unzipping...\")\n","            !unzip -q {drive_path}.zip -d ./\n","        else:\n","            print(f\"Downloading {dataset_name} dataset...\")\n","            !wget -O {dataset_name}.zip \"{zip_url}\"\n","            if save_on_drive:\n","                print(f\"Saving {dataset_name} dataset on drive...\")\n","                !cp {dataset_name}.zip {drive_path}.zip\n","                print(f\"{dataset_name} dataset saved on drive\")\n","            print(f\"Unzipping {dataset_name} dataset...\")\n","            !unzip -q {dataset_name}.zip -d ./\n","    else:\n","        print(f\"{dataset_name} dataset already in local\")\n","\n","# Handle Train dataset\n","#handle_dataset(\"Train\", \"https://zenodo.org/records/5706578/files/Train.zip?download=1\", \"./Train\", \"/content/drive/MyDrive/LoveDA/Train\", SAVE_TRAIN_ON_DRIVE)\n","\n","# Handle Validation dataset => THIS IS ACTUALLY OUR TESTING SET, SINCE ./Test doesn't have labels\n","handle_dataset(\"Validation\", \"https://zenodo.org/records/5706578/files/Val.zip?download=1\", \"./Val\", \"/content/drive/MyDrive/LoveDA/Val\", SAVE_VAL_ON_DRIVE)\n","\n","# Handle Test dataset\n","#handle_dataset(\"Test\", \"https://zenodo.org/records/5706578/files/Test.zip?download=1\", \"./Test\", \"/content/drive/MyDrive/LoveDA/Test\")"]},{"cell_type":"code","source":["# !unzip -q Validation.zip -d ./"],"metadata":{"id":"_vIKj6nzjqAG","executionInfo":{"status":"ok","timestamp":1738260947775,"user_tz":-60,"elapsed":3,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"68BKhuGh2vJH"},"source":["### Dataset Definition"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"iUsQczlZ3Wx0","executionInfo":{"status":"ok","timestamp":1738260950425,"user_tz":-60,"elapsed":2653,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[],"source":["import os\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import torchvision.transforms as T\n","import random\n","import cv2\n","\n","\n","def pil_loader(path, color_type):\n","    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n","    with open(path, 'rb') as f:\n","        img = Image.open(f)\n","        return img.convert(color_type)\n","\n","class LoveDADataset(Dataset):\n","    def __init__(self, baseTransform, augTransforms = None, split = 'Urban', type = 'Train', useBoundaries=True, validation_ratio=0.2, seed=265637):\n","        # Validate type input\n","        if type not in ['Train', 'Validation', 'Total', 'ActualTest']:\n","            raise ValueError(\"Invalid type. Expected 'Train' or 'Validation' or 'Total' or 'ActualTest'.\")\n","        self.directory = []\n","        if type == 'ActualTest':\n","            directory_path = os.path.join('./Val', split, 'images_png')\n","        else:\n","            directory_path = os.path.join('./Train', split, 'images_png')\n","        # Check if the directory exists\n","        if not os.path.exists(directory_path):\n","            raise FileNotFoundError(f\"Directory not found: {directory_path}\")\n","        # Get all image paths\n","        all_images = [os.path.join(directory_path, entry) for entry in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, entry))]\n","        # Shuffle images for random splitting\n","        random.seed(seed)\n","        random.shuffle(all_images)\n","        # Split into training and validation sets\n","        split_idx = int(len(all_images) * (1 - validation_ratio))\n","        if type == 'Train':\n","            self.directory = all_images[:split_idx]\n","        elif type == 'Validation':\n","            self.directory = all_images[split_idx:]\n","        elif type == 'Total':\n","            self.directory = all_images\n","        elif type == 'ActualTest':\n","            self.directory = all_images\n","        else:\n","            raise ValueError(\"Invalid type. Expected 'Train' or 'Validation' or 'Total' or 'ActualTest.\")\n","        self.baseTransforms = baseTransform\n","        self.augTransforms = augTransforms\n","        self.useBoundaries = useBoundaries\n","        self.typeDataset = type\n","        # Print dataset size\n","        print(f\"Dataset size: {len(self.directory)}\")\n","\n","    def __len__(self):\n","        return len(self.directory)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.directory[idx]\n","        image = pil_loader(image_path, 'RGB')\n","        mask_path = image_path.replace('images_png', 'masks_png')\n","        boundaries_path = image_path.replace('images_png', 'boundaries_png')\n","\n","        mask = pil_loader(mask_path, 'L')\n","\n","        if self.useBoundaries:\n","          boundaries = pil_loader(boundaries_path, 'L')\n","        else:\n","          boundaries = mask\n","\n","        base_transformed = self.baseTransforms(image=np.array(image), mask=np.array(mask), boundaries=np.array(boundaries))\n","        base_image = base_transformed['image']\n","        base_mask = base_transformed['mask']\n","        base_boundaries = base_transformed['boundaries']\n","\n","        base_image = T.Compose([T.ToTensor()])(base_image)\n","        base_mask = torch.from_numpy(base_mask).long()\n","        base_mask -= 1\n","        base_boundaries = torch.from_numpy(base_boundaries)\n","\n","        if(self.typeDataset != 'Train'):\n","          return base_image, base_mask, image_path, base_boundaries\n","\n","\n","        if self.augTransforms == None:\n","          return [base_image], [base_mask], image_path, [base_boundaries]\n","        # Apply transformations\n","        augmented = self.augTransforms(image=np.array(image), mask=np.array(mask), boundaries=np.array(boundaries))\n","        augmented_image = T.Compose([T.ToTensor()])(augmented['image'])\n","\n","        mask = augmented['mask']\n","        mask = torch.from_numpy(mask).long()\n","        mask = mask-1\n","        boundaries = augmented['boundaries']\n","        boundaries = torch.from_numpy(boundaries)\n","\n","        image_list = [base_image, augmented_image]\n","        mask_list = [base_mask, mask]\n","        boundaries_list = [base_boundaries, boundaries]\n","\n","        return image_list, mask_list, image_path, boundaries_list"]},{"cell_type":"markdown","source":["### Dataset Utils"],"metadata":{"id":"xSaOfED0-zoc"}},{"cell_type":"code","source":["import matplotlib.patches as mpatches\n","\n","from collections import OrderedDict\n","COLOR_MAP = OrderedDict(\n","    Background=(255, 255, 255),\n","    Building=(255, 0, 0),\n","    Road=(255, 255, 0),\n","    Water=(0, 0, 255),\n","    Barren=(159, 129, 183),\n","    Forest=(34, 139, 34),\n","    Agricultural=(255, 195, 128),\n",")\n","\n","LABEL_MAP = OrderedDict(\n","    Background=0,\n","    Building=1,\n","    Road=2,\n","    Water=3,\n","    Barren=4,\n","    Forest=5,\n","    Agricultural=6,\n",")\n","inverted_label_map = OrderedDict((v, k) for k, v in LABEL_MAP.items())\n","\n","\n","def getLabelColor(label):\n","    # Default color for unclassified labels\n","    default_color = np.array([128, 128, 128])  # Gray\n","\n","    # Check if label exists in inverted_label_map\n","    label_name = inverted_label_map.get(label, None)\n","    if label_name is None or label_name not in COLOR_MAP:\n","        return default_color  # Return default color for unclassified\n","\n","    # Return the mapped color\n","    label_color = np.array(COLOR_MAP[label_name])\n","    return label_color\n","\n","\n","def getLegendHandles():\n","  handles = [mpatches.Patch(color=getLabelColor(i)/255, label=inverted_label_map[i]) for i in range(0, len(LABEL_MAP))]\n","  handles.append(mpatches.Patch(color=getLabelColor(-1)/255, label='Unclassified'))\n","  return handles\n","\n","def new_colors_mask(mask):\n","  new_image = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n","  for i, row in enumerate(mask):\n","    for j, cell in enumerate(row):\n","      new_image[i][j] = getLabelColor(cell.item())\n","  return new_image\n","\n"],"metadata":{"id":"tfi1pG3P-2H1","executionInfo":{"status":"ok","timestamp":1738260950426,"user_tz":-60,"elapsed":7,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Debug"],"metadata":{"id":"IrsSERdV_xZc"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"l5iBVMMn6-3-","executionInfo":{"status":"ok","timestamp":1738260950426,"user_tz":-60,"elapsed":6,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[],"source":["# # Comment this cell to save GPU time\n","\n","# import matplotlib.pyplot as plt\n","# import torch\n","# from torch.utils.data import DataLoader\n","# import matplotlib.patches as mpatches\n","\n","# train_dataset = LoveDADataset(type='Train', seed=222)\n","# print(train_dataset.__len__())\n","\n","# # Get item\n","# image, mask, path, bd = train_dataset.__getitem__(88)\n","\n","# # Show path\n","# print(f\"Image is at {path}\")\n","\n","# # Show image\n","# image = image.permute(1, 2, 0)\n","# image = image.numpy()\n","# plt.imshow(image)\n","\n","# # Show mask\n","# new_image = new_colors_mask(mask)\n","# plt.imshow(image)\n","# plt.show()\n","# plt.legend(handles=getLegendHandles(), loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n","# plt.imshow(new_image)\n","# plt.show()\n","\n","# # Show boundaries\n","# # for row in bd:\n","# #     for col in row:\n","# #         if col != 0 and col != 1:\n","# #             print(col)\n","# bd = bd.numpy()\n","# plt.imshow(bd)\n"]},{"cell_type":"markdown","metadata":{"id":"uZwN55AlCRZc"},"source":["# Initialize model"]},{"cell_type":"markdown","source":["### PIDNet Util Modules"],"metadata":{"id":"V1PldejM1ZCB"}},{"cell_type":"code","source":["# ------------------------------------------------------------------------------\n","# Written by Jiacong Xu (jiacong.xu@tamu.edu)\n","# ------------------------------------------------------------------------------\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","BatchNorm2d = nn.BatchNorm2d\n","bn_mom = 0.1\n","algc = False\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=False):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.no_relu = no_relu\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","\n","        if self.no_relu:\n","            return out\n","        else:\n","            return self.relu(out)\n","\n","class Bottleneck(nn.Module):\n","    expansion = 2\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=True):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n","        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n","                               bias=False)\n","        self.bn3 = BatchNorm2d(planes * self.expansion, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.no_relu = no_relu\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        if self.no_relu:\n","            return out\n","        else:\n","            return self.relu(out)\n","\n","class segmenthead(nn.Module):\n","\n","    def __init__(self, inplanes, interplanes, outplanes, scale_factor=None):\n","        super(segmenthead, self).__init__()\n","        self.bn1 = BatchNorm2d(inplanes, momentum=bn_mom)\n","        self.conv1 = nn.Conv2d(inplanes, interplanes, kernel_size=3, padding=1, bias=False)\n","        self.bn2 = BatchNorm2d(interplanes, momentum=bn_mom)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(interplanes, outplanes, kernel_size=1, padding=0, bias=True)\n","        self.scale_factor = scale_factor\n","\n","    def forward(self, x):\n","\n","        x = self.conv1(self.relu(self.bn1(x)))\n","        out = self.conv2(self.relu(self.bn2(x)))\n","\n","        if self.scale_factor is not None:\n","            height = x.shape[-2] * self.scale_factor\n","            width = x.shape[-1] * self.scale_factor\n","            out = F.interpolate(out,\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)\n","\n","        return out\n","\n","class DAPPM(nn.Module):\n","    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n","        super(DAPPM, self).__init__()\n","        bn_mom = 0.1\n","        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale0 = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.process1 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.process2 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.process3 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.process4 = nn.Sequential(\n","                                    BatchNorm(branch_planes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n","                                    )\n","        self.compression = nn.Sequential(\n","                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n","                                    )\n","        self.shortcut = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n","                                    )\n","\n","    def forward(self, x):\n","        width = x.shape[-1]\n","        height = x.shape[-2]\n","        x_list = []\n","\n","        x_list.append(self.scale0(x))\n","        x_list.append(self.process1((F.interpolate(self.scale1(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[0])))\n","        x_list.append((self.process2((F.interpolate(self.scale2(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[1]))))\n","        x_list.append(self.process3((F.interpolate(self.scale3(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[2])))\n","        x_list.append(self.process4((F.interpolate(self.scale4(x),\n","                        size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_list[3])))\n","\n","        out = self.compression(torch.cat(x_list, 1)) + self.shortcut(x)\n","        return out\n","\n","class PAPPM(nn.Module):\n","    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n","        super(PAPPM, self).__init__()\n","        bn_mom = 0.1\n","        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","\n","        self.scale0 = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n","                                    )\n","\n","        self.scale_process = nn.Sequential(\n","                                    BatchNorm(branch_planes*4, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes*4, branch_planes*4, kernel_size=3, padding=1, groups=4, bias=False),\n","                                    )\n","\n","\n","        self.compression = nn.Sequential(\n","                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n","                                    )\n","\n","        self.shortcut = nn.Sequential(\n","                                    BatchNorm(inplanes, momentum=bn_mom),\n","                                    nn.ReLU(inplace=True),\n","                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n","                                    )\n","\n","\n","    def forward(self, x):\n","        width = x.shape[-1]\n","        height = x.shape[-2]\n","        scale_list = []\n","\n","        x_ = self.scale0(x)\n","        scale_list.append(F.interpolate(self.scale1(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","        scale_list.append(F.interpolate(self.scale2(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","        scale_list.append(F.interpolate(self.scale3(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","        scale_list.append(F.interpolate(self.scale4(x), size=[height, width],\n","                        mode='bilinear', align_corners=algc)+x_)\n","\n","        scale_out = self.scale_process(torch.cat(scale_list, 1))\n","\n","        out = self.compression(torch.cat([x_,scale_out], 1)) + self.shortcut(x)\n","        return out\n","\n","\n","class PagFM(nn.Module):\n","    def __init__(self, in_channels, mid_channels, after_relu=False, with_channel=False, BatchNorm=nn.BatchNorm2d):\n","        super(PagFM, self).__init__()\n","        self.with_channel = with_channel\n","        self.after_relu = after_relu\n","        self.f_x = nn.Sequential(\n","                                nn.Conv2d(in_channels, mid_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(mid_channels)\n","                                )\n","        self.f_y = nn.Sequential(\n","                                nn.Conv2d(in_channels, mid_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(mid_channels)\n","                                )\n","        if with_channel:\n","            self.up = nn.Sequential(\n","                                    nn.Conv2d(mid_channels, in_channels,\n","                                              kernel_size=1, bias=False),\n","                                    BatchNorm(in_channels)\n","                                   )\n","        if after_relu:\n","            self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x, y):\n","        input_size = x.size()\n","        if self.after_relu:\n","            y = self.relu(y)\n","            x = self.relu(x)\n","\n","        y_q = self.f_y(y)\n","        y_q = F.interpolate(y_q, size=[input_size[2], input_size[3]],\n","                            mode='bilinear', align_corners=False)\n","        x_k = self.f_x(x)\n","\n","        if self.with_channel:\n","            sim_map = torch.sigmoid(self.up(x_k * y_q))\n","        else:\n","            sim_map = torch.sigmoid(torch.sum(x_k * y_q, dim=1).unsqueeze(1))\n","\n","        y = F.interpolate(y, size=[input_size[2], input_size[3]],\n","                            mode='bilinear', align_corners=False)\n","        x = (1-sim_map)*x + sim_map*y\n","\n","        return x\n","\n","class Light_Bag(nn.Module):\n","    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n","        super(Light_Bag, self).__init__()\n","        self.conv_p = nn.Sequential(\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","        self.conv_i = nn.Sequential(\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","\n","    def forward(self, p, i, d):\n","        edge_att = torch.sigmoid(d)\n","\n","        p_add = self.conv_p((1-edge_att)*i + p)\n","        i_add = self.conv_i(i + edge_att*p)\n","\n","        return p_add + i_add\n","\n","\n","class DDFMv2(nn.Module):\n","    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n","        super(DDFMv2, self).__init__()\n","        self.conv_p = nn.Sequential(\n","                                BatchNorm(in_channels),\n","                                nn.ReLU(inplace=True),\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","        self.conv_i = nn.Sequential(\n","                                BatchNorm(in_channels),\n","                                nn.ReLU(inplace=True),\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=1, bias=False),\n","                                BatchNorm(out_channels)\n","                                )\n","\n","    def forward(self, p, i, d):\n","        edge_att = torch.sigmoid(d)\n","\n","        p_add = self.conv_p((1-edge_att)*i + p)\n","        i_add = self.conv_i(i + edge_att*p)\n","\n","        return p_add + i_add\n","\n","class Bag(nn.Module):\n","    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n","        super(Bag, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","                                BatchNorm(in_channels),\n","                                nn.ReLU(inplace=True),\n","                                nn.Conv2d(in_channels, out_channels,\n","                                          kernel_size=3, padding=1, bias=False)\n","                                )\n","\n","\n","    def forward(self, p, i, d):\n","        edge_att = torch.sigmoid(d)\n","        return self.conv(edge_att*p + (1-edge_att)*i)\n","\n"],"metadata":{"id":"8lOLZdcA1W04","executionInfo":{"status":"ok","timestamp":1738260950426,"user_tz":-60,"elapsed":6,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### PIDNet Definition"],"metadata":{"id":"uiBwR_Yg1dVO"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"dcHkfgpmCUys","executionInfo":{"status":"ok","timestamp":1738260950426,"user_tz":-60,"elapsed":5,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[],"source":["# ------------------------------------------------------------------------------\n","# Written by Jiacong Xu (jiacong.xu@tamu.edu)\n","# ------------------------------------------------------------------------------\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","import logging\n","\n","BatchNorm2d = nn.BatchNorm2d\n","bn_mom = 0.1\n","algc = False\n","\n","INPUT_SIZE = (512, 512)\n","\n","class PIDNet(nn.Module):\n","\n","    def __init__(self, m=2, n=3, num_classes=19, planes=64, ppm_planes=96, head_planes=128, augment=True):\n","        super(PIDNet, self).__init__()\n","        self.augment = augment\n","\n","        # I Branch\n","        self.conv1 =  nn.Sequential(\n","                          nn.Conv2d(3,planes,kernel_size=3, stride=2, padding=1),\n","                          BatchNorm2d(planes, momentum=bn_mom),\n","                          nn.ReLU(inplace=True),\n","                          nn.Conv2d(planes,planes,kernel_size=3, stride=2, padding=1),\n","                          BatchNorm2d(planes, momentum=bn_mom),\n","                          nn.ReLU(inplace=True),\n","                      )\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        self.layer1 = self._make_layer(BasicBlock, planes, planes, m)\n","        self.layer2 = self._make_layer(BasicBlock, planes, planes * 2, m, stride=2)\n","        self.layer3 = self._make_layer(BasicBlock, planes * 2, planes * 4, n, stride=2)\n","        self.layer4 = self._make_layer(BasicBlock, planes * 4, planes * 8, n, stride=2)\n","        self.layer5 =  self._make_layer(Bottleneck, planes * 8, planes * 8, 2, stride=2)\n","\n","        # P Branch\n","        self.compression3 = nn.Sequential(\n","                                          nn.Conv2d(planes * 4, planes * 2, kernel_size=1, bias=False),\n","                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                          )\n","\n","        self.compression4 = nn.Sequential(\n","                                          nn.Conv2d(planes * 8, planes * 2, kernel_size=1, bias=False),\n","                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                          )\n","        self.pag3 = PagFM(planes * 2, planes)\n","        self.pag4 = PagFM(planes * 2, planes)\n","\n","        self.layer3_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n","        self.layer4_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n","        self.layer5_ = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n","\n","        # D Branch\n","        if m == 2:\n","            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes)\n","            self.layer4_d = self._make_layer(Bottleneck, planes, planes, 1)\n","            self.diff3 = nn.Sequential(\n","                                        nn.Conv2d(planes * 4, planes, kernel_size=3, padding=1, bias=False),\n","                                        BatchNorm2d(planes, momentum=bn_mom),\n","                                        )\n","            self.diff4 = nn.Sequential(\n","                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n","                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                     )\n","            self.spp = PAPPM(planes * 16, ppm_planes, planes * 4)\n","            self.dfm = Light_Bag(planes * 4, planes * 4)\n","        else:\n","            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n","            self.layer4_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n","            self.diff3 = nn.Sequential(\n","                                        nn.Conv2d(planes * 4, planes * 2, kernel_size=3, padding=1, bias=False),\n","                                        BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                        )\n","            self.diff4 = nn.Sequential(\n","                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n","                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n","                                     )\n","            self.spp = DAPPM(planes * 16, ppm_planes, planes * 4)\n","            self.dfm = Bag(planes * 4, planes * 4)\n","\n","        self.layer5_d = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n","\n","        # Prediction Head\n","        if self.augment:\n","            self.seghead_p = segmenthead(planes * 2, head_planes, num_classes)\n","            self.seghead_d = segmenthead(planes * 2, planes, 1)\n","\n","        self.final_layer = segmenthead(planes * 4, head_planes, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","\n","    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n","            )\n","\n","        layers = []\n","        layers.append(block(inplanes, planes, stride, downsample))\n","        inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            if i == (blocks-1):\n","                layers.append(block(inplanes, planes, stride=1, no_relu=True))\n","            else:\n","                layers.append(block(inplanes, planes, stride=1, no_relu=False))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _make_single_layer(self, block, inplanes, planes, stride=1):\n","        downsample = None\n","        if stride != 1 or inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n","            )\n","\n","        layer = block(inplanes, planes, stride, downsample, no_relu=True)\n","\n","        return layer\n","\n","    def forward(self, x):\n","\n","        width_output = x.shape[-1] // 8\n","        height_output = x.shape[-2] // 8\n","\n","        h, w = x.size(2), x.size(3)\n","\n","        x = self.conv1(x)\n","        x = self.layer1(x)\n","        x = self.relu(self.layer2(self.relu(x)))\n","        x_ = self.layer3_(x)\n","        x_d = self.layer3_d(x)\n","\n","        x = self.relu(self.layer3(x))\n","        x_ = self.pag3(x_, self.compression3(x))\n","        x_d = x_d + F.interpolate(\n","                        self.diff3(x),\n","                        size=[height_output, width_output],\n","                        mode='bilinear', align_corners=algc)\n","        if self.augment:\n","            temp_p = x_\n","\n","        x = self.relu(self.layer4(x))\n","        x_ = self.layer4_(self.relu(x_))\n","        x_d = self.layer4_d(self.relu(x_d))\n","\n","        x_ = self.pag4(x_, self.compression4(x))\n","        x_d = x_d + F.interpolate(\n","                        self.diff4(x),\n","                        size=[height_output, width_output],\n","                        mode='bilinear', align_corners=algc)\n","        if self.augment:\n","            temp_d = x_d\n","\n","        x_ = self.layer5_(self.relu(x_))\n","        x_d = self.layer5_d(self.relu(x_d))\n","        x = F.interpolate(\n","                        self.spp(self.layer5(x)),\n","                        size=[height_output, width_output],\n","                        mode='bilinear', align_corners=algc)\n","\n","        x_ = self.final_layer(self.dfm(x_, x, x_d))\n","\n","        if self.augment:\n","            x_extra_p = self.seghead_p(temp_p)\n","            x_extra_d = self.seghead_d(temp_d)\n","            return [x_extra_p, x_, x_extra_d]\n","        else:\n","            return x_\n","\n","def get_seg_model(cfg, imgnet_pretrained):\n","\n","    if 's' in cfg.MODEL.NAME:\n","        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=32, ppm_planes=96, head_planes=128, augment=True)\n","    elif 'm' in cfg.MODEL.NAME:\n","        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=96, head_planes=128, augment=True)\n","    else:\n","        model = PIDNet(m=3, n=4, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=112, head_planes=256, augment=True)\n","\n","    if imgnet_pretrained:\n","        pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n","        model_dict = model.state_dict()\n","        pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n","        model_dict.update(pretrained_state)\n","        msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n","        logging.info('Attention!!!')\n","        logging.info(msg)\n","        logging.info('Over!!!')\n","        model.load_state_dict(model_dict, strict = False)\n","    else:\n","        pretrained_dict = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')\n","        if 'state_dict' in pretrained_dict:\n","            pretrained_dict = pretrained_dict['state_dict']\n","        model_dict = model.state_dict()\n","        pretrained_dict = {k[6:]: v for k, v in pretrained_dict.items() if (k[6:] in model_dict and v.shape == model_dict[k[6:]].shape)}\n","        msg = 'Loaded {} parameters!'.format(len(pretrained_dict))\n","        logging.info('Attention!!!')\n","        logging.info(msg)\n","        logging.info('Over!!!')\n","        model_dict.update(pretrained_dict)\n","        model.load_state_dict(model_dict, strict = False)\n","\n","    return model\n","\n","def get_pred_model(name, num_classes):\n","\n","    if 's' in name:\n","        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=False)\n","    elif 'm' in name:\n","        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=False)\n","    else:\n","        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=False)\n","\n","    return model"]},{"cell_type":"code","source":["# Remember to upsample the input x before running it through this, as the paper says\n","import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, num_classes):\n","        super(Discriminator, self).__init__()\n","        self.domain_classifier = nn.Sequential(\n","            nn.Conv2d(num_classes, 64, kernel_size=4, stride=2, padding=1),  # Conv1\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # Conv2\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # Conv3\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),  # Conv4\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(512, 1, kernel_size=4, stride=2, padding=1),  # Conv5\n","        )\n","\n","    def forward(self, x):\n","        return self.domain_classifier(x)\n","\n","# Initialize the model with Kaiming initialization\n","def initialize_weights_kaiming(m):\n","    if isinstance(m, nn.Conv2d):\n","        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n","        if m.bias is not None:\n","            init.zeros_(m.bias)"],"metadata":{"id":"XGBkU3KzzTdK","executionInfo":{"status":"ok","timestamp":1738260950427,"user_tz":-60,"elapsed":6,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Load PIDNet Model"],"metadata":{"id":"isRcFkZ3CxpI"}},{"cell_type":"code","source":["import gdown\n","import tarfile\n","\n","if (os.path.exists(\"./PIDNet_S_ImageNet.pth.tar\") == False):\n","  url = \"https://drive.google.com/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-\"\n","  output = \"./\"\n","  gdown.download(url, output, quiet=False)\n","# Then keep as tar, as it's already the correct format to feed the model\n","\n","# Create a config object with required parameters\n","class Config:\n","    class MODEL:\n","        NAME = 'pidnet_s'  # or 'pidnet_m' or 'pidnet_l'\n","        PRETRAINED = 'PIDNet_S_ImageNet.pth.tar'\n","    class DATASET:\n","        NUM_CLASSES = len(LABEL_MAP)\n","\n","cfg = Config()\n","\n","model = get_seg_model(cfg, imgnet_pretrained=True)\n","# model = get_pred_model('s', len(LABEL_MAP))\n"],"metadata":{"id":"ujP_2PffskRk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738260951231,"user_tz":-60,"elapsed":809,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"outputId":"f109cfab-ca1e-4815-fc09-1c7527db04b1"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-c45f6e9ccf7a>:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n"]}]},{"cell_type":"code","source":["# import torch.nn.functional as F\n","# from torch.utils.data import DataLoader\n","# import matplotlib.pyplot as plt\n","\n","# train_dataset = LoveDADataset(type='Train')\n","# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n","\n","# model = model.train()\n","# model = model.to('cuda')\n","\n","# for img, mask, _ in train_loader:\n","#     print(f\"iamge shape: {img.shape}\")\n","#     print(f\"mask shape: {mask.shape}\")\n","\n","#     img = img.to('cuda')\n","#     outputs = model(img)\n","\n","#     # bilinear interpolation\n","#     h, w = mask.size(1), mask.size(2)\n","#     ph, pw = outputs[0].size(2), outputs[0].size(3)\n","#     if ph != h or pw != w:\n","#         for i in range(len(outputs)):\n","#             outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear',\n","#                                        align_corners=True)\n","\n","#     for output in outputs:\n","#       print(output.shape)\n","#     break\n","\n","# print(\"===================== Original Image =====================\")\n","# plt.imshow(img[0].permute(1, 2, 0).cpu().numpy())\n","# plt.show()\n","\n","# print(\"===================== Ground Truth =====================\")\n","# plt.imshow(mask[0].cpu().numpy())\n","# plt.show()\n","\n","# print(\"===================== Predicted Mask =====================\")\n","# plt.imshow(torch.argmax(outputs[0][0], dim=0).cpu().numpy())\n","# plt.show()"],"metadata":{"id":"xkTB43MxcDRA","executionInfo":{"status":"ok","timestamp":1738260951231,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3DwQcmOWpUxn"},"source":["# Training & Dataset creation"]},{"cell_type":"markdown","source":["### Setup, Create Datasets and DataLoaders. With annexed transforms."],"metadata":{"id":"IwDwW9Hac6SS"}},{"cell_type":"markdown","source":["# TEST"],"metadata":{"id":"soROYpcYyajp"}},{"cell_type":"code","source":["DEVICE = 'cuda' # 'cuda' or 'cpu'\n","TYPE = 'Test'\n","\n","TEST_ONLY_ON_BEST = False # Leave False unless you're really picky/in need of time\n","TEST_MODELS_FROM_MYDRIVE_TOO = True\n","INTEREST = \"best_DA_model\"\n","\n","RESIZE = 512"],"metadata":{"id":"0hcHUuFXaVZV","executionInfo":{"status":"ok","timestamp":1738260951232,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### Copy Some Models From MyDrive/LoveDA"],"metadata":{"id":"sOvtVA095I7b"}},{"cell_type":"code","source":["# Run the first cell pls\n","import os\n","\n","BASE_PATH = \"/content/drive/MyDrive/LoveDA/\"\n","\n","if TEST_MODELS_FROM_MYDRIVE_TOO:\n","  all_mydrive = os.listdir(BASE_PATH)\n","  all_models = [f for f in all_mydrive if f.endswith('.pth')]\n","  models_of_interest = [f for f in all_models if INTEREST in f]\n","  for model_name in models_of_interest:\n","      if not os.path.exists(model_name):\n","        print(f\"Copying {model_name} locally from MyDrive\")\n","        !cp {BASE_PATH+model_name} ."],"metadata":{"id":"BS2GA3Os5IzP","executionInfo":{"status":"ok","timestamp":1738260951232,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### Define Transforms In Case"],"metadata":{"id":"bWAbBMMKmj0g"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","from albumentations import Compose, HorizontalFlip, RandomRotate90, RandomScale, RandomCrop, GaussNoise, Rotate, Resize, OneOf, Normalize, ColorJitter, GaussianBlur\n","from albumentations.pytorch import ToTensorV2\n","\n","#How big should be the image that we feed to the model?\n","RESIZE = 512\n","# DEFINE TRANSFORMATIONS HERE\n","# To Tensor is not needed since its performed inside the getitem\n","\n","\n","AUGMENTATIONS = {\n","    'Resize': Compose([\n","            Resize(RESIZE, RESIZE),\n","    ], additional_targets={\"boundaries\": \"mask\"}),\n","    'None' : Compose([\n","            ], additional_targets={\"boundaries\": \"mask\"})\n","}"],"metadata":{"id":"J8DhNU7Lc7aD","executionInfo":{"status":"ok","timestamp":1738260951677,"user_tz":-60,"elapsed":448,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fdf59f21-d3ea-4e60-f284-caab24024bef"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","  check_for_updates()\n"]}]},{"cell_type":"markdown","source":["### Actually Test"],"metadata":{"id":"ha8NDG9xmn6H"}},{"cell_type":"code","source":["!pip install torchmetrics ptflops"],"metadata":{"id":"7DKgjGPKydLH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738260953905,"user_tz":-60,"elapsed":2231,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"outputId":"2d1344ed-2de1-40c9-e0fc-fc708f76af26"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: ptflops in /usr/local/lib/python3.11/dist-packages (0.7.4)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu124)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.11.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"]}]},{"cell_type":"code","source":["from torchmetrics import Accuracy\n","from tqdm import tqdm\n","import time\n","import ptflops\n","import os\n","import pandas as pd\n","\n","test_augmentation = AUGMENTATIONS['None']\n","# target_type = 'ActualTest'\n","NUM_CLASSES = len(LABEL_MAP)\n","DEVICE = 'cuda'\n","\n","# Create unweighted models\n","model = get_seg_model(cfg, imgnet_pretrained=False)\n","\n","model_files_paths = [f for f in os.listdir('.') if f.endswith('.pth')]\n","print(model_files_paths)\n","\n","for model_file_path in model_files_paths:\n","    best_model = torch.load(model_file_path, weights_only=True)\n","\n","    model.load_state_dict(best_model)\n","    model = model.to(DEVICE)\n","\n","    accuracy, mIoU = False, True\n","    iou_data = []\n","\n","\n","    TARGETs = ['Urban', 'Rural']\n","    for TARGET in TARGETs:\n","        if TARGET == 'Urban':\n","            target_type = 'ActualTest'\n","            #target_type = 'Validation'\n","        elif TARGET == 'Rural':\n","            target_type = 'ActualTest'\n","            #target_type = 'Total'\n","\n","        test_dataset = LoveDADataset(baseTransform=test_augmentation, augTransforms=None, split=TARGET, type=target_type, useBoundaries=False)\n","        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, drop_last=True, pin_memory=True)\n","\n","        #### TEST LOOP\n","        model.eval()\n","        print(f\"Testing model={model_file_path} on domain={TARGET} on a {target_type} split\")\n","\n","        # # Latency\n","        # with torch.no_grad():\n","        #     start_time = time.time()\n","        #     for _ in range(100):\n","        #         _ = model(torch.randn(1, 3, RESIZE, RESIZE).to(DEVICE))\n","        #     end_time = time.time()\n","        # latency = (end_time - start_time) / 100\n","        # print(f\"Latency: {latency:.4f} seconds\")\n","\n","        # # FLOPs\n","        # macs, _ = ptflops.get_model_complexity_info(model,\n","        #     (3, RESIZE, RESIZE), as_strings=False,\n","        #     print_per_layer_stat=False, verbose=False)\n","        # flops = macs * 2  # MACs perform two FLOPs\n","        # print(\"FLOPs:\", flops)\n","\n","        # # Number of parameters\n","        # total_params = sum(p.numel() for p in model.parameters())\n","        # print(f\"Total number of parameters: {total_params}\")\n","\n","        if TYPE == 'Test':\n","            with torch.no_grad():\n","                total_union = torch.zeros(NUM_CLASSES).to(DEVICE)\n","                total_intersection = torch.zeros(NUM_CLASSES).to(DEVICE)\n","                if accuracy:\n","                  meter = Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(DEVICE)\n","                for (i, batch) in enumerate(tqdm(test_loader)):\n","                    ### Extract input\n","                    images, masks, img_path, bd_gts = batch\n","                    images = images.float().to(DEVICE)\n","                    masks = masks.to(DEVICE)\n","\n","                    #Printing size, testing on original image size\n","                    if i == 0:\n","                        print(\"\")\n","                        print(\"===============================================\")\n","                        print(f\"Image shape: {images.shape}\")\n","                        print(f\"Mask shape: {masks.shape}\")\n","                        print(\"===============================================\")\n","\n","                    # Downscale images batch\n","                    images = F.interpolate(images, size=(RESIZE, RESIZE), mode='bilinear')\n","\n","                    ### ===> Forward, Upscale, Compute Losses\n","                    ## Forward\n","                    outputs = model(images)\n","\n","                    ## Upscale (bilinear interpolation to original size)\n","                    h, w = masks.size(1), masks.size(2)\n","                    ph, pw = outputs[0].size(2), outputs[0].size(3)\n","                    if ph != h or pw != w:\n","                        for i in range(len(outputs)):\n","                            outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n","\n","                    # Output 1 is the prediction\n","\n","                    # Shape: NBATCHES x classes x h x w\n","                    class_indices = torch.argmax(outputs[1], dim=1)  # Shape: NBATCHES x h x w\n","\n","                    if accuracy:\n","                    # Create a mask for valid targets (where target is not -1)\n","                        valid_mask = (masks != -1)  # Mask of shape: NBATCHES x h x w\n","                        # Apply the mask to ignore -1 targets when updating the accuracy metric\n","                        meter.update(class_indices[valid_mask], masks[valid_mask])\n","\n","                    if mIoU:\n","                        for predicted, target in zip(class_indices, masks): # Iterating image for image\n","                            for i in range(NUM_CLASSES):\n","                                total_intersection[i] += torch.sum(torch.logical_and(predicted == i, target == i))\n","                                total_union[i] += torch.sum(torch.logical_or(torch.logical_and(predicted == i, target != -1), target == i))\n","\n","        if accuracy:\n","            accuracy = meter.compute()\n","            print(f'\\nAccuracy on the target domain: {100 * accuracy:.2f}%')\n","\n","        if mIoU:\n","            intersection_over_union = total_intersection / total_union\n","\n","            # Per class IoU\n","            for i, iou in enumerate(intersection_over_union):\n","                class_name = list(LABEL_MAP.keys())[list(LABEL_MAP.values()).index(i)]  # Get the class name from LABEL_MAP\n","                iou_data.append({f'Class Name': class_name, 'IoU': iou.item()})\n","                print(f'{class_name} IoU: {iou:.4f}')\n","\n","            mIoU = torch.mean(intersection_over_union)\n","            iou_data.append({f'Class Name': f'Mean IoU {TARGET}', 'IoU': mIoU.cpu().numpy()})\n","            print(f'\\nmIoU on the {TARGET} domain: {mIoU}')\n","\n","        print(\"========================================================================\")\n","\n","    # Create a pandas DataFrame\n","    iou_df = pd.DataFrame(iou_data)\n","\n","    # Optionally, save the DataFrame to a CSV file\n","    iou_df.to_csv(f'iou_statistics_for{model_file_path}.csv', index=False, float_format=f'%4f', )\n"],"metadata":{"id":"M3c7DFDRymnI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"be2e4e1f-811a-4238-fafb-c9a682ea5c1d","executionInfo":{"status":"ok","timestamp":1738261085993,"user_tz":-60,"elapsed":132090,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-c45f6e9ccf7a>:204: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  pretrained_dict = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')\n"]},{"output_type":"stream","name":"stdout","text":["['best_DA_model_LA=0.0001_LD=0.1_LRdisc=0.0005_usespike=False_s=0_spid=0__bestmIoUUrban.pth', 'best_DA_model_LA=0.0001_LD=0.1_LRdisc=0.0005_usespike=False_s=0_spid=0.pth_20.pth']\n","Dataset size: 677\n","Testing model=best_DA_model_LA=0.0001_LD=0.1_LRdisc=0.0005_usespike=False_s=0_spid=0__bestmIoUUrban.pth on domain=Urban on a ActualTest split\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/21 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","===============================================\n","Image shape: torch.Size([32, 3, 1024, 1024])\n","Mask shape: torch.Size([32, 1024, 1024])\n","===============================================\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 21/21 [00:27<00:00,  1.32s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Background IoU: 0.3236\n","Building IoU: 0.4383\n","Road IoU: 0.5452\n","Water IoU: 0.6357\n","Barren IoU: 0.2448\n","Forest IoU: 0.4628\n","Agricultural IoU: 0.1643\n","\n","mIoU on the Urban domain: 0.4020940363407135\n","========================================================================\n","Dataset size: 992\n","Testing model=best_DA_model_LA=0.0001_LD=0.1_LRdisc=0.0005_usespike=False_s=0_spid=0__bestmIoUUrban.pth on domain=Rural on a ActualTest split\n"]},{"output_type":"stream","name":"stderr","text":["  3%|▎         | 1/31 [00:03<01:41,  3.37s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","===============================================\n","Image shape: torch.Size([32, 3, 1024, 1024])\n","Mask shape: torch.Size([32, 1024, 1024])\n","===============================================\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 31/31 [00:37<00:00,  1.20s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Background IoU: 0.4648\n","Building IoU: 0.2105\n","Road IoU: 0.1701\n","Water IoU: 0.4283\n","Barren IoU: 0.1126\n","Forest IoU: 0.1614\n","Agricultural IoU: 0.1236\n","\n","mIoU on the Rural domain: 0.23875072598457336\n","========================================================================\n","Dataset size: 677\n","Testing model=best_DA_model_LA=0.0001_LD=0.1_LRdisc=0.0005_usespike=False_s=0_spid=0.pth_20.pth on domain=Urban on a ActualTest split\n"]},{"output_type":"stream","name":"stderr","text":["  5%|▍         | 1/21 [00:03<01:09,  3.50s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","===============================================\n","Image shape: torch.Size([32, 3, 1024, 1024])\n","Mask shape: torch.Size([32, 1024, 1024])\n","===============================================\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 21/21 [00:27<00:00,  1.30s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Background IoU: 0.3325\n","Building IoU: 0.4747\n","Road IoU: 0.4969\n","Water IoU: 0.6281\n","Barren IoU: 0.0917\n","Forest IoU: 0.4692\n","Agricultural IoU: 0.1326\n","\n","mIoU on the Urban domain: 0.3750855326652527\n","========================================================================\n","Dataset size: 992\n","Testing model=best_DA_model_LA=0.0001_LD=0.1_LRdisc=0.0005_usespike=False_s=0_spid=0.pth_20.pth on domain=Rural on a ActualTest split\n"]},{"output_type":"stream","name":"stderr","text":["  3%|▎         | 1/31 [00:03<01:41,  3.39s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","===============================================\n","Image shape: torch.Size([32, 3, 1024, 1024])\n","Mask shape: torch.Size([32, 1024, 1024])\n","===============================================\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 31/31 [00:37<00:00,  1.20s/it]"]},{"output_type":"stream","name":"stdout","text":["Background IoU: 0.5052\n","Building IoU: 0.2442\n","Road IoU: 0.1819\n","Water IoU: 0.4140\n","Barren IoU: 0.0347\n","Forest IoU: 0.0372\n","Agricultural IoU: 0.2286\n","\n","mIoU on the Rural domain: 0.23512642085552216\n","========================================================================\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# from google.colab import runtime\n","# runtime.unassign()"],"metadata":{"id":"1NS4WgpdTUmM","executionInfo":{"status":"ok","timestamp":1738261085994,"user_tz":-60,"elapsed":21,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Copy all file with extension .csv (results) on Drive\n","\n","import shutil\n","import glob\n","\n","# Source directory containing .csv files\n","source_dir = \"/content\"\n","# Destination directory on Drive\n","dest_dir = \"/content/drive/MyDrive/LoveDA/Results/\"\n","\n","# Find all .csv files in the source directory\n","csv_files = glob.glob(f\"{source_dir}/*.csv\")\n","\n","# Copy each .csv file to the destination directory\n","for file in csv_files:\n","    shutil.copy(file, dest_dir)\n","\n","print(f\"Copied {len(csv_files)} .csv files to {dest_dir}\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fgql4ATZXCJl","executionInfo":{"status":"ok","timestamp":1738261085994,"user_tz":-60,"elapsed":18,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"outputId":"ffa33d70-41cf-4e32-b108-6c8aa15513f9"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Copied 2 .csv files to /content/drive/MyDrive/LoveDA/Results/\n"]}]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["7zeYu_g-bpip","68BKhuGh2vJH","xSaOfED0-zoc","IrsSERdV_xZc","V1PldejM1ZCB","uiBwR_Yg1dVO","isRcFkZ3CxpI"],"gpuType":"L4","machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}