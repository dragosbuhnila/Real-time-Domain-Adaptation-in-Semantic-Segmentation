{"cells":[{"cell_type":"markdown","metadata":{"id":"cbIwgK7-uZ0l"},"source":["**WARNING**: Remember to run the `ExtractBoundaries` notebook present in this same folder before running all (or specifically the `Import Boundaries` cell)"]},{"cell_type":"markdown","metadata":{"id":"YwDm553BzPlB"},"source":["# Dataset initialization\n"]},{"cell_type":"code","source":["SAVE_VAL_ON_DRIVE = True # Set to `False` if you already have the drive full with ./Train and .pths\n","SAVE_TRAIN_ON_DRIVE = True\n","DEVICE = 'cuda'"],"metadata":{"id":"KYJFWwKpYNK8","executionInfo":{"status":"ok","timestamp":1738259029348,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### Download Data"],"metadata":{"id":"7zeYu_g-bpip"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"cjuW8Yf0zO7K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738259032465,"user_tz":-60,"elapsed":2530,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"outputId":"7322b01b-9bb7-4e5b-f94b-759ab8a1a394"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Validation dataset already in local\n"]}],"source":["from google.colab import drive\n","import os\n","import shutil\n","\n","drive.mount('/content/drive')\n","\n","# Set paths for Validation and Test datasets\n","val_dataset_path = '/content/drive/MyDrive/LoveDA/Val'\n","test_dataset_path = '/content/drive/MyDrive/LoveDA/Test'\n","\n","\n","# Function to handle dataset download and extraction\n","def handle_dataset(dataset_name, zip_url, local_path, drive_path, save_on_drive):\n","    if not os.path.exists(local_path):\n","        if os.path.exists(f\"{drive_path}.zip\"):\n","            print(f\"{dataset_name} dataset available on own drive, unzipping...\")\n","            !unzip -q {drive_path}.zip -d ./\n","        else:\n","            print(f\"Downloading {dataset_name} dataset...\")\n","            !wget -O {dataset_name}.zip \"{zip_url}\"\n","            if save_on_drive:\n","                print(f\"Saving {dataset_name} dataset on drive...\")\n","                !cp {dataset_name}.zip {drive_path}.zip\n","                print(f\"{dataset_name} dataset saved on drive\")\n","            print(f\"Unzipping {dataset_name} dataset...\")\n","            !unzip -q {dataset_name}.zip -d ./\n","    else:\n","        print(f\"{dataset_name} dataset already in local\")\n","\n","# Handle Train dataset\n","#handle_dataset(\"Train\", \"https://zenodo.org/records/5706578/files/Train.zip?download=1\", \"./Train\", \"/content/drive/MyDrive/LoveDA/Train\", SAVE_TRAIN_ON_DRIVE)\n","\n","# Handle Validation dataset => THIS IS ACTUALLY OUR TESTING SET, SINCE ./Test doesn't have labels\n","handle_dataset(\"Validation\", \"https://zenodo.org/records/5706578/files/Val.zip?download=1\", \"./Val\", \"/content/drive/MyDrive/LoveDA/Val\", SAVE_VAL_ON_DRIVE)\n","\n","# Handle Test dataset\n","#handle_dataset(\"Test\", \"https://zenodo.org/records/5706578/files/Test.zip?download=1\", \"./Test\", \"/content/drive/MyDrive/LoveDA/Test\")"]},{"cell_type":"code","source":["# !unzip -q Validation.zip -d ./"],"metadata":{"id":"_vIKj6nzjqAG","executionInfo":{"status":"ok","timestamp":1738259032466,"user_tz":-60,"elapsed":5,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"68BKhuGh2vJH"},"source":["### Dataset Definition"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"iUsQczlZ3Wx0","executionInfo":{"status":"ok","timestamp":1738259034660,"user_tz":-60,"elapsed":2198,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[],"source":["import os\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import torchvision.transforms as T\n","import random\n","import cv2\n","\n","\n","def pil_loader(path, color_type):\n","    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n","    with open(path, 'rb') as f:\n","        img = Image.open(f)\n","        return img.convert(color_type)\n","\n","class LoveDADataset(Dataset):\n","    def __init__(self, baseTransform, augTransforms = None, split = 'Urban', type = 'Train', useBoundaries=True, validation_ratio=0.2, seed=265637):\n","        # Validate type input\n","        if type not in ['Train', 'Validation', 'Total', 'ActualTest']:\n","            raise ValueError(\"Invalid type. Expected 'Train' or 'Validation' or 'Total' or 'ActualTest'.\")\n","        self.directory = []\n","        if type == 'ActualTest':\n","            directory_path = os.path.join('./Val', split, 'images_png')\n","        else:\n","            directory_path = os.path.join('./Train', split, 'images_png')\n","        # Check if the directory exists\n","        if not os.path.exists(directory_path):\n","            raise FileNotFoundError(f\"Directory not found: {directory_path}\")\n","        # Get all image paths\n","        all_images = [os.path.join(directory_path, entry) for entry in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, entry))]\n","        # Shuffle images for random splitting\n","        random.seed(seed)\n","        random.shuffle(all_images)\n","        # Split into training and validation sets\n","        split_idx = int(len(all_images) * (1 - validation_ratio))\n","        if type == 'Train':\n","            self.directory = all_images[:split_idx]\n","        elif type == 'Validation':\n","            self.directory = all_images[split_idx:]\n","        elif type == 'Total':\n","            self.directory = all_images\n","        elif type == 'ActualTest':\n","            self.directory = all_images\n","        else:\n","            raise ValueError(\"Invalid type. Expected 'Train' or 'Validation' or 'Total' or 'ActualTest.\")\n","        self.baseTransforms = baseTransform\n","        self.augTransforms = augTransforms\n","        self.useBoundaries = useBoundaries\n","        self.typeDataset = type\n","        # Print dataset size\n","        print(f\"Dataset size: {len(self.directory)}\")\n","\n","    def __len__(self):\n","        return len(self.directory)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.directory[idx]\n","        image = pil_loader(image_path, 'RGB')\n","        mask_path = image_path.replace('images_png', 'masks_png')\n","        boundaries_path = image_path.replace('images_png', 'boundaries_png')\n","\n","        mask = pil_loader(mask_path, 'L')\n","\n","        if self.useBoundaries:\n","          boundaries = pil_loader(boundaries_path, 'L')\n","        else:\n","          boundaries = mask\n","\n","        base_transformed = self.baseTransforms(image=np.array(image), mask=np.array(mask), boundaries=np.array(boundaries))\n","        base_image = base_transformed['image']\n","        base_mask = base_transformed['mask']\n","        base_boundaries = base_transformed['boundaries']\n","\n","        base_image = T.Compose([T.ToTensor()])(base_image)\n","        base_mask = torch.from_numpy(base_mask).long()\n","        base_mask -= 1\n","        base_boundaries = torch.from_numpy(base_boundaries)\n","\n","        if(self.typeDataset != 'Train'):\n","          return base_image, base_mask, image_path, base_boundaries\n","\n","        return [base_image], [base_mask], image_path, [base_boundaries]"]},{"cell_type":"markdown","source":["### Dataset Utils"],"metadata":{"id":"xSaOfED0-zoc"}},{"cell_type":"code","source":["import matplotlib.patches as mpatches\n","\n","from collections import OrderedDict\n","COLOR_MAP = OrderedDict(\n","    Background=(255, 255, 255),\n","    Building=(255, 0, 0),\n","    Road=(255, 255, 0),\n","    Water=(0, 0, 255),\n","    Barren=(159, 129, 183),\n","    Forest=(34, 139, 34),\n","    Agricultural=(255, 195, 128),\n",")\n","\n","LABEL_MAP = OrderedDict(\n","    Background=0,\n","    Building=1,\n","    Road=2,\n","    Water=3,\n","    Barren=4,\n","    Forest=5,\n","    Agricultural=6,\n",")\n","inverted_label_map = OrderedDict((v, k) for k, v in LABEL_MAP.items())\n","\n","\n","def getLabelColor(label):\n","    # Default color for unclassified labels\n","    default_color = np.array([128, 128, 128])  # Gray\n","\n","    # Check if label exists in inverted_label_map\n","    label_name = inverted_label_map.get(label, None)\n","    if label_name is None or label_name not in COLOR_MAP:\n","        return default_color  # Return default color for unclassified\n","\n","    # Return the mapped color\n","    label_color = np.array(COLOR_MAP[label_name])\n","    return label_color\n","\n","\n","def getLegendHandles():\n","  handles = [mpatches.Patch(color=getLabelColor(i)/255, label=inverted_label_map[i]) for i in range(0, len(LABEL_MAP))]\n","  handles.append(mpatches.Patch(color=getLabelColor(-1)/255, label='Unclassified'))\n","  return handles\n","\n","def new_colors_mask(mask):\n","  new_image = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n","  for i, row in enumerate(mask):\n","    for j, cell in enumerate(row):\n","      new_image[i][j] = getLabelColor(cell.item())\n","  return new_image\n","\n"],"metadata":{"id":"tfi1pG3P-2H1","executionInfo":{"status":"ok","timestamp":1738259034661,"user_tz":-60,"elapsed":7,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Debug"],"metadata":{"id":"IrsSERdV_xZc"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"l5iBVMMn6-3-","executionInfo":{"status":"ok","timestamp":1738259034661,"user_tz":-60,"elapsed":6,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"outputs":[],"source":["# # Comment this cell to save GPU time\n","\n","# import matplotlib.pyplot as plt\n","# import torch\n","# from torch.utils.data import DataLoader\n","# import matplotlib.patches as mpatches\n","\n","# train_dataset = LoveDADataset(type='Train', seed=222)\n","# print(train_dataset.__len__())\n","\n","# # Get item\n","# image, mask, path, bd = train_dataset.__getitem__(88)\n","\n","# # Show path\n","# print(f\"Image is at {path}\")\n","\n","# # Show image\n","# image = image.permute(1, 2, 0)\n","# image = image.numpy()\n","# plt.imshow(image)\n","\n","# # Show mask\n","# new_image = new_colors_mask(mask)\n","# plt.imshow(image)\n","# plt.show()\n","# plt.legend(handles=getLegendHandles(), loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n","# plt.imshow(new_image)\n","# plt.show()\n","\n","# # Show boundaries\n","# # for row in bd:\n","# #     for col in row:\n","# #         if col != 0 and col != 1:\n","# #             print(col)\n","# bd = bd.numpy()\n","# plt.imshow(bd)\n"]},{"cell_type":"markdown","metadata":{"id":"uZwN55AlCRZc"},"source":["# Initialize model"]},{"cell_type":"markdown","source":["### STDC Net\n"],"metadata":{"id":"V1PldejM1ZCB"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import init\n","import math\n","\n","\n","\n","class ConvX(nn.Module):\n","    def __init__(self, in_planes, out_planes, kernel=3, stride=1):\n","        super(ConvX, self).__init__()\n","        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel, stride=stride, padding=kernel//2, bias=False)\n","        self.bn = nn.BatchNorm2d(out_planes)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        out = self.relu(self.bn(self.conv(x)))\n","        return out\n","\n","\n","class AddBottleneck(nn.Module):\n","    def __init__(self, in_planes, out_planes, block_num=3, stride=1):\n","        super(AddBottleneck, self).__init__()\n","        assert block_num > 1, print(\"block number should be larger than 1.\")\n","        self.conv_list = nn.ModuleList()\n","        self.stride = stride\n","        if stride == 2:\n","            self.avd_layer = nn.Sequential(\n","                nn.Conv2d(out_planes//2, out_planes//2, kernel_size=3, stride=2, padding=1, groups=out_planes//2, bias=False),\n","                nn.BatchNorm2d(out_planes//2),\n","            )\n","            self.skip = nn.Sequential(\n","                nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=2, padding=1, groups=in_planes, bias=False),\n","                nn.BatchNorm2d(in_planes),\n","                nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False),\n","                nn.BatchNorm2d(out_planes),\n","            )\n","            stride = 1\n","\n","        for idx in range(block_num):\n","            if idx == 0:\n","                self.conv_list.append(ConvX(in_planes, out_planes//2, kernel=1))\n","            elif idx == 1 and block_num == 2:\n","                self.conv_list.append(ConvX(out_planes//2, out_planes//2, stride=stride))\n","            elif idx == 1 and block_num > 2:\n","                self.conv_list.append(ConvX(out_planes//2, out_planes//4, stride=stride))\n","            elif idx < block_num - 1:\n","                self.conv_list.append(ConvX(out_planes//int(math.pow(2, idx)), out_planes//int(math.pow(2, idx+1))))\n","            else:\n","                self.conv_list.append(ConvX(out_planes//int(math.pow(2, idx)), out_planes//int(math.pow(2, idx))))\n","\n","    def forward(self, x):\n","        out_list = []\n","        out = x\n","\n","        for idx, conv in enumerate(self.conv_list):\n","            if idx == 0 and self.stride == 2:\n","                out = self.avd_layer(conv(out))\n","            else:\n","                out = conv(out)\n","            out_list.append(out)\n","\n","        if self.stride == 2:\n","            x = self.skip(x)\n","\n","        return torch.cat(out_list, dim=1) + x\n","\n","\n","\n","class CatBottleneck(nn.Module):\n","    def __init__(self, in_planes, out_planes, block_num=3, stride=1):\n","        super(CatBottleneck, self).__init__()\n","        assert block_num > 1, print(\"block number should be larger than 1.\")\n","        self.conv_list = nn.ModuleList()\n","        self.stride = stride\n","        if stride == 2:\n","            self.avd_layer = nn.Sequential(\n","                nn.Conv2d(out_planes//2, out_planes//2, kernel_size=3, stride=2, padding=1, groups=out_planes//2, bias=False),\n","                nn.BatchNorm2d(out_planes//2),\n","            )\n","            self.skip = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n","            stride = 1\n","\n","        for idx in range(block_num):\n","            if idx == 0:\n","                self.conv_list.append(ConvX(in_planes, out_planes//2, kernel=1))\n","            elif idx == 1 and block_num == 2:\n","                self.conv_list.append(ConvX(out_planes//2, out_planes//2, stride=stride))\n","            elif idx == 1 and block_num > 2:\n","                self.conv_list.append(ConvX(out_planes//2, out_planes//4, stride=stride))\n","            elif idx < block_num - 1:\n","                self.conv_list.append(ConvX(out_planes//int(math.pow(2, idx)), out_planes//int(math.pow(2, idx+1))))\n","            else:\n","                self.conv_list.append(ConvX(out_planes//int(math.pow(2, idx)), out_planes//int(math.pow(2, idx))))\n","\n","    def forward(self, x):\n","        out_list = []\n","        out1 = self.conv_list[0](x)\n","\n","        for idx, conv in enumerate(self.conv_list[1:]):\n","            if idx == 0:\n","                if self.stride == 2:\n","                    out = conv(self.avd_layer(out1))\n","                else:\n","                    out = conv(out1)\n","            else:\n","                out = conv(out)\n","            out_list.append(out)\n","\n","        if self.stride == 2:\n","            out1 = self.skip(out1)\n","        out_list.insert(0, out1)\n","\n","        out = torch.cat(out_list, dim=1)\n","        return out\n","\n","#STDC2Net\n","class STDCNet1446(nn.Module):\n","    def __init__(self, base=64, layers=[4,5,3], block_num=4, type=\"cat\", num_classes=1000, dropout=0.20, pretrain_model='', use_conv_last=False):\n","        super(STDCNet1446, self).__init__()\n","        if type == \"cat\":\n","            block = CatBottleneck\n","        elif type == \"add\":\n","            block = AddBottleneck\n","        self.use_conv_last = use_conv_last\n","        self.features = self._make_layers(base, layers, block_num, block)\n","        self.conv_last = ConvX(base*16, max(1024, base*16), 1, 1)\n","        self.gap = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Linear(max(1024, base*16), max(1024, base*16), bias=False)\n","        self.bn = nn.BatchNorm1d(max(1024, base*16))\n","        self.relu = nn.ReLU(inplace=True)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.linear = nn.Linear(max(1024, base*16), num_classes, bias=False)\n","\n","        self.x2 = nn.Sequential(self.features[:1])\n","        self.x4 = nn.Sequential(self.features[1:2])\n","        self.x8 = nn.Sequential(self.features[2:6])\n","        self.x16 = nn.Sequential(self.features[6:11])\n","        self.x32 = nn.Sequential(self.features[11:])\n","\n","        if pretrain_model:\n","            print('use pretrain model {}'.format(pretrain_model))\n","            self.init_weight(pretrain_model)\n","        else:\n","            self.init_params()\n","\n","    def init_weight(self, pretrain_model):\n","\n","        state_dict = torch.load(pretrain_model)[\"state_dict\"]\n","        self_state_dict = self.state_dict()\n","        for k, v in state_dict.items():\n","            self_state_dict.update({k: v})\n","        self.load_state_dict(self_state_dict)\n","\n","    def init_params(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                init.kaiming_normal_(m.weight, mode='fan_out')\n","                if m.bias is not None:\n","                    init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                init.constant_(m.weight, 1)\n","                init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                init.normal_(m.weight, std=0.001)\n","                if m.bias is not None:\n","                    init.constant_(m.bias, 0)\n","\n","    def _make_layers(self, base, layers, block_num, block):\n","        features = []\n","        features += [ConvX(3, base//2, 3, 2)]\n","        features += [ConvX(base//2, base, 3, 2)]\n","\n","        for i, layer in enumerate(layers):\n","            for j in range(layer):\n","                if i == 0 and j == 0:\n","                    features.append(block(base, base*4, block_num, 2))\n","                elif j == 0:\n","                    features.append(block(base*int(math.pow(2,i+1)), base*int(math.pow(2,i+2)), block_num, 2))\n","                else:\n","                    features.append(block(base*int(math.pow(2,i+2)), base*int(math.pow(2,i+2)), block_num, 1))\n","\n","        return nn.Sequential(*features)\n","\n","    def forward(self, x):\n","        feat2 = self.x2(x)\n","        feat4 = self.x4(feat2)\n","        feat8 = self.x8(feat4)\n","        feat16 = self.x16(feat8)\n","        feat32 = self.x32(feat16)\n","        if self.use_conv_last:\n","           feat32 = self.conv_last(feat32)\n","\n","        return feat2, feat4, feat8, feat16, feat32\n","\n","    def forward_impl(self, x):\n","        out = self.features(x)\n","        out = self.conv_last(out).pow(2)\n","        out = self.gap(out).flatten(1)\n","        out = self.fc(out)\n","        # out = self.bn(out)\n","        out = self.relu(out)\n","        # out = self.relu(self.bn(self.fc(out)))\n","        out = self.dropout(out)\n","        out = self.linear(out)\n","        return out\n","\n","# STDC1Net\n","class STDCNet813(nn.Module):\n","    def __init__(self, base=64, layers=[2,2,2], block_num=4, type=\"cat\", num_classes=1000, dropout=0.20, pretrain_model='', use_conv_last=False):\n","        super(STDCNet813, self).__init__()\n","        if type == \"cat\":\n","            block = CatBottleneck\n","        elif type == \"add\":\n","            block = AddBottleneck\n","        self.use_conv_last = use_conv_last\n","        self.features = self._make_layers(base, layers, block_num, block)\n","        self.conv_last = ConvX(base*16, max(1024, base*16), 1, 1)\n","        self.gap = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Linear(max(1024, base*16), max(1024, base*16), bias=False)\n","        self.bn = nn.BatchNorm1d(max(1024, base*16))\n","        self.relu = nn.ReLU(inplace=True)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.linear = nn.Linear(max(1024, base*16), num_classes, bias=False)\n","\n","        self.x2 = nn.Sequential(self.features[:1])\n","        self.x4 = nn.Sequential(self.features[1:2])\n","        self.x8 = nn.Sequential(self.features[2:4])\n","        self.x16 = nn.Sequential(self.features[4:6])\n","        self.x32 = nn.Sequential(self.features[6:])\n","\n","        if pretrain_model:\n","            print('use pretrain model {}'.format(pretrain_model))\n","            self.init_weight(pretrain_model)\n","        else:\n","            self.init_params()\n","\n","    def init_weight(self, pretrain_model):\n","\n","        state_dict = torch.load(pretrain_model)[\"state_dict\"]\n","        self_state_dict = self.state_dict()\n","        for k, v in state_dict.items():\n","            self_state_dict.update({k: v})\n","        self.load_state_dict(self_state_dict)\n","\n","    def init_params(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                init.kaiming_normal_(m.weight, mode='fan_out')\n","                if m.bias is not None:\n","                    init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                init.constant_(m.weight, 1)\n","                init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                init.normal_(m.weight, std=0.001)\n","                if m.bias is not None:\n","                    init.constant_(m.bias, 0)\n","\n","    def _make_layers(self, base, layers, block_num, block):\n","        features = []\n","        features += [ConvX(3, base//2, 3, 2)]\n","        features += [ConvX(base//2, base, 3, 2)]\n","\n","        for i, layer in enumerate(layers):\n","            for j in range(layer):\n","                if i == 0 and j == 0:\n","                    features.append(block(base, base*4, block_num, 2))\n","                elif j == 0:\n","                    features.append(block(base*int(math.pow(2,i+1)), base*int(math.pow(2,i+2)), block_num, 2))\n","                else:\n","                    features.append(block(base*int(math.pow(2,i+2)), base*int(math.pow(2,i+2)), block_num, 1))\n","\n","        return nn.Sequential(*features)\n","\n","    def forward(self, x):\n","        feat2 = self.x2(x)\n","        feat4 = self.x4(feat2)\n","        feat8 = self.x8(feat4)\n","        feat16 = self.x16(feat8)\n","        feat32 = self.x32(feat16)\n","        if self.use_conv_last:\n","           feat32 = self.conv_last(feat32)\n","\n","        return feat2, feat4, feat8, feat16, feat32\n","\n","    def forward_impl(self, x):\n","        out = self.features(x)\n","        out = self.conv_last(out).pow(2)\n","        out = self.gap(out).flatten(1)\n","        out = self.fc(out)\n","        # out = self.bn(out)\n","        out = self.relu(out)\n","        # out = self.relu(self.bn(self.fc(out)))\n","        out = self.dropout(out)\n","        out = self.linear(out)\n","        return out"],"metadata":{"id":"8lOLZdcA1W04","executionInfo":{"status":"ok","timestamp":1738259034661,"user_tz":-60,"elapsed":6,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### STDC Definition"],"metadata":{"id":"uiBwR_Yg1dVO"}},{"cell_type":"code","source":["#!/usr/bin/python\n","# -*- encoding: utf-8 -*-\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","\n","BatchNorm2d = nn.BatchNorm2d\n","\n","class ConvBNReLU(nn.Module):\n","    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, *args, **kwargs):\n","        super(ConvBNReLU, self).__init__()\n","        self.conv = nn.Conv2d(in_chan,\n","                out_chan,\n","                kernel_size = ks,\n","                stride = stride,\n","                padding = padding,\n","                bias = False)\n","        self.bn = BatchNorm2d(out_chan)\n","        #self.bn = BatchNorm2d(out_chan, activation='none')\n","        self.relu = nn.ReLU()\n","        self.init_weight()\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        return x\n","\n","    def init_weight(self):\n","        for ly in self.children():\n","            if isinstance(ly, nn.Conv2d):\n","                nn.init.kaiming_normal_(ly.weight, a=1)\n","                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n","\n","\n","class BiSeNetOutput(nn.Module):\n","    def __init__(self, in_chan, mid_chan, n_classes, *args, **kwargs):\n","        super(BiSeNetOutput, self).__init__()\n","        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1)\n","        self.conv_out = nn.Conv2d(mid_chan, n_classes, kernel_size=1, bias=False)\n","        self.init_weight()\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.conv_out(x)\n","        return x\n","\n","    def init_weight(self):\n","        for ly in self.children():\n","            if isinstance(ly, nn.Conv2d):\n","                nn.init.kaiming_normal_(ly.weight, a=1)\n","                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n","\n","    def get_params(self):\n","        wd_params, nowd_params = [], []\n","        for name, module in self.named_modules():\n","            if isinstance(module, (nn.Linear, nn.Conv2d)):\n","                wd_params.append(module.weight)\n","                if not module.bias is None:\n","                    nowd_params.append(module.bias)\n","            elif isinstance(module, BatchNorm2d):\n","                nowd_params += list(module.parameters())\n","        return wd_params, nowd_params\n","\n","\n","class AttentionRefinementModule(nn.Module):\n","    def __init__(self, in_chan, out_chan, *args, **kwargs):\n","        super(AttentionRefinementModule, self).__init__()\n","        self.conv = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1)\n","        self.conv_atten = nn.Conv2d(out_chan, out_chan, kernel_size= 1, bias=False)\n","        self.bn_atten = BatchNorm2d(out_chan)\n","        #self.bn_atten = BatchNorm2d(out_chan, activation='none')\n","\n","        self.sigmoid_atten = nn.Sigmoid()\n","        self.init_weight()\n","\n","    def forward(self, x):\n","        feat = self.conv(x)\n","        atten = F.avg_pool2d(feat, feat.size()[2:])\n","        atten = self.conv_atten(atten)\n","        atten = self.bn_atten(atten)\n","        atten = self.sigmoid_atten(atten)\n","        out = torch.mul(feat, atten)\n","        return out\n","\n","    def init_weight(self):\n","        for ly in self.children():\n","            if isinstance(ly, nn.Conv2d):\n","                nn.init.kaiming_normal_(ly.weight, a=1)\n","                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n","\n","\n","class ContextPath(nn.Module):\n","    def __init__(self, backbone='CatNetSmall', pretrain_model='', use_conv_last=False, *args, **kwargs):\n","        super(ContextPath, self).__init__()\n","\n","        self.backbone_name = backbone\n","        if backbone == 'STDCNet1446':\n","            self.backbone = STDCNet1446(pretrain_model=pretrain_model, use_conv_last=use_conv_last)\n","            self.arm16 = AttentionRefinementModule(512, 128)\n","            inplanes = 1024\n","            if use_conv_last:\n","                inplanes = 1024\n","            self.arm32 = AttentionRefinementModule(inplanes, 128)\n","            self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n","            self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n","            self.conv_avg = ConvBNReLU(inplanes, 128, ks=1, stride=1, padding=0)\n","\n","        elif backbone == 'STDCNet813':\n","            self.backbone = STDCNet813(pretrain_model=pretrain_model, use_conv_last=use_conv_last)\n","            self.arm16 = AttentionRefinementModule(512, 128)\n","            inplanes = 1024\n","            if use_conv_last:\n","                inplanes = 1024\n","            self.arm32 = AttentionRefinementModule(inplanes, 128)\n","            self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n","            self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n","            self.conv_avg = ConvBNReLU(inplanes, 128, ks=1, stride=1, padding=0)\n","        else:\n","            print(\"backbone is not in backbone lists\")\n","            exit(0)\n","\n","        self.init_weight()\n","\n","    def forward(self, x):\n","        H0, W0 = x.size()[2:]\n","\n","        feat2, feat4, feat8, feat16, feat32 = self.backbone(x)\n","        H8, W8 = feat8.size()[2:]\n","        H16, W16 = feat16.size()[2:]\n","        H32, W32 = feat32.size()[2:]\n","\n","        avg = F.avg_pool2d(feat32, feat32.size()[2:])\n","\n","        avg = self.conv_avg(avg)\n","        avg_up = F.interpolate(avg, (H32, W32), mode='nearest')\n","\n","        feat32_arm = self.arm32(feat32)\n","        feat32_sum = feat32_arm + avg_up\n","        feat32_up = F.interpolate(feat32_sum, (H16, W16), mode='nearest')\n","        feat32_up = self.conv_head32(feat32_up)\n","\n","        feat16_arm = self.arm16(feat16)\n","        feat16_sum = feat16_arm + feat32_up\n","        feat16_up = F.interpolate(feat16_sum, (H8, W8), mode='nearest')\n","        feat16_up = self.conv_head16(feat16_up)\n","\n","        return feat2, feat4, feat8, feat16, feat16_up, feat32_up # x8, x16\n","\n","    def init_weight(self):\n","        for ly in self.children():\n","            if isinstance(ly, nn.Conv2d):\n","                nn.init.kaiming_normal_(ly.weight, a=1)\n","                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n","\n","    def get_params(self):\n","        wd_params, nowd_params = [], []\n","        for name, module in self.named_modules():\n","            if isinstance(module, (nn.Linear, nn.Conv2d)):\n","                wd_params.append(module.weight)\n","                if not module.bias is None:\n","                    nowd_params.append(module.bias)\n","            elif isinstance(module, BatchNorm2d):\n","                nowd_params += list(module.parameters())\n","        return wd_params, nowd_params\n","\n","\n","class FeatureFusionModule(nn.Module):\n","    def __init__(self, in_chan, out_chan, *args, **kwargs):\n","        super(FeatureFusionModule, self).__init__()\n","        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n","        self.conv1 = nn.Conv2d(out_chan,\n","                out_chan//4,\n","                kernel_size = 1,\n","                stride = 1,\n","                padding = 0,\n","                bias = False)\n","        self.conv2 = nn.Conv2d(out_chan//4,\n","                out_chan,\n","                kernel_size = 1,\n","                stride = 1,\n","                padding = 0,\n","                bias = False)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.sigmoid = nn.Sigmoid()\n","        self.init_weight()\n","\n","    def forward(self, fsp, fcp):\n","        fcat = torch.cat([fsp, fcp], dim=1)\n","        feat = self.convblk(fcat)\n","        atten = F.avg_pool2d(feat, feat.size()[2:])\n","        atten = self.conv1(atten)\n","        atten = self.relu(atten)\n","        atten = self.conv2(atten)\n","        atten = self.sigmoid(atten)\n","        feat_atten = torch.mul(feat, atten)\n","        feat_out = feat_atten + feat\n","        return feat_out\n","\n","    def init_weight(self):\n","        for ly in self.children():\n","            if isinstance(ly, nn.Conv2d):\n","                nn.init.kaiming_normal_(ly.weight, a=1)\n","                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n","\n","    def get_params(self):\n","        wd_params, nowd_params = [], []\n","        for name, module in self.named_modules():\n","            if isinstance(module, (nn.Linear, nn.Conv2d)):\n","                wd_params.append(module.weight)\n","                if not module.bias is None:\n","                    nowd_params.append(module.bias)\n","            elif isinstance(module, BatchNorm2d):\n","                nowd_params += list(module.parameters())\n","        return wd_params, nowd_params\n","\n","\n","class BiSeNet(nn.Module):\n","    def __init__(self, backbone, n_classes, pretrain_model='', use_boundary_2=False, use_boundary_4=False, use_boundary_8=False, use_boundary_16=False, use_conv_last=False, heat_map=False, *args, **kwargs):\n","        super(BiSeNet, self).__init__()\n","\n","        self.use_boundary_2 = use_boundary_2\n","        self.use_boundary_4 = use_boundary_4\n","        self.use_boundary_8 = use_boundary_8\n","        self.use_boundary_16 = use_boundary_16\n","        # self.heat_map = heat_map\n","        self.cp = ContextPath(backbone, pretrain_model, use_conv_last=use_conv_last)\n","\n","\n","\n","        if backbone == 'STDCNet1446':\n","            conv_out_inplanes = 128\n","            sp2_inplanes = 32\n","            sp4_inplanes = 64\n","            sp8_inplanes = 256\n","            sp16_inplanes = 512\n","            inplane = sp8_inplanes + conv_out_inplanes\n","\n","        elif backbone == 'STDCNet813':\n","            conv_out_inplanes = 128\n","            sp2_inplanes = 32\n","            sp4_inplanes = 64\n","            sp8_inplanes = 256\n","            sp16_inplanes = 512\n","            inplane = sp8_inplanes + conv_out_inplanes\n","\n","        else:\n","            print(\"backbone is not in backbone lists\")\n","            exit(0)\n","\n","        self.ffm = FeatureFusionModule(inplane, 256)\n","        self.conv_out = BiSeNetOutput(256, 256, n_classes)\n","        self.conv_out16 = BiSeNetOutput(conv_out_inplanes, 64, n_classes)\n","        self.conv_out32 = BiSeNetOutput(conv_out_inplanes, 64, n_classes)\n","\n","        self.conv_out_sp16 = BiSeNetOutput(sp16_inplanes, 64, 1)\n","\n","        self.conv_out_sp8 = BiSeNetOutput(sp8_inplanes, 64, 1)\n","        self.conv_out_sp4 = BiSeNetOutput(sp4_inplanes, 64, 1)\n","        self.conv_out_sp2 = BiSeNetOutput(sp2_inplanes, 64, 1)\n","        self.init_weight()\n","\n","    def forward(self, x):\n","        H, W = x.size()[2:]\n","\n","        feat_res2, feat_res4, feat_res8, feat_res16, feat_cp8, feat_cp16 = self.cp(x)\n","\n","        feat_out_sp2 = self.conv_out_sp2(feat_res2)\n","\n","        feat_out_sp4 = self.conv_out_sp4(feat_res4)\n","\n","        feat_out_sp8 = self.conv_out_sp8(feat_res8)\n","\n","        feat_out_sp16 = self.conv_out_sp16(feat_res16)\n","\n","        feat_fuse = self.ffm(feat_res8, feat_cp8)\n","\n","        feat_out = self.conv_out(feat_fuse)\n","        feat_out16 = self.conv_out16(feat_cp8)\n","        feat_out32 = self.conv_out32(feat_cp16)\n","\n","        feat_out = F.interpolate(feat_out, (H, W), mode='bilinear', align_corners=True)\n","        feat_out16 = F.interpolate(feat_out16, (H, W), mode='bilinear', align_corners=True)\n","        feat_out32 = F.interpolate(feat_out32, (H, W), mode='bilinear', align_corners=True)\n","\n","\n","        if self.use_boundary_2 and self.use_boundary_4 and self.use_boundary_8:\n","            return feat_out, feat_out16, feat_out32, feat_out_sp2, feat_out_sp4, feat_out_sp8\n","\n","        if (not self.use_boundary_2) and self.use_boundary_4 and self.use_boundary_8:\n","            return feat_out, feat_out16, feat_out32, feat_out_sp4, feat_out_sp8\n","\n","        if (not self.use_boundary_2) and (not self.use_boundary_4) and self.use_boundary_8:\n","            return feat_out, feat_out16, feat_out32, feat_out_sp8\n","\n","        if (not self.use_boundary_2) and (not self.use_boundary_4) and (not self.use_boundary_8):\n","            return feat_out, feat_out16, feat_out32\n","\n","    def init_weight(self):\n","        for ly in self.children():\n","            if isinstance(ly, nn.Conv2d):\n","                nn.init.kaiming_normal_(ly.weight, a=1)\n","                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n","\n","    def get_params(self):\n","        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n","        for name, child in self.named_children():\n","            child_wd_params, child_nowd_params = child.get_params()\n","            if isinstance(child, (FeatureFusionModule, BiSeNetOutput)):\n","                lr_mul_wd_params += child_wd_params\n","                lr_mul_nowd_params += child_nowd_params\n","            else:\n","                wd_params += child_wd_params\n","                nowd_params += child_nowd_params\n","        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params\n","\n"],"metadata":{"id":"XGBkU3KzzTdK","executionInfo":{"status":"ok","timestamp":1738259034661,"user_tz":-60,"elapsed":6,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Load STDC Model"],"metadata":{"id":"isRcFkZ3CxpI"}},{"cell_type":"code","source":["import gdown\n","import tarfile\n","import os\n","\n","if (os.path.exists(\"./model_maxmIOU75.pth\") == False):\n","  url = \"https://drive.google.com/uc?id=17nPaTe9mCQ9OEb0VFz32eJVfWzvEPKGW\"\n","  output = \"./\"\n","  gdown.download(url, output, quiet=False)\n","# Then keep as tar, as it's already the correct format to feed the model\n","\n","# # Create a config object with required parameters\n","# class Config:\n","#     class MODEL:\n","#         NAME = 'pidnet_s'  # or 'pidnet_m' or 'pidnet_l'\n","#         PRETRAINED = 'PIDNet_S_ImageNet.pth.tar'\n","#     class DATASET:\n","#         NUM_CLASSES = len(LABEL_MAP)\n","\n","# cfg = Config()\n","pretrain_path = ''\n","\n","\n","model = BiSeNet(backbone='STDCNet1446', n_classes=len(LABEL_MAP), pretrain_model=pretrain_path,\n","    use_boundary_2=True, use_boundary_4=True, use_boundary_8=True,\n","    use_boundary_16=False, use_conv_last=False)\n","\n","# Load the checkpoint with map_location to ensure it loads on CPU\n","checkpoint = torch.load('./model_maxmIOU75.pth', map_location=torch.device('cpu'))\n","model_state_dict = model.state_dict()\n","\n","# Filter out unnecessary keys\n","pretrained_dict = {k: v for k, v in checkpoint.items() if k in model_state_dict and model_state_dict[k].shape == v.shape}\n","\n","# Overwrite entries in the existing state dict\n","model_state_dict.update(pretrained_dict)\n","\n","# Load the new state dict\n","model.load_state_dict(model_state_dict)\n","\n","print(\"Model loaded\")"],"metadata":{"id":"ujP_2PffskRk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738259036065,"user_tz":-60,"elapsed":1409,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"outputId":"41b168d1-dfd5-46fd-b6d3-f39e4b18debc"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model loaded\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-e20eaad5c52b>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load('./model_maxmIOU75.pth', map_location=torch.device('cpu'))\n"]}]},{"cell_type":"markdown","metadata":{"id":"3DwQcmOWpUxn"},"source":["# Training & Dataset creation"]},{"cell_type":"markdown","source":["### Setup, Create Datasets and DataLoaders. With annexed transforms."],"metadata":{"id":"IwDwW9Hac6SS"}},{"cell_type":"markdown","source":["# TEST"],"metadata":{"id":"soROYpcYyajp"}},{"cell_type":"code","source":["DEVICE = 'cuda' # 'cuda' or 'cpu'\n","TYPE = 'Test'\n","\n","TEST_ONLY_ON_BEST = False # Leave False unless you're really picky/in need of time\n","TEST_MODELS_FROM_MYDRIVE_TOO = True\n","INTEREST = \"s=0\"\n","\n","RESIZE = 512"],"metadata":{"id":"0hcHUuFXaVZV","executionInfo":{"status":"ok","timestamp":1738259036065,"user_tz":-60,"elapsed":4,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### Copy Some Models From MyDrive/LoveDA"],"metadata":{"id":"sOvtVA095I7b"}},{"cell_type":"code","source":["# Run the first cell pls\n","import os\n","\n","BASE_PATH = \"/content/drive/MyDrive/LoveDA/\"\n","\n","if TEST_MODELS_FROM_MYDRIVE_TOO:\n","  all_mydrive = os.listdir(BASE_PATH)\n","  all_models = [f for f in all_mydrive if f.endswith('.pth')]\n","  models_of_interest = [f for f in all_models if INTEREST in f]\n","  for model_name in models_of_interest:\n","      if not os.path.exists(model_name):\n","        print(f\"Copying {model_name} locally from MyDrive\")\n","        !cp {BASE_PATH+model_name} ."],"metadata":{"id":"BS2GA3Os5IzP","executionInfo":{"status":"ok","timestamp":1738259036065,"user_tz":-60,"elapsed":3,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Define Transforms In Case"],"metadata":{"id":"bWAbBMMKmj0g"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","from albumentations import Compose, HorizontalFlip, RandomRotate90, RandomScale, RandomCrop, GaussNoise, Rotate, Resize, OneOf, Normalize, ColorJitter, GaussianBlur\n","from albumentations.pytorch import ToTensorV2\n","\n","#How big should be the image that we feed to the model?\n","RESIZE = 512\n","# DEFINE TRANSFORMATIONS HERE\n","# To Tensor is not needed since its performed inside the getitem\n","\n","\n","AUGMENTATIONS = {\n","    'Resize': Compose([\n","            Resize(RESIZE, RESIZE),\n","    ], additional_targets={\"boundaries\": \"mask\"}),\n","    'None' : Compose([\n","            ], additional_targets={\"boundaries\": \"mask\"})\n","}"],"metadata":{"id":"J8DhNU7Lc7aD","executionInfo":{"status":"ok","timestamp":1738259036595,"user_tz":-60,"elapsed":533,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"578e6af3-3d0e-4cd8-f87e-305e63fd24a8"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","  check_for_updates()\n"]}]},{"cell_type":"markdown","source":["### Actually Test"],"metadata":{"id":"ha8NDG9xmn6H"}},{"cell_type":"code","source":["!pip install torchmetrics ptflops"],"metadata":{"id":"7DKgjGPKydLH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738259038681,"user_tz":-60,"elapsed":2091,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"outputId":"a72bf3d2-276a-4884-a527-970ad913752a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: ptflops in /usr/local/lib/python3.11/dist-packages (0.7.4)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu124)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.11.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"]}]},{"cell_type":"code","source":["from torchmetrics import Accuracy\n","from tqdm import tqdm\n","import time\n","import ptflops\n","import os\n","import pandas as pd\n","\n","test_augmentation = AUGMENTATIONS['None']\n","# target_type = 'ActualTest'\n","NUM_CLASSES = len(LABEL_MAP)\n","DEVICE = 'cuda'\n","\n","# Create unweighted models\n","model = BiSeNet(backbone='STDCNet1446', n_classes=len(LABEL_MAP), pretrain_model=pretrain_path,\n","    use_boundary_2=True, use_boundary_4=True, use_boundary_8=True,\n","    use_boundary_16=False, use_conv_last=False)\n","\n","model_files_paths = [f for f in os.listdir('.') if f.endswith('.pth')]\n","model_files_paths.remove('model_maxmIOU75.pth')\n","print(model_files_paths)\n","\n","for model_file_path in model_files_paths:\n","    best_model = torch.load(model_file_path, weights_only=True)\n","\n","    model.load_state_dict(best_model)\n","    model = model.to(DEVICE)\n","\n","    accuracy, mIoU = False, True\n","    iou_data = []\n","\n","\n","    TARGETs = ['Urban', 'Rural']\n","    for TARGET in TARGETs:\n","        if TARGET == 'Urban':\n","            target_type = 'ActualTest'\n","            #target_type = 'Validation'\n","        elif TARGET == 'Rural':\n","            target_type = 'ActualTest'\n","            #target_type = 'Total'\n","\n","        test_dataset = LoveDADataset(baseTransform=test_augmentation, augTransforms=None, split=TARGET, type=target_type, useBoundaries=False)\n","        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, drop_last=True, pin_memory=True)\n","\n","        #### TEST LOOP\n","        model.eval()\n","        print(f\"Testing model={model_file_path} on domain={TARGET} on a {target_type} split\")\n","\n","        # # Latency\n","        # with torch.no_grad():\n","        #     start_time = time.time()\n","        #     for _ in range(100):\n","        #         _ = model(torch.randn(1, 3, RESIZE, RESIZE).to(DEVICE))\n","        #     end_time = time.time()\n","        # latency = (end_time - start_time) / 100\n","        # print(f\"Latency: {latency:.4f} seconds\")\n","\n","        # # FLOPs\n","        # macs, _ = ptflops.get_model_complexity_info(model,\n","        #     (3, RESIZE, RESIZE), as_strings=False,\n","        #     print_per_layer_stat=False, verbose=False)\n","        # flops = macs * 2  # MACs perform two FLOPs\n","        # print(\"FLOPs:\", flops)\n","\n","        # # Number of parameters\n","        # total_params = sum(p.numel() for p in model.parameters())\n","        # print(f\"Total number of parameters: {total_params}\")\n","\n","        if TYPE == 'Test':\n","            with torch.no_grad():\n","                total_union = torch.zeros(NUM_CLASSES).to(DEVICE)\n","                total_intersection = torch.zeros(NUM_CLASSES).to(DEVICE)\n","                if accuracy:\n","                  meter = Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(DEVICE)\n","                for (i, batch) in enumerate(tqdm(test_loader)):\n","                    ### Extract input\n","                    images, masks, img_path, bd_gts = batch\n","                    images = images.float().to(DEVICE)\n","                    masks = masks.to(DEVICE)\n","\n","                    #Printing size, testing on original image size\n","                    if i == 0:\n","                        print(\"\")\n","                        print(\"===============================================\")\n","                        print(f\"Image shape: {images.shape}\")\n","                        print(f\"Mask shape: {masks.shape}\")\n","                        print(\"===============================================\")\n","\n","                    # Downscale images batch\n","                    images = F.interpolate(images, size=(RESIZE, RESIZE), mode='bilinear')\n","\n","                    ### ===> Forward, Upscale, Compute Losses\n","                    ## Forward\n","                    outputs = model(images)\n","\n","                    ## Upscale (bilinear interpolation to original size)\n","                    h, w = masks.size(1), masks.size(2)\n","                    ph, pw = outputs[0].size(2), outputs[0].size(3)\n","                    if ph != h or pw != w:\n","                        outputs = (F.interpolate(outputs[0], size=(h, w), mode='bilinear', align_corners=True), *outputs[1:])\n","\n","                    # Output 0 is the prediction\n","\n","                    # Shape: NBATCHES x classes x h x w\n","                    class_indices = torch.argmax(outputs[0], dim=1)  # Shape: NBATCHES x h x w\n","                    if accuracy:\n","                    # Create a mask for valid targets (where target is not -1)\n","                        valid_mask = (masks != -1)  # Mask of shape: NBATCHES x h x w\n","                        # Apply the mask to ignore -1 targets when updating the accuracy metric\n","                        meter.update(class_indices[valid_mask], masks[valid_mask])\n","\n","                    if mIoU:\n","                        for predicted, target in zip(class_indices, masks): # Iterating image for image\n","                            for i in range(NUM_CLASSES):\n","                                total_intersection[i] += torch.sum(torch.logical_and(predicted == i, target == i))\n","                                total_union[i] += torch.sum(torch.logical_or(torch.logical_and(predicted == i, target != -1), target == i))\n","\n","\n","        if accuracy:\n","            accuracy = meter.compute()\n","            print(f'\\nAccuracy on the target domain: {100 * accuracy:.2f}%')\n","\n","        if mIoU:\n","            intersection_over_union = total_intersection / total_union\n","\n","            # Per class IoU\n","            for i, iou in enumerate(intersection_over_union):\n","                class_name = list(LABEL_MAP.keys())[list(LABEL_MAP.values()).index(i)]  # Get the class name from LABEL_MAP\n","                iou_data.append({f'Class Name': class_name, 'IoU': iou.item()})\n","                print(f'{class_name} IoU: {iou:.4f}')\n","\n","            mIoU = torch.mean(intersection_over_union)\n","            iou_data.append({f'Class Name': f'Mean IoU {TARGET}', 'IoU': mIoU.cpu().numpy()})\n","            print(f'\\nmIoU on the {TARGET} domain: {mIoU}')\n","\n","        print(\"========================================================================\")\n","\n","    # Create a pandas DataFrame\n","    iou_df = pd.DataFrame(iou_data)\n","\n","    # Optionally, save the DataFrame to a CSV file\n","    iou_df.to_csv(f'iou_statistics_for{model_file_path}.csv', index=False, float_format=f'%4f', )\n"],"metadata":{"id":"M3c7DFDRymnI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a9ab05be-62cd-476c-c4a2-e7bc024d816c","executionInfo":{"status":"ok","timestamp":1738259170772,"user_tz":-60,"elapsed":132095,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["['best_DA_STDC_model_LA=0.0001_LD=0.1_LRdisc=0.0005_s=0_spid=0.pth_20.pth', 'best_DA_STDC_model_LA=0.0001_LD=0.1_LRdisc=0.0005_s=0_spid=0__bestmIoUUrban.pth']\n","Dataset size: 677\n","Testing model=best_DA_STDC_model_LA=0.0001_LD=0.1_LRdisc=0.0005_s=0_spid=0.pth_20.pth on domain=Urban on a ActualTest split\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/21 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","===============================================\n","Image shape: torch.Size([32, 3, 1024, 1024])\n","Mask shape: torch.Size([32, 1024, 1024])\n","===============================================\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 21/21 [00:27<00:00,  1.31s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Background IoU: 0.3146\n","Building IoU: 0.3983\n","Road IoU: 0.3801\n","Water IoU: 0.4274\n","Barren IoU: 0.1666\n","Forest IoU: 0.3984\n","Agricultural IoU: 0.1329\n","\n","mIoU on the Urban domain: 0.316921204328537\n","========================================================================\n","Dataset size: 992\n","Testing model=best_DA_STDC_model_LA=0.0001_LD=0.1_LRdisc=0.0005_s=0_spid=0.pth_20.pth on domain=Rural on a ActualTest split\n"]},{"output_type":"stream","name":"stderr","text":["  3%|▎         | 1/31 [00:03<01:46,  3.54s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","===============================================\n","Image shape: torch.Size([32, 3, 1024, 1024])\n","Mask shape: torch.Size([32, 1024, 1024])\n","===============================================\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 31/31 [00:37<00:00,  1.20s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Background IoU: 0.4744\n","Building IoU: 0.2141\n","Road IoU: 0.1161\n","Water IoU: 0.2778\n","Barren IoU: 0.0987\n","Forest IoU: 0.0139\n","Agricultural IoU: 0.0694\n","\n","mIoU on the Rural domain: 0.18062131106853485\n","========================================================================\n","Dataset size: 677\n","Testing model=best_DA_STDC_model_LA=0.0001_LD=0.1_LRdisc=0.0005_s=0_spid=0__bestmIoUUrban.pth on domain=Urban on a ActualTest split\n"]},{"output_type":"stream","name":"stderr","text":["  5%|▍         | 1/21 [00:03<01:11,  3.57s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","===============================================\n","Image shape: torch.Size([32, 3, 1024, 1024])\n","Mask shape: torch.Size([32, 1024, 1024])\n","===============================================\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 21/21 [00:27<00:00,  1.30s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Background IoU: 0.3146\n","Building IoU: 0.3983\n","Road IoU: 0.3801\n","Water IoU: 0.4274\n","Barren IoU: 0.1666\n","Forest IoU: 0.3984\n","Agricultural IoU: 0.1329\n","\n","mIoU on the Urban domain: 0.316921204328537\n","========================================================================\n","Dataset size: 992\n","Testing model=best_DA_STDC_model_LA=0.0001_LD=0.1_LRdisc=0.0005_s=0_spid=0__bestmIoUUrban.pth on domain=Rural on a ActualTest split\n"]},{"output_type":"stream","name":"stderr","text":["  3%|▎         | 1/31 [00:03<01:43,  3.45s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","===============================================\n","Image shape: torch.Size([32, 3, 1024, 1024])\n","Mask shape: torch.Size([32, 1024, 1024])\n","===============================================\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 31/31 [00:36<00:00,  1.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Background IoU: 0.4744\n","Building IoU: 0.2141\n","Road IoU: 0.1161\n","Water IoU: 0.2778\n","Barren IoU: 0.0987\n","Forest IoU: 0.0139\n","Agricultural IoU: 0.0694\n","\n","mIoU on the Rural domain: 0.18062131106853485\n","========================================================================\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# from google.colab import runtime\n","# runtime.unassign()"],"metadata":{"id":"1NS4WgpdTUmM","executionInfo":{"status":"ok","timestamp":1738259170772,"user_tz":-60,"elapsed":9,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Copy all file with extension .csv (results) on Drive\n","\n","import shutil\n","import glob\n","\n","# Source directory containing .csv files\n","source_dir = \"/content\"\n","# Destination directory on Drive\n","dest_dir = \"/content/drive/MyDrive/LoveDA/Results/\"\n","\n","# Find all .csv files in the source directory\n","csv_files = glob.glob(f\"{source_dir}/*.csv\")\n","\n","# Copy each .csv file to the destination directory\n","for file in csv_files:\n","    shutil.copy(file, dest_dir)\n","\n","print(f\"Copied {len(csv_files)} .csv files to {dest_dir}\")\n","\n","\n"],"metadata":{"id":"fgql4ATZXCJl","executionInfo":{"status":"ok","timestamp":1738259170773,"user_tz":-60,"elapsed":9,"user":{"displayName":"Fabio Gigante","userId":"11849699345227849703"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"36bbb52c-9a5a-458d-8532-53ccc428ad12"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Copied 2 .csv files to /content/drive/MyDrive/LoveDA/Results/\n"]}]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["7zeYu_g-bpip","68BKhuGh2vJH","xSaOfED0-zoc","IrsSERdV_xZc","V1PldejM1ZCB","uiBwR_Yg1dVO","isRcFkZ3CxpI"],"gpuType":"L4","machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}